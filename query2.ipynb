{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ef25d9-e6be-4ccf-a25d-84a4c3487da8",
   "metadata": {},
   "source": [
    "Output URI\n",
    "s3://groups-bucket-dblab-905418150721/group23/outputs/query2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d331609f",
   "metadata": {},
   "source": [
    "Να βρεθούν, για κάθε έτος, τα 3 Αστυνομικά Τμήματα με το υψηλότερο ποσοστό κλεισμένων (περατω-\n",
    "μένων) υποθέσεων. Να τυπωθούν το έτος, τα ονόματα (τοποθεσίες) των τμημάτων, τα ποσοστά τους\n",
    "καθώς και οι αριθμοί του ranking τους στην ετήσια κατάταξη. Τα αποτελέσματα να δοθούν σε σειρά\n",
    "αύξουσα ως προς το έτος και το ranking \n",
    "\n",
    "Να υλοποιηθεί το Query 2 χρησιμοποιώντας τα DataFrame και SQL APIs. Να αναφέρετε και\n",
    "να συγκρίνετε τους χρόνους εκτέλεσης μεταξύ των δύο υλοποιήσεων. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf4be18",
   "metadata": {},
   "source": [
    "θετουμε το configuration όπως το θέλει η εκφώνηση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd7fd1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '4', 'spark.executor.memory': '2g', 'spark.executor.cores': '2', 'spark.driver.memory': '4g'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>300</td><td>application_1738075734771_0301</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0301/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-241.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0301_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>326</td><td>application_1738075734771_0327</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0327/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-241.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0327_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>330</td><td>application_1738075734771_0331</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0331/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0331_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>343</td><td>application_1738075734771_0344</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0344/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0344_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>353</td><td>application_1738075734771_0354</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0354/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0354_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>373</td><td>application_1738075734771_0374</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0374/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-115.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0374_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>379</td><td>application_1738075734771_0380</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0380/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-220.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0380_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>384</td><td>application_1738075734771_0385</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0385/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0385_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>387</td><td>application_1738075734771_0388</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0388/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-104.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0388_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>388</td><td>application_1738075734771_0389</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0389/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-241.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0389_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>392</td><td>application_1738075734771_0393</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0393/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0393_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>395</td><td>application_1738075734771_0396</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0396/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-139.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0396_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>422</td><td>application_1738075734771_0422</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0422/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-245.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0422_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>425</td><td>application_1738075734771_0425</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0425/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-115.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0425_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>427</td><td>application_1738075734771_0427</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0427/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0427_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>428</td><td>application_1738075734771_0428</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0428/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-104.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0428_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"4\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"2\",\n",
    "        \"spark.driver.memory\": \"4g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1665f41f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90ff1008",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>429</td><td>application_1738075734771_0429</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0429/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-86.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0429_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query2\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75dd2a99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Query 2\n",
      "Sample of Date Rptd values:\n",
      "+----------------------+\n",
      "|Date Rptd             |\n",
      "+----------------------+\n",
      "|08/25/2011 12:00:00 AM|\n",
      "|03/19/2013 12:00:00 AM|\n",
      "|12/24/2012 12:00:00 AM|\n",
      "|03/28/2011 12:00:00 AM|\n",
      "|12/19/2011 12:00:00 AM|\n",
      "|05/11/2017 12:00:00 AM|\n",
      "|02/23/2011 12:00:00 AM|\n",
      "|12/01/2014 12:00:00 AM|\n",
      "|04/01/2011 12:00:00 AM|\n",
      "|07/25/2013 12:00:00 AM|\n",
      "+----------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----+-------------------------+\n",
      "|Year|count(DISTINCT Area Name)|\n",
      "+----+-------------------------+\n",
      "|2020|                       21|\n",
      "|2014|                       21|\n",
      "|2018|                       21|\n",
      "|2022|                       21|\n",
      "|2023|                       21|\n",
      "|2010|                       21|\n",
      "|2021|                       21|\n",
      "|2015|                       21|\n",
      "|2011|                       21|\n",
      "|2024|                       21|\n",
      "|2016|                       21|\n",
      "|2012|                       21|\n",
      "|2019|                       21|\n",
      "|2017|                       21|\n",
      "|2013|                       21|\n",
      "+----+-------------------------+\n",
      "\n",
      "DataFrame Results:\n",
      "+----+-----------+------------------+----+\n",
      "|Year|Area Name  |closed_case_rate  |rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|Rampart    |32.947355855318136|1   |\n",
      "|2010|Olympic    |31.962706191728422|2   |\n",
      "|2010|Harbor     |29.63203463203463 |3   |\n",
      "|2011|Olympic    |35.212167689161554|1   |\n",
      "|2011|Rampart    |32.511779630300836|2   |\n",
      "|2011|Harbor     |28.652205202015008|3   |\n",
      "|2012|Olympic    |34.414818310523835|1   |\n",
      "|2012|Rampart    |32.9464181029429  |2   |\n",
      "|2012|Harbor     |29.815133276010318|3   |\n",
      "|2013|Olympic    |33.52812271731191 |1   |\n",
      "|2013|Rampart    |32.08287360549221 |2   |\n",
      "|2013|Harbor     |29.16422459266206 |3   |\n",
      "|2014|Van Nuys   |31.80567315834039 |1   |\n",
      "|2014|West Valley|31.31198995605775 |2   |\n",
      "|2014|Mission    |31.16279069767442 |3   |\n",
      "|2015|Van Nuys   |32.64134698172773 |1   |\n",
      "|2015|West Valley|30.27597402597403 |2   |\n",
      "|2015|Mission    |30.179460678380153|3   |\n",
      "|2016|Van Nuys   |31.880755720117726|1   |\n",
      "|2016|West Valley|31.54798761609907 |2   |\n",
      "|2016|Foothill   |29.870291843352458|3   |\n",
      "|2017|Van Nuys   |32.02034211742949 |1   |\n",
      "|2017|Mission    |31.03892518634398 |2   |\n",
      "|2017|Foothill   |30.469226081657524|3   |\n",
      "|2018|Foothill   |30.708950655075302|1   |\n",
      "|2018|Mission    |30.690661478599225|2   |\n",
      "|2018|Van Nuys   |29.078685730517943|3   |\n",
      "|2019|West Valley|30.77447195094254 |1   |\n",
      "|2019|Mission    |30.748519116855142|2   |\n",
      "|2019|Foothill   |29.53842186694172 |3   |\n",
      "|2020|West Valley|31.144886009717204|1   |\n",
      "|2020|Mission    |30.38777908343126 |2   |\n",
      "|2020|Harbor     |29.880657509569918|3   |\n",
      "|2021|Mission    |30.91391367253436 |1   |\n",
      "|2021|West Valley|28.877503493246394|2   |\n",
      "|2021|Foothill   |28.464788732394364|3   |\n",
      "|2022|West Valley|26.64366494153728 |1   |\n",
      "|2022|Harbor     |26.334056399132322|2   |\n",
      "|2022|Topanga    |26.27340236376948 |3   |\n",
      "|2023|Foothill   |26.8215994531784  |1   |\n",
      "|2023|Topanga    |26.407806464728605|2   |\n",
      "|2023|Mission    |25.941195616795053|3   |\n",
      "|2024|N Hollywood|19.514978601997147|1   |\n",
      "|2024|Foothill   |18.531827515400412|2   |\n",
      "|2024|77th Street|17.349137931034484|3   |\n",
      "+----+-----------+------------------+----+\n",
      "\n",
      "DataFrame Time taken: 37.35 seconds"
     ]
    }
   ],
   "source": [
    "#Spark DataFrame code \n",
    "\n",
    "from pyspark.sql.functions import col, lower, udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, rank, desc\n",
    "from pyspark.sql.functions import year, to_date\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.functions import dense_rank\n",
    "import csv\n",
    "import time\n",
    "\n",
    "\n",
    "# Define file paths\n",
    "crime_2019_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "crime_2020_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "\n",
    "print(\"Starting Query 2\")\n",
    "\n",
    "# --- DataFrame-based Implementation ---\n",
    "start_time_df = time.time()\n",
    "\n",
    "# Load datasets\n",
    "crime_2019 = spark.read.csv(crime_2019_path, header=True)\n",
    "crime_2020 = spark.read.csv(crime_2020_path, header=True)\n",
    "\n",
    "# Combine datasets\n",
    "crime_data = crime_2019.union(crime_2020)\n",
    "\n",
    "print(\"Sample of Date Rptd values:\")\n",
    "crime_data.select(\"Date Rptd\").distinct().show(10, truncate=False)\n",
    "\n",
    "# Add a year column\n",
    "crime_data = crime_data.withColumn(\"Year\", col(\"Date Rptd\").substr(7, 4))\n",
    "\n",
    "\n",
    "# Filter and calculate closed case rates, we need both the closed and all of them\n",
    "closed_cases = crime_data.filter(((col(\"Status Desc\") != \"UNK\") & (col(\"Status Desc\") != \"Invest Cont\")) & col(\"Year\").isNotNull())\n",
    "total_cases = crime_data.filter(col(\"Year\").isNotNull())\n",
    "\n",
    "crime_data.groupBy(\"Year\").agg(countDistinct(\"Area Name\")).show()\n",
    "\n",
    "\n",
    "# This is the number of closed cases per year and area\n",
    "closed_case_rates = closed_cases.groupBy(col(\"Year\"), col(\"Area Name\")).count() \\\n",
    "    .withColumnRenamed(\"count\", \"closed_count\")\n",
    "\n",
    "# This is the number of cases per year and area\n",
    "area_totals = total_cases.groupBy(col(\"Year\"), col(\"Area Name\")).count() \\\n",
    "    .withColumnRenamed(\"count\", \"total_count\")\n",
    "\n",
    "# This is the rate of closed cases per year and area\n",
    "rate_df = closed_case_rates.join(area_totals, [\"Year\", \"Area Name\"], how=\"inner\") \\\n",
    "    .withColumn(\"closed_case_rate\", (col(\"closed_count\") / col(\"total_count\")) * 100)\n",
    "\n",
    "# We partition to order in descending order over the year\n",
    "window_spec = Window.partitionBy(\"Year\").orderBy(desc(\"closed_case_rate\"))\n",
    "# We calculate the rank\n",
    "ranked_df = rate_df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "\n",
    "# Add rankings per year (This was a test)\n",
    "# rank_test = rate_df.withColumn(\"rnk\", dense_rank().over(window_spec))\n",
    "# rank_test.orderBy(\"Year\", desc(\"closed_case_rate\")).show(1000, truncate=False)\n",
    "\n",
    "# Filter for top 3 per year\n",
    "top3_df = ranked_df.filter(col(\"rank\") <= 3).orderBy(\"Year\", \"rank\")\n",
    "\n",
    "# Rename columns to match example output\n",
    "formatted_df = top3_df.select(\n",
    "    col(\"Year\").alias(\"year\"),\n",
    "    col(\"Area Name\").alias(\"precinct\"),\n",
    "    col(\"closed_case_rate\"),\n",
    "    col(\"rank\").alias(\"#\")\n",
    ")\n",
    "# Show results\n",
    "print(\"DataFrame Results:\")\n",
    "top3_df.select(\"Year\", \"Area Name\", \"closed_case_rate\", \"rank\").show(1000, truncate=False)\n",
    "\n",
    "end_time_df = time.time()\n",
    "df_elapsed_time = end_time_df - start_time_df\n",
    "print(f\"DataFrame Time taken: {df_elapsed_time:.2f} seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f076212",
   "metadata": {},
   "source": [
    "Now lets test the implementation in SQL code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fae2e7",
   "metadata": {},
   "source": [
    "We can achieve the same result in SQL by:\n",
    "\n",
    "    *Creating or replacing a temporary view (crime_view) from the (already filtered!) crime_data.\n",
    "    *Running a CTE query with closed_cases, total_cases, and a rank window function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b440b37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01ace148",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----------------+---+\n",
      "|year|precinct   |closed_case_rate |#  |\n",
      "+----+-----------+-----------------+---+\n",
      "|2010|Rampart    |32.94735585531813|1  |\n",
      "|2010|Olympic    |31.96270619172842|2  |\n",
      "|2010|Harbor     |29.63203463203463|3  |\n",
      "|2011|Olympic    |35.21216768916155|1  |\n",
      "|2011|Rampart    |32.51177963030083|2  |\n",
      "|2011|Harbor     |28.65220520201501|3  |\n",
      "|2012|Olympic    |34.41481831052383|1  |\n",
      "|2012|Rampart    |32.94641810294290|2  |\n",
      "|2012|Harbor     |29.81513327601032|3  |\n",
      "|2013|Olympic    |33.52812271731191|1  |\n",
      "|2013|Rampart    |32.08287360549222|2  |\n",
      "|2013|Harbor     |29.16422459266206|3  |\n",
      "|2014|Van Nuys   |31.80567315834039|1  |\n",
      "|2014|West Valley|31.31198995605775|2  |\n",
      "|2014|Mission    |31.16279069767442|3  |\n",
      "|2015|Van Nuys   |32.64134698172773|1  |\n",
      "|2015|West Valley|30.27597402597403|2  |\n",
      "|2015|Mission    |30.17946067838016|3  |\n",
      "|2016|Van Nuys   |31.88075572011773|1  |\n",
      "|2016|West Valley|31.54798761609907|2  |\n",
      "|2016|Foothill   |29.87029184335246|3  |\n",
      "|2017|Van Nuys   |32.02034211742950|1  |\n",
      "|2017|Mission    |31.03892518634398|2  |\n",
      "|2017|Foothill   |30.46922608165753|3  |\n",
      "|2018|Foothill   |30.70895065507530|1  |\n",
      "|2018|Mission    |30.69066147859922|2  |\n",
      "|2018|Van Nuys   |29.07868573051794|3  |\n",
      "|2019|West Valley|30.77447195094254|1  |\n",
      "|2019|Mission    |30.74851911685514|2  |\n",
      "|2019|Foothill   |29.53842186694172|3  |\n",
      "|2020|West Valley|31.14488600971720|1  |\n",
      "|2020|Mission    |30.38777908343126|2  |\n",
      "|2020|Harbor     |29.88065750956992|3  |\n",
      "|2021|Mission    |30.91391367253436|1  |\n",
      "|2021|West Valley|28.87750349324639|2  |\n",
      "|2021|Foothill   |28.46478873239437|3  |\n",
      "|2022|West Valley|26.64366494153728|1  |\n",
      "|2022|Harbor     |26.33405639913232|2  |\n",
      "|2022|Topanga    |26.27340236376948|3  |\n",
      "|2023|Foothill   |26.82159945317840|1  |\n",
      "|2023|Topanga    |26.40780646472860|2  |\n",
      "|2023|Mission    |25.94119561679505|3  |\n",
      "|2024|N Hollywood|19.51497860199715|1  |\n",
      "|2024|Foothill   |18.53182751540041|2  |\n",
      "|2024|77th Street|17.34913793103448|3  |\n",
      "+----+-----------+-----------------+---+\n",
      "\n",
      "SQL Time taken: 18.65 seconds"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import to_date, year, col\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 1. Assuming \"crime_data\" is already loaded and combined as above.\n",
    "# --------------------------------------------------------------------------------\n",
    "start_time_sql = time.time()\n",
    "\n",
    "# Load datasets\n",
    "crime19 = spark.read.csv(crime_2019_path, header=True)\n",
    "crime20 = spark.read.csv(crime_2020_path, header=True)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 2. Create a temp view for querying via Spark SQL\n",
    "# --------------------------------------------------------------------------------\n",
    "crime19.createOrReplaceTempView(\"crime19\")\n",
    "crime20.createOrReplaceTempView(\"crime20\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 3. SQL Query using CTEs\n",
    "# --------------------------------------------------------------------------------\n",
    "sql_query = \"\"\"\n",
    "    WITH crime_view AS (\n",
    "        SELECT * FROM crime19\n",
    "        UNION\n",
    "        SELECT * FROM crime20\n",
    "    ),\n",
    "    crime_with_year AS (\n",
    "        SELECT *, substr(`Date Rptd`, 7, 4) AS Year\n",
    "        FROM crime_view\n",
    "        WHERE substr(`Date Rptd`, 7, 4) IS NOT NULL\n",
    "    ),\n",
    "    closed_cases AS (\n",
    "        SELECT \n",
    "            Year,\n",
    "            `Area Name` AS precinct,\n",
    "            COUNT(*) AS closed_count\n",
    "        FROM crime_with_year\n",
    "        WHERE `Status Desc` != 'UNK'\n",
    "          AND `Status Desc` != 'Invest Cont'\n",
    "        GROUP BY Year, `Area Name`\n",
    "    ),\n",
    "    total_cases AS (\n",
    "        SELECT \n",
    "            Year,\n",
    "            `Area Name` AS precinct,\n",
    "            COUNT(*) AS total_count\n",
    "        FROM crime_with_year\n",
    "        GROUP BY Year, `Area Name`\n",
    "    )\n",
    "SELECT\n",
    "    sub.Year AS year,\n",
    "    sub.precinct,\n",
    "    (sub.closed_count * 100.0 / sub.total_count) AS closed_case_rate,\n",
    "    sub.rnk AS `#`\n",
    "FROM (\n",
    "    SELECT\n",
    "        c.Year,\n",
    "        c.precinct,\n",
    "        c.closed_count,\n",
    "        t.total_count,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY c.Year\n",
    "            ORDER BY (c.closed_count * 1.0 / t.total_count) DESC\n",
    "        ) AS rnk\n",
    "    FROM closed_cases c\n",
    "    JOIN total_cases t\n",
    "      ON c.Year     = t.Year\n",
    "     AND c.precinct = t.precinct\n",
    ") sub\n",
    "WHERE sub.rnk <= 3\n",
    "ORDER BY sub.Year, sub.rnk\n",
    "\"\"\"\n",
    "\n",
    "# 4) Execute the SQL and display the results\n",
    "sql_result = spark.sql(sql_query)\n",
    "sql_result.show(50, truncate=False)\n",
    "\n",
    "# 5) End timing for SQL approach\n",
    "end_time_sql = time.time()\n",
    "sql_elapsed_time = end_time_sql - start_time_sql\n",
    "print(f\"SQL Time taken: {sql_elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43636cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Time taken: 37.35 seconds\n",
      "SQL Time taken: 18.65 seconds"
     ]
    }
   ],
   "source": [
    "print(f\"DataFrame Time taken: {df_elapsed_time:.2f} seconds\")\n",
    "print(f\"SQL Time taken: {sql_elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db4dedc-5369-4b62-a495-e9717a9a36a7",
   "metadata": {},
   "source": [
    "# Ερώτημα β"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e99d720-5dc6-45ec-94a5-911be4efd221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151fc45088a94b1cbe6d82dd6736fd73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "csv_2019_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "csv_2020_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "\n",
    "# Load datasets\n",
    "crime_2019 = spark.read.csv(crime_2019_path, header=True)\n",
    "crime_2020 = spark.read.csv(crime_2020_path, header=True)\n",
    "\n",
    "# Combine datasets\n",
    "crime_data = crime_2019.union(crime_2020)\n",
    "\n",
    "# output URI\n",
    "\n",
    "output_URI = \"s3://groups-bucket-dblab-905418150721/group23/outputs/query2/\"\n",
    "\n",
    "# Save the combined DataFrame as a single Parquet file\n",
    "crime_data.coalesce(1).write.mode(\"overwrite\").parquet(output_URI)\n",
    "\n",
    "print(f\"Data successfully written as a single Parquet file to {output_URI}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931826ea-8763-4bbd-82ef-6a7a45e7a079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, countDistinct, desc, row_number\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# Start timer\n",
    "start_time_df = time.time()\n",
    "\n",
    "\n",
    "parquet_file_path = \"s3://groups-bucket-dblab-905418150721/group23/outputs/query2/part-00000-9934d31d-7771-49a9-97e2-e54e63b10046-c000.snappy.parquet\"\n",
    "\n",
    "# Load the Parquet file\n",
    "crime_data = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "print(\"Sample of Date Rptd values:\")\n",
    "crime_data.select(\"Date Rptd\").distinct().show(10, truncate=False)\n",
    "\n",
    "# Add a year column\n",
    "crime_data = crime_data.withColumn(\"Year\", col(\"Date Rptd\").substr(7, 4))\n",
    "\n",
    "\n",
    "# Filter and calculate closed case rates, we need both the closed and all of them\n",
    "closed_cases = crime_data.filter(((col(\"Status Desc\") != \"UNK\") & (col(\"Status Desc\") != \"Invest Cont\")) & col(\"Year\").isNotNull())\n",
    "total_cases = crime_data.filter(col(\"Year\").isNotNull())\n",
    "\n",
    "crime_data.groupBy(\"Year\").agg(countDistinct(\"Area Name\")).show()\n",
    "\n",
    "\n",
    "# This is the number of closed cases per year and area\n",
    "closed_case_rates = closed_cases.groupBy(col(\"Year\"), col(\"Area Name\")).count() \\\n",
    "    .withColumnRenamed(\"count\", \"closed_count\")\n",
    "\n",
    "# This is the number of cases per year and area\n",
    "area_totals = total_cases.groupBy(col(\"Year\"), col(\"Area Name\")).count() \\\n",
    "    .withColumnRenamed(\"count\", \"total_count\")\n",
    "\n",
    "# This is the rate of closed cases per year and area\n",
    "rate_df = closed_case_rates.join(area_totals, [\"Year\", \"Area Name\"], how=\"inner\") \\\n",
    "    .withColumn(\"closed_case_rate\", (col(\"closed_count\") / col(\"total_count\")) * 100)\n",
    "\n",
    "# We partition to order in descending order over the year\n",
    "window_spec = Window.partitionBy(\"Year\").orderBy(desc(\"closed_case_rate\"))\n",
    "# We calculate the rank\n",
    "ranked_df = rate_df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "\n",
    "# Add rankings per year (This was a test)\n",
    "# rank_test = rate_df.withColumn(\"rnk\", dense_rank().over(window_spec))\n",
    "# rank_test.orderBy(\"Year\", desc(\"closed_case_rate\")).show(1000, truncate=False)\n",
    "\n",
    "# Filter for top 3 per year\n",
    "top3_df = ranked_df.filter(col(\"rank\") <= 3).orderBy(\"Year\", \"rank\")\n",
    "\n",
    "# Rename columns to match example output\n",
    "formatted_df = top3_df.select(\n",
    "    col(\"Year\").alias(\"year\"),\n",
    "    col(\"Area Name\").alias(\"precinct\"),\n",
    "    col(\"closed_case_rate\"),\n",
    "    col(\"rank\").alias(\"#\")\n",
    ")\n",
    "# Show results\n",
    "print(\"DataFrame Results:\")\n",
    "top3_df.select(\"Year\", \"Area Name\", \"closed_case_rate\", \"rank\").show(1000, truncate=False)\n",
    "\n",
    "end_time_df = time.time()\n",
    "df_elapsed_time = end_time_df - start_time_df\n",
    "print(f\"DataFrame Time taken: {df_elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e08007",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, desc, rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1. Αρχικοποίηση SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSVvsParquet\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. S3 paths \n",
    "csv_2019_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "csv_2020_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "parquet_output_path = \"s3://buckets/groups-bucket-dblab-905418150721/group23/outputs/query2/output_parquet_singlefile\"  # Εδώ θα γράψουμε 1 .parquet\n",
    "parquet_input_path  = \"s3://buckets/groups-bucket-dblab-905418150721/group23/outputs/query2/output_parquet_singlefile\"  # Για να το ξαναφορτώσουμε\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Function for \"top 3 closed cases per year\"\n",
    "# ----------------------------------------------------------------------------\n",
    "def compute_top3_closed(df):\n",
    "    \"\"\"\n",
    "    df: DataFrame με columns: [Date Rptd, Status Desc, Area Name, ...]\n",
    "    Επιστρέφει DataFrame με Year, Area Name, closed_case_rate, rank\n",
    "    όπου \"closed\" = οτιδήποτε εκτός από \"UNK\" και \"Invest Cont\"\n",
    "    και total = όλα τα records.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Εξαγωγή έτους απλό, π.χ. substr(7,4) ή καλύτερα χρήση regexp/ημερομηνίας\n",
    "    df = df.withColumn(\"Year\", col(\"Date Rptd\").substr(7,4))\n",
    "    \n",
    "    # Closed = Status Desc != 'UNK' & != 'Invest Cont'\n",
    "    closed_df = df.filter((col(\"Status Desc\") != \"UNK\") &\n",
    "                          (col(\"Status Desc\") != \"Invest Cont\"))\n",
    "    total_df  = df  # όλα\n",
    "    \n",
    "    # Μετράμε closed / total per (Year, Area Name)\n",
    "    closed_agg = closed_df.groupBy(\"Year\", \"Area Name\").count().alias(\"closed_count\")\n",
    "    closed_agg = closed_agg.withColumnRenamed(\"count\", \"closed_count\")\n",
    "\n",
    "    total_agg = total_df.groupBy(\"Year\", \"Area Name\").count().alias(\"total_count\")\n",
    "    total_agg = total_agg.withColumnRenamed(\"count\", \"total_count\")\n",
    "    \n",
    "    # Join και υπολογισμός ποσοστού\n",
    "    joined = closed_agg.join(total_agg, [\"Year\", \"Area Name\"], how=\"inner\") \\\n",
    "                       .withColumn(\"closed_case_rate\",\n",
    "                                   (col(\"closed_count\")*100.0 / col(\"total_count\")))\n",
    "    \n",
    "    # Παράθυρο για κατάταξη top 3 ανά Year\n",
    "    w = Window.partitionBy(\"Year\").orderBy(desc(\"closed_case_rate\"))\n",
    "    ranked = joined.withColumn(\"rank\", rank().over(w))\n",
    "    \n",
    "    top3 = ranked.filter(col(\"rank\") <= 3).orderBy(\"Year\", \"rank\")\n",
    "    return top3\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# A) Φόρτωση από CSV & υπολογισμός χρόνου\n",
    "# ----------------------------------------------------------------------------\n",
    "start_time_csv = time.time()\n",
    "\n",
    "csv_2019_df = spark.read.csv(csv_2019_path, header=True, inferSchema=True)\n",
    "csv_2020_df = spark.read.csv(csv_2020_path, header=True, inferSchema=True)\n",
    "\n",
    "df_csv = csv_2019_df.union(csv_2020_df)\n",
    "\n",
    "# Ερώτημα top3\n",
    "top3_from_csv = compute_top3_closed(df_csv)\n",
    "\n",
    "# Δείξε αποτελέσματα (προαιρετικά)\n",
    "top3_from_csv.show(10, truncate=False)\n",
    "\n",
    "end_time_csv = time.time()\n",
    "csv_elapsed = end_time_csv - start_time_csv\n",
    "print(f\"Time (load CSV + compute query) = {csv_elapsed:.2f} sec\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# B) Αποθήκευση σε Parquet (ένα μόνο αρχείο) & Επαναφόρτωση\n",
    "# ----------------------------------------------------------------------------\n",
    "# Παράδειγμα αποθήκευσης ως Parquet σε ΕΝΑ αρχείο:\n",
    "df_csv.coalesce(1).write.mode(\"overwrite\").parquet(parquet_output_path)\n",
    "# Το coalesce(1) αναγκάζει τη δημιουργία ενός μόνο part αρχείου. \n",
    "# Μεγάλα datasets -> δεν συνιστάται για production, αλλά οκ για την άσκηση.\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# C) Φόρτωση από Parquet & υπολογισμός χρόνου\n",
    "# ----------------------------------------------------------------------------\n",
    "start_time_parquet = time.time()\n",
    "\n",
    "df_parquet = spark.read.parquet(parquet_input_path)\n",
    "top3_from_parquet = compute_top3_closed(df_parquet)\n",
    "top3_from_parquet.show(10, truncate=False)\n",
    "\n",
    "end_time_parquet = time.time()\n",
    "parquet_elapsed = end_time_parquet - start_time_parquet\n",
    "print(f\"Time (load Parquet + compute query) = {parquet_elapsed:.2f} sec\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# D) Σύγκριση Χρόνων\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"===========================================\")\n",
    "print(f\"CSV total time:      {csv_elapsed:.2f} sec\")\n",
    "print(f\"Parquet total time:  {parquet_elapsed:.2f} sec\")\n",
    "print(\"===========================================\")\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
