{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ef25d9-e6be-4ccf-a25d-84a4c3487da8",
   "metadata": {},
   "source": [
    "Output URI\n",
    "s3://groups-bucket-dblab-905418150721/group23/outputs/query2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d331609f",
   "metadata": {},
   "source": [
    "Να βρεθούν, για κάθε έτος, τα 3 Αστυνομικά Τμήματα με το υψηλότερο ποσοστό κλεισμένων (περατω-\n",
    "μένων) υποθέσεων. Να τυπωθούν το έτος, τα ονόματα (τοποθεσίες) των τμημάτων, τα ποσοστά τους\n",
    "καθώς και οι αριθμοί του ranking τους στην ετήσια κατάταξη. Τα αποτελέσματα να δοθούν σε σειρά\n",
    "αύξουσα ως προς το έτος και το ranking \n",
    "\n",
    "Να υλοποιηθεί το Query 2 χρησιμοποιώντας τα DataFrame και SQL APIs. Να αναφέρετε και\n",
    "να συγκρίνετε τους χρόνους εκτέλεσης μεταξύ των δύο υλοποιήσεων. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf4be18",
   "metadata": {},
   "source": [
    "θετουμε το configuration όπως το θέλει η εκφώνηση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fd7fd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 3986 did not start up in 60 seconds..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '4', 'spark.executor.memory': '2g', 'spark.executor.cores': '2', 'spark.driver.memory': '4g'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3839</td><td>application_1732639283265_3779</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3779/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3779_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3843</td><td>application_1732639283265_3783</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3783/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3783_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3850</td><td>application_1732639283265_3790</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3790/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-203.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3790_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3855</td><td>application_1732639283265_3795</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3795/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3795_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3872</td><td>application_1732639283265_3812</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3812/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-119.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3812_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3875</td><td>application_1732639283265_3815</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3815/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3815_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3882</td><td>application_1732639283265_3822</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3822/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-247.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3822_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3883</td><td>application_1732639283265_3823</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3823/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-166.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3823_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3887</td><td>application_1732639283265_3827</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3827/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-166.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3827_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3897</td><td>application_1732639283265_3837</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3837/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-119.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3837_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3959</td><td>application_1732639283265_3899</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3899/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3899_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3979</td><td>application_1732639283265_3919</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3919/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3919_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3988</td><td>application_1732639283265_3928</td><td>pyspark</td><td>starting</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3928/\">Link</a></td><td></td><td>None</td><td></td></tr><tr><td>3989</td><td>application_1732639283265_3929</td><td>pyspark</td><td>starting</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3929/\">Link</a></td><td></td><td>None</td><td></td></tr><tr><td>3990</td><td>None</td><td>pyspark</td><td>starting</td><td></td><td></td><td>None</td><td></td></tr><tr><td>3991</td><td>None</td><td>pyspark</td><td>starting</td><td></td><td></td><td>None</td><td></td></tr><tr><td>3992</td><td>None</td><td>pyspark</td><td>starting</td><td></td><td></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"4\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"2\",\n",
    "        \"spark.driver.memory\": \"4g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1665f41f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90ff1008",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3828</td><td>application_1732639283265_3768</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3768/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-174.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3768_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query2\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75dd2a99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Query 2\n",
      "Sample of Date Rptd values:\n",
      "+----------------------+\n",
      "|Date Rptd             |\n",
      "+----------------------+\n",
      "|11/12/2013 12:00:00 AM|\n",
      "|01/18/2013 12:00:00 AM|\n",
      "|03/13/2022 12:00:00 AM|\n",
      "|03/19/2013 12:00:00 AM|\n",
      "|09/22/2021 12:00:00 AM|\n",
      "|12/24/2012 12:00:00 AM|\n",
      "|04/11/2013 12:00:00 AM|\n",
      "|09/14/2012 12:00:00 AM|\n",
      "|01/22/2012 12:00:00 AM|\n",
      "|12/01/2014 12:00:00 AM|\n",
      "+----------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----+-------------------------+\n",
      "|Year|count(DISTINCT Area Name)|\n",
      "+----+-------------------------+\n",
      "|2020|                       21|\n",
      "|2014|                       21|\n",
      "|2018|                       21|\n",
      "|2022|                       21|\n",
      "|2023|                       21|\n",
      "|2010|                       21|\n",
      "|2021|                       21|\n",
      "|2015|                       21|\n",
      "|2011|                       21|\n",
      "|2024|                       21|\n",
      "|2016|                       21|\n",
      "|2012|                       21|\n",
      "|2019|                       21|\n",
      "|2017|                       21|\n",
      "|2013|                       21|\n",
      "+----+-------------------------+\n",
      "\n",
      "DataFrame Results:\n",
      "+----+-----------+------------------+----+\n",
      "|Year|Area Name  |closed_case_rate  |rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|Rampart    |32.947355855318136|1   |\n",
      "|2010|Olympic    |31.962706191728422|2   |\n",
      "|2010|Harbor     |29.63203463203463 |3   |\n",
      "|2011|Olympic    |35.212167689161554|1   |\n",
      "|2011|Rampart    |32.511779630300836|2   |\n",
      "|2011|Harbor     |28.652205202015008|3   |\n",
      "|2012|Olympic    |34.414818310523835|1   |\n",
      "|2012|Rampart    |32.9464181029429  |2   |\n",
      "|2012|Harbor     |29.815133276010318|3   |\n",
      "|2013|Olympic    |33.52812271731191 |1   |\n",
      "|2013|Rampart    |32.08287360549221 |2   |\n",
      "|2013|Harbor     |29.16422459266206 |3   |\n",
      "|2014|Van Nuys   |31.80567315834039 |1   |\n",
      "|2014|West Valley|31.31198995605775 |2   |\n",
      "|2014|Mission    |31.16279069767442 |3   |\n",
      "|2015|Van Nuys   |32.64134698172773 |1   |\n",
      "|2015|West Valley|30.27597402597403 |2   |\n",
      "|2015|Mission    |30.179460678380153|3   |\n",
      "|2016|Van Nuys   |31.880755720117726|1   |\n",
      "|2016|West Valley|31.54798761609907 |2   |\n",
      "|2016|Foothill   |29.870291843352458|3   |\n",
      "|2017|Van Nuys   |32.02034211742949 |1   |\n",
      "|2017|Mission    |31.03892518634398 |2   |\n",
      "|2017|Foothill   |30.469226081657524|3   |\n",
      "|2018|Foothill   |30.708950655075302|1   |\n",
      "|2018|Mission    |30.690661478599225|2   |\n",
      "|2018|Van Nuys   |29.078685730517943|3   |\n",
      "|2019|West Valley|30.77447195094254 |1   |\n",
      "|2019|Mission    |30.748519116855142|2   |\n",
      "|2019|Foothill   |29.53842186694172 |3   |\n",
      "|2020|West Valley|31.144886009717204|1   |\n",
      "|2020|Mission    |30.38777908343126 |2   |\n",
      "|2020|Harbor     |29.880657509569918|3   |\n",
      "|2021|Mission    |30.91391367253436 |1   |\n",
      "|2021|West Valley|28.877503493246394|2   |\n",
      "|2021|Foothill   |28.464788732394364|3   |\n",
      "|2022|West Valley|26.64366494153728 |1   |\n",
      "|2022|Harbor     |26.334056399132322|2   |\n",
      "|2022|Topanga    |26.27340236376948 |3   |\n",
      "|2023|Foothill   |26.8215994531784  |1   |\n",
      "|2023|Topanga    |26.407806464728605|2   |\n",
      "|2023|Mission    |25.941195616795053|3   |\n",
      "|2024|N Hollywood|19.514978601997147|1   |\n",
      "|2024|Foothill   |18.531827515400412|2   |\n",
      "|2024|77th Street|17.349137931034484|3   |\n",
      "+----+-----------+------------------+----+\n",
      "\n",
      "DataFrame Time taken: 38.84 seconds"
     ]
    }
   ],
   "source": [
    "#Spark DataFrame code \n",
    "\n",
    "from pyspark.sql.functions import col, lower, udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, rank, desc\n",
    "from pyspark.sql.functions import year, to_date\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.functions import dense_rank\n",
    "import csv\n",
    "import time\n",
    "\n",
    "\n",
    "# Define file paths\n",
    "crime_2019_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "crime_2020_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "\n",
    "print(\"Starting Query 2\")\n",
    "\n",
    "# --- DataFrame-based Implementation ---\n",
    "start_time_df = time.time()\n",
    "\n",
    "# Load datasets\n",
    "crime_2019 = spark.read.csv(crime_2019_path, header=True)\n",
    "crime_2020 = spark.read.csv(crime_2020_path, header=True)\n",
    "\n",
    "# Combine datasets\n",
    "crime_data = crime_2019.union(crime_2020)\n",
    "\n",
    "print(\"Sample of Date Rptd values:\")\n",
    "crime_data.select(\"Date Rptd\").distinct().show(10, truncate=False)\n",
    "\n",
    "# Add a year column\n",
    "crime_data = crime_data.withColumn(\"Year\", col(\"Date Rptd\").substr(7, 4))\n",
    "\n",
    "\n",
    "# Filter and calculate closed case rates, we need both the closed and all of them\n",
    "closed_cases = crime_data.filter(((col(\"Status Desc\") != \"UNK\") & (col(\"Status Desc\") != \"Invest Cont\")) & col(\"Year\").isNotNull())\n",
    "total_cases = crime_data.filter(col(\"Year\").isNotNull())\n",
    "\n",
    "crime_data.groupBy(\"Year\").agg(countDistinct(\"Area Name\")).show()\n",
    "\n",
    "\n",
    "# This is the number of closed cases per year and area\n",
    "closed_case_rates = closed_cases.groupBy(col(\"Year\"), col(\"Area Name\")).count() \\\n",
    "    .withColumnRenamed(\"count\", \"closed_count\")\n",
    "\n",
    "# This is the number of cases per year and area\n",
    "area_totals = total_cases.groupBy(col(\"Year\"), col(\"Area Name\")).count() \\\n",
    "    .withColumnRenamed(\"count\", \"total_count\")\n",
    "\n",
    "# This is the rate of closed cases per year and area\n",
    "rate_df = closed_case_rates.join(area_totals, [\"Year\", \"Area Name\"], how=\"inner\") \\\n",
    "    .withColumn(\"closed_case_rate\", (col(\"closed_count\") / col(\"total_count\")) * 100)\n",
    "\n",
    "# We partition to order in descending order over the year\n",
    "window_spec = Window.partitionBy(\"Year\").orderBy(desc(\"closed_case_rate\"))\n",
    "# We calculate the rank\n",
    "ranked_df = rate_df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "\n",
    "# Add rankings per year (This was a test)\n",
    "# rank_test = rate_df.withColumn(\"rnk\", dense_rank().over(window_spec))\n",
    "# rank_test.orderBy(\"Year\", desc(\"closed_case_rate\")).show(1000, truncate=False)\n",
    "\n",
    "# Filter for top 3 per year\n",
    "top3_df = ranked_df.filter(col(\"rank\") <= 3).orderBy(\"Year\", \"rank\")\n",
    "\n",
    "# Rename columns to match example output\n",
    "formatted_df = top3_df.select(\n",
    "    col(\"Year\").alias(\"year\"),\n",
    "    col(\"Area Name\").alias(\"precinct\"),\n",
    "    col(\"closed_case_rate\"),\n",
    "    col(\"rank\").alias(\"#\")\n",
    ")\n",
    "# Show results\n",
    "print(\"DataFrame Results:\")\n",
    "top3_df.select(\"Year\", \"Area Name\", \"closed_case_rate\", \"rank\").show(1000, truncate=False)\n",
    "\n",
    "end_time_df = time.time()\n",
    "df_elapsed_time = end_time_df - start_time_df\n",
    "print(f\"DataFrame Time taken: {df_elapsed_time:.2f} seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f076212",
   "metadata": {},
   "source": [
    "Now lets test the implementation in SQL code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fae2e7",
   "metadata": {},
   "source": [
    "We can achieve the same result in SQL by:\n",
    "\n",
    "    *Creating or replacing a temporary view (crime_view) from the (already filtered!) crime_data.\n",
    "    *Running a CTE query with closed_cases, total_cases, and a rank window function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b440b37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01ace148",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----------------+---+\n",
      "|year|precinct   |closed_case_rate |#  |\n",
      "+----+-----------+-----------------+---+\n",
      "|2010|Rampart    |32.94735585531813|1  |\n",
      "|2010|Olympic    |31.96270619172842|2  |\n",
      "|2010|Harbor     |29.63203463203463|3  |\n",
      "|2011|Olympic    |35.21216768916155|1  |\n",
      "|2011|Rampart    |32.51177963030083|2  |\n",
      "|2011|Harbor     |28.65220520201501|3  |\n",
      "|2012|Olympic    |34.41481831052383|1  |\n",
      "|2012|Rampart    |32.94641810294290|2  |\n",
      "|2012|Harbor     |29.81513327601032|3  |\n",
      "|2013|Olympic    |33.52812271731191|1  |\n",
      "|2013|Rampart    |32.08287360549222|2  |\n",
      "|2013|Harbor     |29.16422459266206|3  |\n",
      "|2014|Van Nuys   |31.80567315834039|1  |\n",
      "|2014|West Valley|31.31198995605775|2  |\n",
      "|2014|Mission    |31.16279069767442|3  |\n",
      "|2015|Van Nuys   |32.64134698172773|1  |\n",
      "|2015|West Valley|30.27597402597403|2  |\n",
      "|2015|Mission    |30.17946067838016|3  |\n",
      "|2016|Van Nuys   |31.88075572011773|1  |\n",
      "|2016|West Valley|31.54798761609907|2  |\n",
      "|2016|Foothill   |29.87029184335246|3  |\n",
      "|2017|Van Nuys   |32.02034211742950|1  |\n",
      "|2017|Mission    |31.03892518634398|2  |\n",
      "|2017|Foothill   |30.46922608165753|3  |\n",
      "|2018|Foothill   |30.70895065507530|1  |\n",
      "|2018|Mission    |30.69066147859922|2  |\n",
      "|2018|Van Nuys   |29.07868573051794|3  |\n",
      "|2019|West Valley|30.77447195094254|1  |\n",
      "|2019|Mission    |30.74851911685514|2  |\n",
      "|2019|Foothill   |29.53842186694172|3  |\n",
      "|2020|West Valley|31.14488600971720|1  |\n",
      "|2020|Mission    |30.38777908343126|2  |\n",
      "|2020|Harbor     |29.88065750956992|3  |\n",
      "|2021|Mission    |30.91391367253436|1  |\n",
      "|2021|West Valley|28.87750349324639|2  |\n",
      "|2021|Foothill   |28.46478873239437|3  |\n",
      "|2022|West Valley|26.64366494153728|1  |\n",
      "|2022|Harbor     |26.33405639913232|2  |\n",
      "|2022|Topanga    |26.27340236376948|3  |\n",
      "|2023|Foothill   |26.82159945317840|1  |\n",
      "|2023|Topanga    |26.40780646472860|2  |\n",
      "|2023|Mission    |25.94119561679505|3  |\n",
      "|2024|N Hollywood|19.51497860199715|1  |\n",
      "|2024|Foothill   |18.53182751540041|2  |\n",
      "|2024|77th Street|17.34913793103448|3  |\n",
      "+----+-----------+-----------------+---+\n",
      "\n",
      "SQL Time taken: 19.90 seconds"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import to_date, year, col\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 1. Assuming \"crime_data\" is already loaded and combined as above.\n",
    "# --------------------------------------------------------------------------------\n",
    "start_time_sql = time.time()\n",
    "\n",
    "# Load datasets\n",
    "crime19 = spark.read.csv(crime_2019_path, header=True)\n",
    "crime20 = spark.read.csv(crime_2020_path, header=True)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 2. Create a temp view for querying via Spark SQL\n",
    "# --------------------------------------------------------------------------------\n",
    "crime19.createOrReplaceTempView(\"crime19\")\n",
    "crime20.createOrReplaceTempView(\"crime20\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 3. SQL Query using CTEs\n",
    "# --------------------------------------------------------------------------------\n",
    "sql_query = \"\"\"\n",
    "    WITH crime_view AS (\n",
    "        SELECT * FROM crime19\n",
    "        UNION\n",
    "        SELECT * FROM crime20\n",
    "    ),\n",
    "    crime_with_year AS (\n",
    "        SELECT *, substr(`Date Rptd`, 7, 4) AS Year\n",
    "        FROM crime_view\n",
    "        WHERE substr(`Date Rptd`, 7, 4) IS NOT NULL\n",
    "    ),\n",
    "    closed_cases AS (\n",
    "        SELECT \n",
    "            Year,\n",
    "            `Area Name` AS precinct,\n",
    "            COUNT(*) AS closed_count\n",
    "        FROM crime_with_year\n",
    "        WHERE `Status Desc` != 'UNK'\n",
    "          AND `Status Desc` != 'Invest Cont'\n",
    "        GROUP BY Year, `Area Name`\n",
    "    ),\n",
    "    total_cases AS (\n",
    "        SELECT \n",
    "            Year,\n",
    "            `Area Name` AS precinct,\n",
    "            COUNT(*) AS total_count\n",
    "        FROM crime_with_year\n",
    "        GROUP BY Year, `Area Name`\n",
    "    )\n",
    "SELECT\n",
    "    sub.Year AS year,\n",
    "    sub.precinct,\n",
    "    (sub.closed_count * 100.0 / sub.total_count) AS closed_case_rate,\n",
    "    sub.rnk AS `#`\n",
    "FROM (\n",
    "    SELECT\n",
    "        c.Year,\n",
    "        c.precinct,\n",
    "        c.closed_count,\n",
    "        t.total_count,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY c.Year\n",
    "            ORDER BY (c.closed_count * 1.0 / t.total_count) DESC\n",
    "        ) AS rnk\n",
    "    FROM closed_cases c\n",
    "    JOIN total_cases t\n",
    "      ON c.Year     = t.Year\n",
    "     AND c.precinct = t.precinct\n",
    ") sub\n",
    "WHERE sub.rnk <= 3\n",
    "ORDER BY sub.Year, sub.rnk\n",
    "\"\"\"\n",
    "\n",
    "# 4) Execute the SQL and display the results\n",
    "sql_result = spark.sql(sql_query)\n",
    "sql_result.show(50, truncate=False)\n",
    "\n",
    "# 5) End timing for SQL approach\n",
    "end_time_sql = time.time()\n",
    "sql_elapsed_time = end_time_sql - start_time_sql\n",
    "print(f\"SQL Time taken: {sql_elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43636cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Time taken: 38.84 seconds\n",
      "SQL Time taken: 19.90 seconds"
     ]
    }
   ],
   "source": [
    "print(f\"DataFrame Time taken: {df_elapsed_time:.2f} seconds\")\n",
    "print(f\"SQL Time taken: {sql_elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db4dedc-5369-4b62-a495-e9717a9a36a7",
   "metadata": {},
   "source": [
    "# Ερώτημα β"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e99d720-5dc6-45ec-94a5-911be4efd221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written as a single Parquet file to s3://groups-bucket-dblab-905418150721/group23/outputs/query2/."
     ]
    }
   ],
   "source": [
    "\n",
    "csv_2019_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "csv_2020_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "\n",
    "# Load datasets\n",
    "crime_2019 = spark.read.csv(crime_2019_path, header=True)\n",
    "crime_2020 = spark.read.csv(crime_2020_path, header=True)\n",
    "\n",
    "# Combine datasets\n",
    "crime_data = crime_2019.union(crime_2020)\n",
    "\n",
    "# output URI\n",
    "\n",
    "output_URI = \"s3://groups-bucket-dblab-905418150721/group23/outputs/query2/\"\n",
    "\n",
    "# Save the combined DataFrame as a single Parquet file\n",
    "crime_data.coalesce(1).write.mode(\"overwrite\").parquet(output_URI)\n",
    "\n",
    "print(f\"Data successfully written as a single Parquet file to {output_URI}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "931826ea-8763-4bbd-82ef-6a7a45e7a079",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of Date Rptd values:\n",
      "+----------------------+\n",
      "|Date Rptd             |\n",
      "+----------------------+\n",
      "|08/25/2011 12:00:00 AM|\n",
      "|11/12/2013 12:00:00 AM|\n",
      "|02/08/2020 12:00:00 AM|\n",
      "|11/15/2021 12:00:00 AM|\n",
      "|12/18/2021 12:00:00 AM|\n",
      "|10/20/2017 12:00:00 AM|\n",
      "|11/15/2019 12:00:00 AM|\n",
      "|01/18/2013 12:00:00 AM|\n",
      "|03/13/2022 12:00:00 AM|\n",
      "|11/27/2023 12:00:00 AM|\n",
      "+----------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----+-------------------------+\n",
      "|Year|count(DISTINCT Area Name)|\n",
      "+----+-------------------------+\n",
      "|2020|                       21|\n",
      "|2014|                       21|\n",
      "|2018|                       21|\n",
      "|2022|                       21|\n",
      "|2023|                       21|\n",
      "|2010|                       21|\n",
      "|2021|                       21|\n",
      "|2015|                       21|\n",
      "|2011|                       21|\n",
      "|2024|                       21|\n",
      "|2016|                       21|\n",
      "|2012|                       21|\n",
      "|2019|                       21|\n",
      "|2017|                       21|\n",
      "|2013|                       21|\n",
      "+----+-------------------------+\n",
      "\n",
      "DataFrame Results:\n",
      "+----+-----------+------------------+----+\n",
      "|Year|Area Name  |closed_case_rate  |rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|Rampart    |32.947355855318136|1   |\n",
      "|2010|Olympic    |31.962706191728422|2   |\n",
      "|2010|Harbor     |29.63203463203463 |3   |\n",
      "|2011|Olympic    |35.212167689161554|1   |\n",
      "|2011|Rampart    |32.511779630300836|2   |\n",
      "|2011|Harbor     |28.652205202015008|3   |\n",
      "|2012|Olympic    |34.414818310523835|1   |\n",
      "|2012|Rampart    |32.9464181029429  |2   |\n",
      "|2012|Harbor     |29.815133276010318|3   |\n",
      "|2013|Olympic    |33.52812271731191 |1   |\n",
      "|2013|Rampart    |32.08287360549221 |2   |\n",
      "|2013|Harbor     |29.16422459266206 |3   |\n",
      "|2014|Van Nuys   |31.80567315834039 |1   |\n",
      "|2014|West Valley|31.31198995605775 |2   |\n",
      "|2014|Mission    |31.16279069767442 |3   |\n",
      "|2015|Van Nuys   |32.64134698172773 |1   |\n",
      "|2015|West Valley|30.27597402597403 |2   |\n",
      "|2015|Mission    |30.179460678380153|3   |\n",
      "|2016|Van Nuys   |31.880755720117726|1   |\n",
      "|2016|West Valley|31.54798761609907 |2   |\n",
      "|2016|Foothill   |29.870291843352458|3   |\n",
      "|2017|Van Nuys   |32.02034211742949 |1   |\n",
      "|2017|Mission    |31.03892518634398 |2   |\n",
      "|2017|Foothill   |30.469226081657524|3   |\n",
      "|2018|Foothill   |30.708950655075302|1   |\n",
      "|2018|Mission    |30.690661478599225|2   |\n",
      "|2018|Van Nuys   |29.078685730517943|3   |\n",
      "|2019|West Valley|30.77447195094254 |1   |\n",
      "|2019|Mission    |30.748519116855142|2   |\n",
      "|2019|Foothill   |29.53842186694172 |3   |\n",
      "|2020|West Valley|31.144886009717204|1   |\n",
      "|2020|Mission    |30.38777908343126 |2   |\n",
      "|2020|Harbor     |29.880657509569918|3   |\n",
      "|2021|Mission    |30.91391367253436 |1   |\n",
      "|2021|West Valley|28.877503493246394|2   |\n",
      "|2021|Foothill   |28.464788732394364|3   |\n",
      "|2022|West Valley|26.64366494153728 |1   |\n",
      "|2022|Harbor     |26.334056399132322|2   |\n",
      "|2022|Topanga    |26.27340236376948 |3   |\n",
      "|2023|Foothill   |26.8215994531784  |1   |\n",
      "|2023|Topanga    |26.407806464728605|2   |\n",
      "|2023|Mission    |25.941195616795053|3   |\n",
      "|2024|N Hollywood|19.514978601997147|1   |\n",
      "|2024|Foothill   |18.531827515400412|2   |\n",
      "|2024|77th Street|17.349137931034484|3   |\n",
      "+----+-----------+------------------+----+\n",
      "\n",
      "DataFrame Time taken: 8.67 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, countDistinct, desc, row_number\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# Start timer\n",
    "start_time_df = time.time()\n",
    "\n",
    "\n",
    "parquet_file_path = \"s3://groups-bucket-dblab-905418150721/group23/outputs/query2/part-00000-9934d31d-7771-49a9-97e2-e54e63b10046-c000.snappy.parquet\"\n",
    "\n",
    "# Load the Parquet file\n",
    "crime_data = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "print(\"Sample of Date Rptd values:\")\n",
    "crime_data.select(\"Date Rptd\").distinct().show(10, truncate=False)\n",
    "\n",
    "# Add a year column\n",
    "crime_data = crime_data.withColumn(\"Year\", col(\"Date Rptd\").substr(7, 4))\n",
    "\n",
    "\n",
    "# Filter and calculate closed case rates, we need both the closed and all of them\n",
    "closed_cases = crime_data.filter(((col(\"Status Desc\") != \"UNK\") & (col(\"Status Desc\") != \"Invest Cont\")) & col(\"Year\").isNotNull())\n",
    "total_cases = crime_data.filter(col(\"Year\").isNotNull())\n",
    "\n",
    "crime_data.groupBy(\"Year\").agg(countDistinct(\"Area Name\")).show()\n",
    "\n",
    "\n",
    "# This is the number of closed cases per year and area\n",
    "closed_case_rates = closed_cases.groupBy(col(\"Year\"), col(\"Area Name\")).count() \\\n",
    "    .withColumnRenamed(\"count\", \"closed_count\")\n",
    "\n",
    "# This is the number of cases per year and area\n",
    "area_totals = total_cases.groupBy(col(\"Year\"), col(\"Area Name\")).count() \\\n",
    "    .withColumnRenamed(\"count\", \"total_count\")\n",
    "\n",
    "# This is the rate of closed cases per year and area\n",
    "rate_df = closed_case_rates.join(area_totals, [\"Year\", \"Area Name\"], how=\"inner\") \\\n",
    "    .withColumn(\"closed_case_rate\", (col(\"closed_count\") / col(\"total_count\")) * 100)\n",
    "\n",
    "# We partition to order in descending order over the year\n",
    "window_spec = Window.partitionBy(\"Year\").orderBy(desc(\"closed_case_rate\"))\n",
    "# We calculate the rank\n",
    "ranked_df = rate_df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "\n",
    "# Add rankings per year (This was a test)\n",
    "# rank_test = rate_df.withColumn(\"rnk\", dense_rank().over(window_spec))\n",
    "# rank_test.orderBy(\"Year\", desc(\"closed_case_rate\")).show(1000, truncate=False)\n",
    "\n",
    "# Filter for top 3 per year\n",
    "top3_df = ranked_df.filter(col(\"rank\") <= 3).orderBy(\"Year\", \"rank\")\n",
    "\n",
    "# Rename columns to match example output\n",
    "formatted_df = top3_df.select(\n",
    "    col(\"Year\").alias(\"year\"),\n",
    "    col(\"Area Name\").alias(\"precinct\"),\n",
    "    col(\"closed_case_rate\"),\n",
    "    col(\"rank\").alias(\"#\")\n",
    ")\n",
    "# Show results\n",
    "print(\"DataFrame Results:\")\n",
    "top3_df.select(\"Year\", \"Area Name\", \"closed_case_rate\", \"rank\").show(1000, truncate=False)\n",
    "\n",
    "end_time_df = time.time()\n",
    "df_elapsed_time = end_time_df - start_time_df\n",
    "print(f\"DataFrame Time taken: {df_elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7e08007",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o619.parquet.\n",
      ": java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 2E55JT6FDG525ZY5; S3 Extended Request ID: I/MX4tzHK6Ww5ODRPNYznvrs487U2TaRATmEH8Htf1Gqb+aVzt0mBv8M7tVufUj6s/XepfkYhJc=; Proxy: null), S3 Extended Request ID: I/MX4tzHK6Ww5ODRPNYznvrs487U2TaRATmEH8Htf1Gqb+aVzt0mBv8M7tVufUj6s/XepfkYhJc=\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:429)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolderUsingFolderObject(Jets3tNativeFileSystemStore.java:255)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolder(Jets3tNativeFileSystemStore.java:218)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:554)\n",
      "\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1869)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.exists(EmrFileSystem.java:408)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:125)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:126)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:157)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$10(SQLExecution.scala:220)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:220)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:405)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:114)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:520)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:520)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:297)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:293)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:496)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:101)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:99)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:164)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:874)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:402)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:362)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:241)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:806)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 2E55JT6FDG525ZY5; S3 Extended Request ID: I/MX4tzHK6Ww5ODRPNYznvrs487U2TaRATmEH8Htf1Gqb+aVzt0mBv8M7tVufUj6s/XepfkYhJc=; Proxy: null), S3 Extended Request ID: I/MX4tzHK6Ww5ODRPNYznvrs487U2TaRATmEH8Htf1Gqb+aVzt0mBv8M7tVufUj6s/XepfkYhJc=\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5558)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5505)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:1002)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:114)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:141)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:196)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:420)\n",
      "\t... 53 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1732639283265_3767/container_1732639283265_3767_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 1721, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1732639283265_3767/container_1732639283265_3767_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1732639283265_3767/container_1732639283265_3767_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1732639283265_3767/container_1732639283265_3767_01_000001/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o619.parquet.\n",
      ": java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 2E55JT6FDG525ZY5; S3 Extended Request ID: I/MX4tzHK6Ww5ODRPNYznvrs487U2TaRATmEH8Htf1Gqb+aVzt0mBv8M7tVufUj6s/XepfkYhJc=; Proxy: null), S3 Extended Request ID: I/MX4tzHK6Ww5ODRPNYznvrs487U2TaRATmEH8Htf1Gqb+aVzt0mBv8M7tVufUj6s/XepfkYhJc=\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:429)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolderUsingFolderObject(Jets3tNativeFileSystemStore.java:255)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolder(Jets3tNativeFileSystemStore.java:218)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:554)\n",
      "\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1869)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.exists(EmrFileSystem.java:408)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:125)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:126)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:157)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$10(SQLExecution.scala:220)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:220)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:405)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:114)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:520)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:520)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:297)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:293)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:496)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:101)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:99)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:164)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:874)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:402)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:362)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:241)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:806)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 2E55JT6FDG525ZY5; S3 Extended Request ID: I/MX4tzHK6Ww5ODRPNYznvrs487U2TaRATmEH8Htf1Gqb+aVzt0mBv8M7tVufUj6s/XepfkYhJc=; Proxy: null), S3 Extended Request ID: I/MX4tzHK6Ww5ODRPNYznvrs487U2TaRATmEH8Htf1Gqb+aVzt0mBv8M7tVufUj6s/XepfkYhJc=\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5558)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5505)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:1002)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:114)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:141)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:196)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:420)\n",
      "\t... 53 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, desc, rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1. Αρχικοποίηση SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSVvsParquet\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. S3 paths \n",
    "csv_2019_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "csv_2020_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "parquet_output_path = \"s3://buckets/groups-bucket-dblab-905418150721/group23/outputs/query2/output_parquet_singlefile\"  # Εδώ θα γράψουμε 1 .parquet\n",
    "parquet_input_path  = \"s3://buckets/groups-bucket-dblab-905418150721/group23/outputs/query2/output_parquet_singlefile\"  # Για να το ξαναφορτώσουμε\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Function for \"top 3 closed cases per year\"\n",
    "# ----------------------------------------------------------------------------\n",
    "def compute_top3_closed(df):\n",
    "    \"\"\"\n",
    "    df: DataFrame με columns: [Date Rptd, Status Desc, Area Name, ...]\n",
    "    Επιστρέφει DataFrame με Year, Area Name, closed_case_rate, rank\n",
    "    όπου \"closed\" = οτιδήποτε εκτός από \"UNK\" και \"Invest Cont\"\n",
    "    και total = όλα τα records.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Εξαγωγή έτους απλό, π.χ. substr(7,4) ή καλύτερα χρήση regexp/ημερομηνίας\n",
    "    df = df.withColumn(\"Year\", col(\"Date Rptd\").substr(7,4))\n",
    "    \n",
    "    # Closed = Status Desc != 'UNK' & != 'Invest Cont'\n",
    "    closed_df = df.filter((col(\"Status Desc\") != \"UNK\") &\n",
    "                          (col(\"Status Desc\") != \"Invest Cont\"))\n",
    "    total_df  = df  # όλα\n",
    "    \n",
    "    # Μετράμε closed / total per (Year, Area Name)\n",
    "    closed_agg = closed_df.groupBy(\"Year\", \"Area Name\").count().alias(\"closed_count\")\n",
    "    closed_agg = closed_agg.withColumnRenamed(\"count\", \"closed_count\")\n",
    "\n",
    "    total_agg = total_df.groupBy(\"Year\", \"Area Name\").count().alias(\"total_count\")\n",
    "    total_agg = total_agg.withColumnRenamed(\"count\", \"total_count\")\n",
    "    \n",
    "    # Join και υπολογισμός ποσοστού\n",
    "    joined = closed_agg.join(total_agg, [\"Year\", \"Area Name\"], how=\"inner\") \\\n",
    "                       .withColumn(\"closed_case_rate\",\n",
    "                                   (col(\"closed_count\")*100.0 / col(\"total_count\")))\n",
    "    \n",
    "    # Παράθυρο για κατάταξη top 3 ανά Year\n",
    "    w = Window.partitionBy(\"Year\").orderBy(desc(\"closed_case_rate\"))\n",
    "    ranked = joined.withColumn(\"rank\", rank().over(w))\n",
    "    \n",
    "    top3 = ranked.filter(col(\"rank\") <= 3).orderBy(\"Year\", \"rank\")\n",
    "    return top3\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# A) Φόρτωση από CSV & υπολογισμός χρόνου\n",
    "# ----------------------------------------------------------------------------\n",
    "start_time_csv = time.time()\n",
    "\n",
    "csv_2019_df = spark.read.csv(csv_2019_path, header=True, inferSchema=True)\n",
    "csv_2020_df = spark.read.csv(csv_2020_path, header=True, inferSchema=True)\n",
    "\n",
    "df_csv = csv_2019_df.union(csv_2020_df)\n",
    "\n",
    "# Ερώτημα top3\n",
    "top3_from_csv = compute_top3_closed(df_csv)\n",
    "\n",
    "# Δείξε αποτελέσματα (προαιρετικά)\n",
    "top3_from_csv.show(10, truncate=False)\n",
    "\n",
    "end_time_csv = time.time()\n",
    "csv_elapsed = end_time_csv - start_time_csv\n",
    "print(f\"Time (load CSV + compute query) = {csv_elapsed:.2f} sec\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# B) Αποθήκευση σε Parquet (ένα μόνο αρχείο) & Επαναφόρτωση\n",
    "# ----------------------------------------------------------------------------\n",
    "# Παράδειγμα αποθήκευσης ως Parquet σε ΕΝΑ αρχείο:\n",
    "df_csv.coalesce(1).write.mode(\"overwrite\").parquet(parquet_output_path)\n",
    "# Το coalesce(1) αναγκάζει τη δημιουργία ενός μόνο part αρχείου. \n",
    "# Μεγάλα datasets -> δεν συνιστάται για production, αλλά οκ για την άσκηση.\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# C) Φόρτωση από Parquet & υπολογισμός χρόνου\n",
    "# ----------------------------------------------------------------------------\n",
    "start_time_parquet = time.time()\n",
    "\n",
    "df_parquet = spark.read.parquet(parquet_input_path)\n",
    "top3_from_parquet = compute_top3_closed(df_parquet)\n",
    "top3_from_parquet.show(10, truncate=False)\n",
    "\n",
    "end_time_parquet = time.time()\n",
    "parquet_elapsed = end_time_parquet - start_time_parquet\n",
    "print(f\"Time (load Parquet + compute query) = {parquet_elapsed:.2f} sec\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# D) Σύγκριση Χρόνων\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"===========================================\")\n",
    "print(f\"CSV total time:      {csv_elapsed:.2f} sec\")\n",
    "print(f\"Parquet total time:  {parquet_elapsed:.2f} sec\")\n",
    "print(\"===========================================\")\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
