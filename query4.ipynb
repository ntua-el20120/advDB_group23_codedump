{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb9896c4-3e38-478d-9036-e2b0f2a745fe",
   "metadata": {},
   "source": [
    "Output URI\n",
    "s3://groups-bucket-dblab-905418150721/group23/outputs/query4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f3b1483",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>14</td><td>application_1738075734771_0015</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0015/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-139.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0015_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '2', 'spark.executor.cores': '1', 'spark.executor.memory': '2g', 'spark.driver.memory': '2g'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>4</td><td>application_1738075734771_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-104.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0005_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>7</td><td>application_1738075734771_0008</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0008/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-104.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0008_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>10</td><td>application_1738075734771_0011</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0011/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-210.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0011_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>11</td><td>application_1738075734771_0012</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0012/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-100.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0012_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>14</td><td>application_1738075734771_0015</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0015/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-139.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0015_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.cores\": \"1\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.driver.memory\": \"2g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5255acd5-49d7-4efa-98f3-2b22d59f07ab",
   "metadata": {},
   "source": [
    "## Πρώτος τρόπος\n",
    "Ακολουθούμε την εξής μεθοδολογία:\n",
    "- Φορτώνουμε το census και φιλτράρουμε ο,τι δεν έχει geometry\n",
    "- Κρατάμε μόνο τις στήλες που χρειαζόμαστε για το average income per person\n",
    "- Κάνουμε group by ZCTA10, COMM κρατώντας άθροισμα housing και population\n",
    "- Ανοίγουμε το αρχείο με το household income per zip code και μετατρέπουμε το income σε double\n",
    "- Κάνουμε join τον πίνακα με (zcta10,comm) με τον πίνακα Inocme βάσει zip code\n",
    "- Κάνουμε group by comm για να βρούμε το average income per person per community\n",
    "- Κάνουμε order by average income είτε descending για τα top είτε ascending για τα bottom και κρατάμε μόνο τα πρώτα 3\n",
    "Πλέον έχουμε βρει τα communities που μας νοιάζουν και κάνουμε τις ίδιες πράξεις για τα top και bottom οπότε τα αναφέρουμε μαζί:\n",
    "- Ανοίγουμε το αρχείο με τα εγκλήματα, κάνουμε filter όσα ήταν εκτός του 15 και όσα δεν έχουν γεωμετρία.\n",
    "- Ενώνουμε τον πίνακα με τα blocks για να βρούμε σε ποιο community είναι κάθε έγκλημα\n",
    "- Ενώνουμε τον νέο πίνακα με τα αποτελέσματα από το Income για να βρούμε μόνο τα εγκλήματα που μας ενδιαφέρουν.\n",
    "- Κάνουμε group by Vict Descent και μετράμε rows\n",
    "- Ανοίγουμε το αρχείο με το race description και το κάνουμε join με τα αποτελέσματά μας για να πάρουμε την περιγραφή του race\n",
    "- Κάνουμε order by # desc για να τυπώσουμε με φθίνουσα σειρά εγκλημάτων."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9709806",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6928c711df45475f94ee50b9060385ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Interrupted by user\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number, expr\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import (\n",
    "    col, substring, rank, count, desc, sum as spark_sum\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"QUERY 4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "\n",
    "crime19path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "income_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\"\n",
    "census_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "re_codes_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# load blocks\n",
    "census = sedona.read.format('geojson').option(\"multiLine\", \"true\").load(census_path).selectExpr(\"explode(features) as features\").select(\"features.*\")\n",
    "census = census.select([col(f\"properties.{col_name}\").alias(col_name) for col_name in census.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\n",
    "\n",
    "census = census.filter(col(\"geometry\").isNotNull())\n",
    "\n",
    "\n",
    "\n",
    "# group by ZCTA10, COMM\n",
    "# GET POP 2010 and HOUSING 10 per zcta10, comm\n",
    "blocks = (\n",
    "    census.filter((col(\"CITY\") == \"Los Angeles\"))\n",
    "    .select(\n",
    "        \"ZCTA10\",\n",
    "        \"COMM\",\n",
    "        \"POP_2010\",\n",
    "        \"HOUSING10\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"blocks:\")\n",
    "blocks.show(10)\n",
    "\n",
    "zip_pop_all = (\n",
    "    blocks\n",
    "    .groupBy(\"ZCTA10\", \"COMM\")\n",
    "    .agg(\n",
    "        F.sum(\"POP_2010\").alias(\"population\"),\n",
    "        F.sum(\"HOUSING10\").alias(\"houses_per_zip_code_in_community\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"zip_pop_all\")\n",
    "zip_pop_all.show(10)\n",
    "\n",
    "# load income\n",
    "income = spark.read.format('csv').options(header='true').load(income_path)\n",
    "# normalise income\n",
    "\n",
    "norm_inc = (\n",
    "    income\n",
    "    .select(\n",
    "        F.col(\"Zip Code\").alias(\"zip_code\"),\n",
    "        F.regexp_replace(\n",
    "            F.regexp_replace(F.col(\"Estimated Median Income\"), \"\\\\$\", \"\"),\n",
    "            \",\", \"\"\n",
    "        ).cast(\"int\").alias(\"med_inc\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"norm_inc\")\n",
    "norm_inc.show(10)\n",
    "\n",
    "# join the tables to find money per zcta10, comm\n",
    "income_data = (\n",
    "        zip_pop_all\n",
    "        .join(\n",
    "            norm_inc,\n",
    "            zip_pop_all.ZCTA10 == norm_inc.zip_code,\n",
    "            \"inner\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# group by comm and find average income per person\n",
    "income_result = income_data.groupBy(\"COMM\").agg(\n",
    "    F.expr(\"CAST(SUM(med_inc * houses_per_zip_code_in_community) / SUM(population) AS INT)\").alias(\"average_income\")\n",
    ")\n",
    "print(\"income result:\")\n",
    "income_result.show(10)\n",
    "\n",
    "# keep only top 3 and only the comm row A\n",
    "top_3 = income_result.orderBy(F.desc(\"average_income\")).limit(3).select(\"COMM\")\n",
    "\n",
    "print(\"top3 bottom3\")\n",
    "# also keep only the bottom 3 and only comm row B\n",
    "bottom_3 = income_result.orderBy(\"average_income\").limit(3).select(\"COMM\")\n",
    "\n",
    "top_3.show(10)\n",
    "bottom_3.show(10)\n",
    "\n",
    "\n",
    "# load crimes filter 2015\n",
    "crime19 = spark.read.format('csv').options(header='true').load(crime19path)\n",
    "crime19 = crime19.filter((col(\"LAT\").isNotNull()) & (col(\"LAT\") != 0) & (col(\"LON\").isNotNull()) & (col(\"LON\") != 0))\n",
    "crime19 = crime19.withColumn(\"point\", expr(\"ST_Point(LON, LAT)\"))\n",
    "crime_2015_df = crime19.withColumn(\n",
    "    \"Year\",\n",
    "    substring(col(\"Date Rptd\"), 7, 4)\n",
    ").filter(col(\"Year\") == \"2015\")\n",
    "\n",
    "print(\"crimes 2015\")\n",
    "crime_2015_df.show(10)\n",
    "\n",
    "# join crimes with blocks on inside polygon to get the comm\n",
    "crimes_in_block = (\n",
    "        census\n",
    "        .join(\n",
    "            crime_2015_df,\n",
    "            # The condition for your spatial join\n",
    "            ST_Within(F.col(\"point\"), F.col(\"geometry\")),\n",
    "            \"inner\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(f\"crimes in block {crimes_in_block.count()} rows:\")\n",
    "crimes_in_block.show(10)\n",
    "\n",
    "# keep only victim descent, comm\n",
    "crimes_in_block_filtered = crimes_in_block.select(\"Vict Descent\", \"COMM\")\n",
    "\n",
    "# join with A \n",
    "joined_top_3 = crimes_in_block_filtered.join(top_3, on=\"COMM\", how=\"inner\")\n",
    "\n",
    "# join with B\n",
    "joined_bottom_3 = crimes_in_block_filtered.join(bottom_3, on=\"COMM\", how=\"inner\")\n",
    "\n",
    "# aggregate on victim descent with count A and B\n",
    "result_top = joined_top_3.groupBy(\"Vict Descent\").agg(\n",
    "    F.count(\"*\").alias(\"#\")\n",
    ")\n",
    "\n",
    "result_bottom = joined_bottom_3.groupBy(\"Vict Descent\").agg(\n",
    "    F.count(\"*\").alias(\"#\")\n",
    ")\n",
    "\n",
    "# Open Vict Desc\n",
    "re_codes_df = spark.read.csv(re_codes_path, header=True, inferSchema=True)\n",
    "\n",
    "# join to rename columns\n",
    "\n",
    "renamed_top = result_top.join(\n",
    "    re_codes_df,\n",
    "    result_top[\"Vict Descent\"] == re_codes_df[\"Vict Descent\"],\n",
    "    how=\"inner\"\n",
    ").select(\n",
    "    re_codes_df[\"Vict Descent Full\"].alias(\"Victim Descent\"),\n",
    "    result_top[\"#\"]\n",
    ")\n",
    "\n",
    "renamed_bottom = result_bottom.join(\n",
    "    re_codes_df,\n",
    "    result_bottom[\"Vict Descent\"] == re_codes_df[\"Vict Descent\"],\n",
    "    how=\"inner\"\n",
    ").select(\n",
    "    re_codes_df[\"Vict Descent Full\"].alias(\"Victim Descent\"),\n",
    "    result_bottom[\"#\"]\n",
    ")\n",
    "\n",
    "# order in descending\n",
    "ordered_top = renamed_top.orderBy(F.desc(\"#\"))\n",
    "ordered_bottom = renamed_bottom.orderBy(F.desc(\"#\"))\n",
    "\n",
    "\n",
    "ordered_top.show(100)\n",
    "ordered_bottom.show(100)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Calculated in {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f2c1aa-08a2-4862-a470-b3a6191906d5",
   "metadata": {},
   "source": [
    "## Δεύτερος Τρόπος\n",
    "Αρχικά θέλουμε να βρούμε τα 3 communities με το μεγαλύτερο και τα 3 communities με το μικρότερο εισόδημα:\n",
    "\n",
    "- Φορτώνουμε το csv με τα blocks.\n",
    "- Φιλτράρουμε όσα δεν έχουν geometry και κρατάμε μόνο τα columns ZCTA10, COMM, POP_2010, HOUSING10, geometry.\n",
    "- Φορτώνουμε το dataset με το μέσο household income per zipcode και κανονικοποιούμε το average income σε double.\n",
    "- Κάνουμε join τα blocks με το income για να βρούμε το average household income για κάθε block.\n",
    "- Υπολογίζουμε το average income per person για κάθε community κάνοντας aggregation στο COMΜ\n",
    "- Στο aggregation κατά COMM κάνουμε και union στα blocks με την εντολή ST_Union_Aggr.\n",
    "- Κάνουμε orderBy average income και κρατάμε 3 rows (για top 3 descending για bottom 3 ascending)\n",
    "Πλέον έχουμε τους πίνακες με τα top και bottom 3 communitites και τις γεωμετρίες μέσα στις οποίες αν είναι τα εγκλήματα τα κρατάμε. Κάνουμε την ίδια δουλειά για τα top και bottom 3 ανεξάρτητα οπότε αναφέρουμε μόνο για το top 3.\n",
    "\n",
    "- Φορτώνουμε τον πίνακα με τα εγκλήματα από το 2015 μέρχρι το 2019, φιλτράρουμε όσα δεν έχουν συντεταγμένες και κρατάμε μόνο αυτά που έγιναν το 2015. Επιπλέον φτιάχνουμε ST_Point με τις συντεταγμένες του κάθε ενός\n",
    "- Κάνουμε join τον πίνακα με τις γεωμετρίες των top 3 communities με τον πίνακα των εγκλημάτων με join condition να είναι το έγκλημα μέσα σε κάποιο από τα communities. (θα μπορούσαμε να κάνουμε άλλο ένα union και να συγκρίνουμε μόνο με μια τιμή)\n",
    "- Για κάθε έγκλημα κρατάμε μόνο το Vict Descent και το Comm (το comm δεν χρειαζόταν αλλά θέλαμε να τυπώσουμε κάποια ενδιάμεσα αποτελέσματα).\n",
    "- Κάνουμε τα δεδομένα group By Vict Descent και μετράμε rows.\n",
    "- Ανοίγουμε το αρχείο με τα race codes.\n",
    "- Κάνουμε join τα αποτελέσματά μας με το αρχείο race codes για να πάρουμε τα ονόματα των races\n",
    "- Κάνουμε order by # descending για να τυπώσουμε τα αποτελέσματα με φθίνουσα σειρά.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72e84302",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top3 bottom3\n",
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|               White|695|\n",
      "|               Other| 86|\n",
      "|Hispanic/Latin/Me...| 77|\n",
      "|             Unknown| 49|\n",
      "|               Black| 43|\n",
      "|         Other Asian| 22|\n",
      "|             Chinese|  1|\n",
      "|American Indian/A...|  1|\n",
      "+--------------------+---+\n",
      "\n",
      "+--------------------+----+\n",
      "|      Victim Descent|   #|\n",
      "+--------------------+----+\n",
      "|Hispanic/Latin/Me...|3342|\n",
      "|               Black|1127|\n",
      "|               White| 428|\n",
      "|               Other| 252|\n",
      "|         Other Asian| 138|\n",
      "|             Unknown|  30|\n",
      "|American Indian/A...|  23|\n",
      "|              Korean|   4|\n",
      "|            Filipino|   3|\n",
      "|             Chinese|   3|\n",
      "|         AsianIndian|   1|\n",
      "|           Guamanian|   1|\n",
      "+--------------------+----+\n",
      "\n",
      "Calculated in 91.66757154464722 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number, expr, year\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import (\n",
    "    col, substring, rank, count, desc, sum as spark_sum, to_date\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"QUERY 4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "\n",
    "crime19path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "income_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\"\n",
    "census_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "re_codes_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# load blocks\n",
    "census = sedona.read.format('geojson').option(\"multiLine\", \"true\").load(census_path).selectExpr(\"explode(features) as features\").select(\"features.*\")\n",
    "census = census.select([col(f\"properties.{col_name}\").alias(col_name) for col_name in census.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\n",
    "\n",
    "census = census.filter(col(\"geometry\").isNotNull())\n",
    "\n",
    "\n",
    "\n",
    "# group by ZCTA10, COMM\n",
    "# GET POP 2010 and HOUSING 10 per zcta10, comm\n",
    "blocks = (\n",
    "    census.filter((col(\"CITY\") == \"Los Angeles\"))\n",
    "    .select(\n",
    "        \"ZCTA10\",\n",
    "        \"COMM\",\n",
    "        \"POP_2010\",\n",
    "        \"HOUSING10\",\n",
    "        \"geometry\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# print(\"blocks:\")\n",
    "# blocks.show(10)\n",
    "\n",
    "\n",
    "# load income\n",
    "income = spark.read.format('csv').options(header='true').load(income_path)\n",
    "# normalise income\n",
    "\n",
    "norm_inc = (\n",
    "    income\n",
    "    .select(\n",
    "        F.col(\"Zip Code\").alias(\"zip_code\"),\n",
    "        F.regexp_replace(\n",
    "            F.regexp_replace(F.col(\"Estimated Median Income\"), \"\\\\$\", \"\"),\n",
    "            \",\", \"\"\n",
    "        ).cast(\"double\").alias(\"med_inc\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# print(\"norm_inc\")\n",
    "# norm_inc.show(10)\n",
    "\n",
    "# join the tables to find money per zcta10, comm\n",
    "income_data = (\n",
    "        blocks\n",
    "        .join(\n",
    "            norm_inc,\n",
    "            blocks.ZCTA10 == norm_inc.zip_code,\n",
    "            \"inner\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# group by comm and find average income per person\n",
    "income_result = income_data.groupBy(\"COMM\").agg(\n",
    "    F.expr(\"CAST(SUM(med_inc * HOUSING10) / SUM(POP_2010) AS INT)\").alias(\"average_income\"),\n",
    "    ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    ")\n",
    "# print(\"income result:\")\n",
    "# income_result.show(10)\n",
    "\n",
    "# keep only top 3 and only the comm row A\n",
    "top_3 = income_result.orderBy(F.desc(\"average_income\")).limit(3).select(\"COMM\", \"geometry\")\n",
    "\n",
    "print(\"top3 bottom3\")\n",
    "# also keep only the bottom 3 and only comm row B\n",
    "bottom_3 = income_result.orderBy(\"average_income\").limit(3).select(\"COMM\", \"geometry\")\n",
    "\n",
    "# top_3.show(10)\n",
    "# bottom_3.show(10)\n",
    "\n",
    "\n",
    "# load crimes filter 2015\n",
    "crime19 = spark.read.format('csv').options(header='true').load(crime19path)\n",
    "crime19 = crime19.filter((col(\"LAT\").isNotNull()) & (col(\"LAT\") != 0) & (col(\"LON\").isNotNull()) & (col(\"LON\") != 0))\n",
    "crime19 = crime19.withColumn(\"point\", expr(\"ST_Point(LON, LAT)\"))\n",
    "crime_2015_df = crime19.withColumn(\"Date\", to_date(col(\"DATE OCC\"), \"MM/dd/yyyy hh:mm:ss a\")).filter(year(col(\"Date\")) == 2015)\n",
    "\n",
    "# print(\"crimes 2015\")\n",
    "# crime_2015_df.show(10)\n",
    "\n",
    "# join crimes with blocks on inside polygon to get the comm\n",
    "crimes_in_top = (\n",
    "        top_3\n",
    "        .join(\n",
    "            crime_2015_df,\n",
    "            # The condition for your spatial join\n",
    "            ST_Within(F.col(\"point\"), F.col(\"geometry\")),\n",
    "            \"inner\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "crimes_in_bottom = (\n",
    "        bottom_3\n",
    "        .join(\n",
    "            crime_2015_df,\n",
    "            # The condition for your spatial join\n",
    "            ST_Within(F.col(\"point\"), F.col(\"geometry\")),\n",
    "            \"inner\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# keep only victim descent, comm\n",
    "joined_top_3 = crimes_in_top.select(\"Vict Descent\", \"COMM\")\n",
    "joined_bottom_3 = crimes_in_bottom.select(\"Vict Descent\", \"COMM\")\n",
    "\n",
    "\n",
    "# aggregate on victim descent with count A and B\n",
    "result_top = joined_top_3.groupBy(\"Vict Descent\").agg(\n",
    "    F.count(\"*\").alias(\"#\")\n",
    ")\n",
    "\n",
    "result_bottom = joined_bottom_3.groupBy(\"Vict Descent\").agg(\n",
    "    F.count(\"*\").alias(\"#\")\n",
    ")\n",
    "\n",
    "# Open Vict Desc\n",
    "re_codes_df = spark.read.csv(re_codes_path, header=True, inferSchema=True)\n",
    "\n",
    "# join to rename columns\n",
    "\n",
    "renamed_top = result_top.join(\n",
    "    re_codes_df,\n",
    "    result_top[\"Vict Descent\"] == re_codes_df[\"Vict Descent\"],\n",
    "    how=\"inner\"\n",
    ").select(\n",
    "    re_codes_df[\"Vict Descent Full\"].alias(\"Victim Descent\"),\n",
    "    result_top[\"#\"]\n",
    ")\n",
    "\n",
    "renamed_bottom = result_bottom.join(\n",
    "    re_codes_df,\n",
    "    result_bottom[\"Vict Descent\"] == re_codes_df[\"Vict Descent\"],\n",
    "    how=\"inner\"\n",
    ").select(\n",
    "    re_codes_df[\"Vict Descent Full\"].alias(\"Victim Descent\"),\n",
    "    result_bottom[\"#\"]\n",
    ")\n",
    "\n",
    "# order in descending\n",
    "ordered_top = renamed_top.orderBy(F.desc(\"#\"))\n",
    "ordered_bottom = renamed_bottom.orderBy(F.desc(\"#\"))\n",
    "\n",
    "\n",
    "ordered_top.show(100)\n",
    "ordered_bottom.show(100)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Calculated in {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b019d76-d27b-4c66-bb46-2a20c5a8a035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
