{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f417941a-b02f-41df-b957-1d239f654f87",
   "metadata": {},
   "source": [
    "Output URI\n",
    "s3://groups-bucket-dblab-905418150721/group23/outputs/query3/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fe631d-98c9-45ce-81ad-0576fb8b609a",
   "metadata": {},
   "source": [
    "Χρησιμοποιώντας ως αναφορά τα δεδομένα της απογραφής 2010 για τον πληθυσμό και εκείνα της απογραφής του 2015 για το εισόδημα ανα νοικοκυριό, να υπολογίσετε για κάθε περιοχή του Los Angeles\n",
    "τα παρακάτω: Το μέσο ετήσιο εισόδημα ανά άτομο και την αναλογία συνολικού αριθμού εγκλημάτων\n",
    "ανά άτομο. Τα αποτελέσματα να συγκεντρωθούν σε ένα πίνακα.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f8a930-10b5-441e-a8c7-761fcb9ef06d",
   "metadata": {},
   "source": [
    "## Ανάλυση Δεδομένων ##\n",
    "\n",
    "Αρχικά τυπώνουμε το Census_Blocks_fields.csv για να δούμε τα πεδία που χρειαζόμαστε"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88948a6b-ea22-4025-9dd5-47b69bb97acf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3623</td><td>application_1732639283265_3579</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3579/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-119.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3579_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+-------------------------------------------------------------------------+\n",
      "|field        |type    |meaning                                                                  |\n",
      "+-------------+--------+-------------------------------------------------------------------------+\n",
      "|BG10         |string  |7-digit block group number (2010)                                        |\n",
      "|BG10FIP10    |string  |Combination of BG10 and LA_FIP10 (Los Angeles County FIP code)           |\n",
      "|BG12         |string  |7-digit block group number (2012)                                        |\n",
      "|CB10         |string  |4-digit census block number                                              |\n",
      "|CEN_FIP13    |string  |-                                                                        |\n",
      "|CITY         |string  |Incorporated city name                                                   |\n",
      "|CITYCOM      |string  |City/Community name label                                                |\n",
      "|COMM         |string  |Unincorporated area community name and LA City neighborhood              |\n",
      "|CT10         |string  |6-digit census tract number (2010)                                       |\n",
      "|CT12         |string  |6-digit census tract number (2012)                                       |\n",
      "|CTCB10       |string  |Combination of CT10 and CB10 (4-digit census block number)               |\n",
      "|HD_2012      |long    |Health District number                                                   |\n",
      "|HD_NAME      |string  |Health District name                                                     |\n",
      "|HOUSING10    |long    |2010 housing (PL 94-171 Redistricting Data Summary File - Total Housing) |\n",
      "|LA_FIP10     |string  |Los Angeles County FIP code                                              |\n",
      "|OBJECTID     |long    |-                                                                        |\n",
      "|POP_2010     |long    |Population (PL 94-171 Redistricting Data Summary File - Total Population)|\n",
      "|PUMA10       |string  |-                                                                        |\n",
      "|SPA_2012     |long    |Service Planning Area number (2012)                                      |\n",
      "|SPA_NAME     |string  |Service Planning Area name                                               |\n",
      "|SUP_DIST     |string  |Supervisorial District number                                            |\n",
      "|SUP_LABEL    |string  |Supervisorial District label                                             |\n",
      "|ShapeSTArea  |double  |-                                                                        |\n",
      "|ShapeSTLength|double  |-                                                                        |\n",
      "|ZCTA10       |string  |Zip Code Tabulation Area                                                 |\n",
      "|geometry     |geometry|Geometry of the block                                                    |\n",
      "+-------------+--------+-------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Block Label Meaning\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "labels_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks_fields.csv\"\n",
    "labels = spark.read.format('csv').options(header='true').load(labels_path)\n",
    "labels.show(30, truncate = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae37a4ab-f0a6-4e6e-b43e-3521e8b83ebb",
   "metadata": {},
   "source": [
    "Κάνουμε τις εξής παραδοχές και αποφασίζουμε να χρησιμοποιήσουμε τα εξής πεδία:\n",
    "\n",
    "COMM: Η περιοχή στην οποία ανήκει το μπλοκ. Εν τέλει θα κάνουμε aggregate όλα τα αποτελέσματα βάσει αυτού\n",
    "\n",
    "HOUSING10: Πόσα occupied σπίτια υπάρχουν στο block. Το χρειαζόμαστε για να υπολογίσουμε το μέσο εισόδημα κάθε κατοίκου. Στο αντίστοιχο csv για το income μας δίνεται average household      income, όχι ανά κάτοικο, οπότε πρέπει να κάνουμε την υπόθεση ότι το total housing που μας δίνεται είναι πόσα σπίτια είναι occupied στο block. Θα ήταν λάθος να θεωρήσουμε το average income στο άλλο csv να είναι average ανά κάτοικο και να κάνουμε απλώς weighted average στους κατοίκους του community βάσει zip-code.\n",
    "\n",
    "POP_2010: Μας δίνεται ότι αυτός είναι ο πληθυσμός του block. Θα το χρησιμοποιήσουμε για να κάνουμε aggregate τον συνολικό πληθυσμό του community και να μετρήσουμε τα εγκλήματα ανά άτομο και το income ανά άτομο στο community. Δεδομένου ότι κάθε θα έχουμε περισσότερους κατοίκους από ότι housing, περιμένουμε το income per person να είναι μικρότερο από το income per household\n",
    "\n",
    "ZCTA10: Το χρησιμοποιούμε για ταχυδρομικό κώδικα. Θα κάνουμε join τα block με το household income βάσει zip code για να υπολογίσουμε τα συνολικά χρήματα του zipcode πριν κάνουμε aggregation βάσει community\n",
    "\n",
    "geometry: Μας δίνει ένα πολύγωνο, το εσωτερικό του οποίου θεωρείται μέσα στο block. Θα το χρησιμοποιήσουμε για να τοποθετήσουμε τα εγκλήματα μέσα σε block. Αφού μετρήσουμε τα εγκλήματα ανα block μπορούμε να κάνουμε aggregation σε community."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c78acc-f226-4add-9c7e-031dcd460113",
   "metadata": {},
   "source": [
    "Μετράμε τα blocks, τα zipcodes, και τα communities για να ξέρουμε τις τάξεις μεγέθους (και αν μπορούμε να τυπώσουμε ολόκληρο τον τελικό πίνακα)\n",
    "Βρίσκουμε επίσης το primary key του block για τα queries μετά"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c336f6-3025-4df3-a0d1-a726aae128ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blocks: 109279\n",
      "Number of BG10, CB10: 109279\n",
      "Number of Zip Codes: 301\n",
      "Number of Communities: 323"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"communities\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "\n",
    "census_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "census = sedona.read.format('geojson').option(\"multiLine\", \"true\").load(census_path).selectExpr(\"explode(features) as features\").select(\"features.*\")\n",
    "census = census.select([col(f\"properties.{col_name}\").alias(col_name) for col_name in census.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\n",
    "\n",
    "census = census.filter(col(\"geometry\").isNotNull())\n",
    "census.createOrReplaceTempView(\"census\")\n",
    "\n",
    "query = \"\"\" \n",
    "        SELECT *\n",
    "        FROM census\n",
    "        \"\"\"\n",
    "data = spark.sql(query)\n",
    "row_count = data.count()\n",
    "print(f\"Number of blocks: {row_count}\")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT BG10, CB10\n",
    "    FROM census\n",
    "    GROUP BY BG10, CB10\n",
    "    \"\"\"\n",
    "\n",
    "data = spark.sql(query)\n",
    "row_count = data.count()\n",
    "print(f\"Number of BG10, CB10: {row_count}\")\n",
    "\n",
    "\n",
    "query = \"\"\" \n",
    "        SELECT ZCTA10\n",
    "        FROM census\n",
    "        GROUP BY ZCTA10\n",
    "        \"\"\"\n",
    "\n",
    "data = spark.sql(query)\n",
    "row_count = data.count()\n",
    "print(f\"Number of Zip Codes: {row_count}\")\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT COMM\n",
    "        FROM census\n",
    "        GROUP BY COMM        \n",
    "        \"\"\"\n",
    "\n",
    "data = spark.sql(query)\n",
    "row_count = data.count()\n",
    "print(f\"Number of Communities: {row_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba37fd5b-24b1-4ea7-8a7d-532b5d25ee71",
   "metadata": {},
   "source": [
    "Παρατηρούμε ότι έχουμε περισσότερα Communities από ότι zipcodes\n",
    "Αυτό σημαίνει ότι υπάρχουν εγγραφές με το ίδιο zip code και διαφορετικά communities ή ότι υπάρχουν εγγραφές με NULL communities\n",
    "Δεν μας πειράζει αυτό ενώ θα βγάζουμε το αποτέλεσμα, αφού το zip code το χρειαζόμαστε μόνο για τα βάρη του income, αλλά θα πρέπει να προσέξουμε να μην κάνουμε group by με το zip code.\n",
    "\n",
    "Παρατηρούμε επίσης ότι το μέγεθος του τελικού πίνακα θα έχει 323 γραμμές. Το χρησιμοποιούμε ως ένα check για το αν έχουμε κάνει λάθος και δεν τυπώνουμε ολόκληρο τον τελικό πίνακα."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbc1c0f-24ba-402f-99ed-8fcc521ca795",
   "metadata": {},
   "source": [
    "Ελέγχουμε επίσης τον αριθμό των zipcodes στο income table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a6749e6-eb45-4776-ac17-bb9c96ede771",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Zip Codes In income: 284\n",
      "Number of Common Zip Codes In income: 283"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"QUERY 3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "income_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\"\n",
    "census_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "\n",
    "income = spark.read.format('csv').options(header='true').load(income_path)\n",
    "census = sedona.read.format('geojson').option(\"multiLine\", \"true\").load(census_path).selectExpr(\"explode(features) as features\").select(\"features.*\")\n",
    "\n",
    "census = census.select([col(f\"properties.{col_name}\").alias(col_name) for col_name in census.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\n",
    "\n",
    "census = census.filter(col(\"geometry\").isNotNull())\n",
    "\n",
    "income.createOrReplaceTempView(\"income\")\n",
    "census.createOrReplaceTempView(\"census\")\n",
    "\n",
    "query  =  \"\"\"\n",
    "        SELECT `Zip Code`\n",
    "        FROM income\n",
    "        GROUP BY `Zip Code`\n",
    "        \"\"\"\n",
    "\n",
    "data = spark.sql(query)\n",
    "row_count = data.count()\n",
    "print(f\"Number of Zip Codes In income: {row_count}\")\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT ZCTA10\n",
    "        FROM census\n",
    "        JOIN income \n",
    "        ON ZCTA10 = `Zip Code`\n",
    "        GROUP BY ZCTA10\n",
    "       \"\"\"\n",
    "\n",
    "data = spark.sql(query)\n",
    "row_count = data.count()\n",
    "print(f\"Number of Common Zip Codes In income: {row_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f53ea7-425c-41f2-8631-62b2055b8e89",
   "metadata": {},
   "source": [
    "Παρατηρούμε ότι από τα 301 zip codes στο census, μπορούμε να πάρουμε income μόνο για τα 283 από αυτά. Αυτό οδηγεί αργότερα σε πρόβλημα, καθώς για 2 communities εξαιτίας αυτού δεν μπορούμε να βγάλουμε census αφού κανένα από τα zcta10 που έχουν μέσα τους δεν δίνει το average income του"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dd48bb-6bd0-45e3-8331-64fd7c0bf266",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Περιγράφουμε τον τρόπο με τον οποίο υπολογίζουμε το query:\n",
    "- Αρχικά φορτώνουμε τα δεδομένα από τα csv αρχεία και τα διαμορφώνουμε πριν τα φορτώσουμε ως tables στη βάση\n",
    "- Δημιουργούμε ST_Points για τα εγκλήματα ώστε να μπορούμε να κάνουμε join με τα blocks και να ξέρουμε μέσα σε ποιο block έγινε κάθε έγκλημα\n",
    "- Κάνουμε το πρώτο join τα εγκλήματα με τα blocks και κάνουμε groupBy community. \n",
    "Πλέον ξέρουμε πόσα εγκλήματα έγιναν σε κάθε community. Αν ένα community δεν έχει εγκλήματα, δυστυχώς θα το χάσουμε. Θεωρητικά θα μπορούσαμε να το λύσουμε αυτό κάνοντας **left join**. Επειδή το left join κολλούσε επ άπειρον το κάναμε χειροκίνητα ως εξής:\n",
    "- Στον αρχικό πίνακα με τα blocks κάνουμε group by community με στήλη *crime_counter* η οποία έχει το literal 0.\n",
    "- Κάνουμε union το αποτέλεσμα του join με τον νέο πίνακα.\n",
    "- Κάνουμε groupBy *community* και κρατάμε το μέγιστο των εγκλημάτων.\n",
    "Έτσι όποιο community είχε εγκλήματα κρατάει το counter του, ενώ τα υπόλοιπα δεν εξαφανίζονται (αν δεν το κάναμε αυτό θα χάναμε 80 communitites)\n",
    "\n",
    "Θέλουμε τώρα να υπολογίσουμε τον συνολικό πληθυσμό του κάθε community και το average income\n",
    "- Αρχίζουμε με τον πίνακα census από τον οποίο κρατάμε τα *ZCTA10*, *COMM*, *POP_2010*, *HOUSING10*.\n",
    "- Αφού το income βασίζεται στο zip code, αλλά τα zip codes επεκείνονται σε πολλά communities, κάνουμε groupBy τη δυάδα (*ZCTA10,COMM*) κρατώντας το άθροισμα των στοιχείων.\n",
    "- Καθαρίζουμε τα δεδομένα του table *income* για να μπορούμε να τα χρησιμοποιήσουμε σε αριθμητικές πράξεις.\n",
    "- Κάνουμε join τα entries του πίνακα των περιοχών με τον πίνακα των εσόδων βάσει του *ZCTA10*.\n",
    "Σε αυτό το σημείο πάλι χάσαμε κάποιες περιοχές για τις οποίες δεν έχουμε πληροφορίες για το average income. Θα κάνουμε την ίδια τεχνική για **left join** με μια διαφορά. Αν για τον πληθυσμό μιας περιοχής (*ZCTA10,COMM*) δεν έχουμε income data, δεν θέλουμε να συμπεριλάβουμε τον πληθυσμό τα average income data (αφού θα τα μειώσει σαν να είχε average income 0) αλλά θα τον συμπεριλάβουμε στα crime data. Για να το κάνουμε αυτό θα κρατήσουμε δύο πληθυσμούς για κάθε row. Αυτός για το income θα είναι ίσος με τον αρχικό πληθυσμό σε περίπτωση που έχουμε income data για το zip code, διαφορετικά θα είναι 0.\n",
    "- Συμπληρώνουμε τον πίνακα με τα income όπως περιγράψαμε παραπάνω\n",
    "- Κάνουμε groupBy στο community βγάζοντας κρατώντας το αρχικό population και το average income με τον εξής τύπο\n",
    "CAST(SUM(med_inc * houses_per_zip_code_in_community) / SUM(population_for_income) AS INT)\n",
    "Πλέον έχουμε τα income data και τον πληθυσμό.\n",
    "- Τέλος κάνουμε join με το *crime_per_community* και υπολογίζουμε τα crimes per person με τον εξής τύπο:\n",
    "F.col(\"crime_counter\") / F.col(\"population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c813251f-3f90-4c68-b974-d59dcbebd67e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Created 'crimes' view with 3109880 total rows.\n",
      "+---------+--------------------+--------------------+--------+-----+---------+-----------+--------+------+--------------------+--------------+--------+--------+------------+---------+--------------------+--------------+--------------------+------+------------+--------+--------+--------+--------+--------------------+--------------------+-------+---------+--------------------+\n",
      "|    DR_NO|           Date Rptd|            DATE OCC|TIME OCC|AREA |AREA NAME|Rpt Dist No|Part 1-2|Crm Cd|         Crm Cd Desc|       Mocodes|Vict Age|Vict Sex|Vict Descent|Premis Cd|         Premis Desc|Weapon Used Cd|         Weapon Desc|Status| Status Desc|Crm Cd 1|Crm Cd 2|Crm Cd 3|Crm Cd 4|            LOCATION|        Cross Street|    LAT|      LON|               point|\n",
      "+---------+--------------------+--------------------+--------+-----+---------+-----------+--------+------+--------------------+--------------+--------+--------+------------+---------+--------------------+--------------+--------------------+------+------------+--------+--------+--------+--------+--------------------+--------------------+-------+---------+--------------------+\n",
      "|001307355|02/20/2010 12:00:...|02/20/2010 12:00:...|    1350|   13|   Newton|       1385|       2|   900|VIOLATION OF COUR...|0913 1814 2000|      48|       M|           H|      501|SINGLE FAMILY DWE...|          NULL|                NULL|    AA|Adult Arrest|     900|    NULL|    NULL|    NULL|300 E  GAGE      ...|                NULL|33.9825|-118.2695|POINT (-118.2695 ...|\n",
      "|011401303|09/13/2010 12:00:...|09/12/2010 12:00:...|    0045|   14|  Pacific|       1485|       2|   740|VANDALISM - FELON...|          0329|       0|       M|           W|      101|              STREET|          NULL|                NULL|    IC| Invest Cont|     740|    NULL|    NULL|    NULL|SEPULVEDA        ...|MANCHESTER       ...|33.9599|-118.3962|POINT (-118.3962 ...|\n",
      "|070309629|08/09/2010 12:00:...|08/09/2010 12:00:...|    1515|   13|   Newton|       1324|       2|   946|OTHER MISCELLANEO...|          0344|       0|       M|           H|      103|               ALLEY|          NULL|                NULL|    IC| Invest Cont|     946|    NULL|    NULL|    NULL|1300 E  21ST     ...|                NULL|34.0224|-118.2524|POINT (-118.2524 ...|\n",
      "|090631215|01/05/2010 12:00:...|01/05/2010 12:00:...|    0150|   06|Hollywood|       0646|       2|   900|VIOLATION OF COUR...|1100 0400 1402|      47|       F|           W|      101|              STREET|           102|            HAND GUN|    IC| Invest Cont|     900|     998|    NULL|    NULL|CAHUENGA         ...|HOLLYWOOD        ...|34.1016|-118.3295|POINT (-118.3295 ...|\n",
      "|100100501|01/03/2010 12:00:...|01/02/2010 12:00:...|    2100|   01|  Central|       0176|       1|   122|     RAPE, ATTEMPTED|          0400|      47|       F|           H|      103|               ALLEY|           400|STRONG-ARM (HANDS...|    IC| Invest Cont|     122|    NULL|    NULL|    NULL|8TH              ...|SAN PEDRO        ...|34.0387|-118.2488|POINT (-118.2488 ...|\n",
      "|100100506|01/05/2010 12:00:...|01/04/2010 12:00:...|    1650|   01|  Central|       0162|       1|   442|SHOPLIFTING - PET...|     0344 1402|      23|       M|           B|      404|    DEPARTMENT STORE|          NULL|                NULL|    AA|Adult Arrest|     442|    NULL|    NULL|    NULL|700 W  7TH       ...|                NULL| 34.048|-118.2577|POINT (-118.2577 ...|\n",
      "|100100508|01/08/2010 12:00:...|01/07/2010 12:00:...|    2005|   01|  Central|       0182|       1|   330|BURGLARY FROM VEH...|          0344|      46|       M|           H|      101|              STREET|          NULL|                NULL|    IC| Invest Cont|     330|    NULL|    NULL|    NULL|PICO             ...|GRAND            ...|34.0389|-118.2643|POINT (-118.2643 ...|\n",
      "|100100509|01/09/2010 12:00:...|01/08/2010 12:00:...|    2100|   01|  Central|       0157|       1|   230|ASSAULT WITH DEAD...|          0416|      51|       M|           B|      710|       OTHER PREMISE|           500|UNKNOWN WEAPON/OT...|    AA|Adult Arrest|     230|    NULL|    NULL|    NULL|500    CROCKER   ...|                NULL|34.0435|-118.2427|POINT (-118.2427 ...|\n",
      "|100100510|01/09/2010 12:00:...|01/09/2010 12:00:...|    0230|   01|  Central|       0171|       1|   230|ASSAULT WITH DEAD...|     0400 0416|      30|       M|           H|      108|         PARKING LOT|           400|STRONG-ARM (HANDS...|    IC| Invest Cont|     230|    NULL|    NULL|    NULL|800 W  OLYMPIC   ...|                NULL| 34.045| -118.264|POINT (-118.264 3...|\n",
      "|100100511|01/09/2010 12:00:...|01/06/2010 12:00:...|    2100|   01|  Central|       0132|       1|   341|THEFT-GRAND ($950...|     0344 1402|      55|       M|           W|      710|       OTHER PREMISE|          NULL|                NULL|    IC| Invest Cont|     341|     998|    NULL|    NULL|200 S  OLIVE     ...|                NULL|34.0538|-118.2488|POINT (-118.2488 ...|\n",
      "+---------+--------------------+--------------------+--------+-----+---------+-----------+--------+------+--------------------+--------------+--------+--------+------------+---------+--------------------+--------------+--------------------+------+------------+--------+--------+--------+--------+--------------------+--------------------+-------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "[2] Added `id` column to census with 109279 rows\n",
      "\n",
      "+-------+------------+-------+----+---------+-------------------+--------------------+--------------------+------+------+----------+-------+---------------+---------+--------+--------+--------+------+--------+---------------+--------+----------+--------------------+------------------+------+--------------------+\n",
      "|   BG10|   BG10FIP10|   BG12|CB10|CEN_FIP13|               CITY|             CITYCOM|                COMM|  CT10|  CT12|    CTCB10|HD_2012|        HD_NAME|HOUSING10|LA_FIP10|OBJECTID|POP_2010|PUMA10|SPA_2012|       SPA_NAME|SUP_DIST| SUP_LABEL|         ShapeSTArea|     ShapeSTLength|ZCTA10|            geometry|\n",
      "+-------+------------+-------+----+---------+-------------------+--------------------+--------------------+------+------+----------+-------+---------------+---------+--------+--------+--------+------+--------+---------------+--------+----------+--------------------+------------------+------+--------------------+\n",
      "|9012091|901209199037|9012091|1742|         |     Unincorporated|Unincorporated - ...|West Antelope Valley|901209|901209|9012091742|      5|Antelope Valley|        0|   99037|       1|       0| 03701|       1|Antelope Valley|       5|District 5|1.0126828784741674E8|49249.970965066546| 93243|POLYGON ((-118.76...|\n",
      "|9012091|901209199037|9012091|1776|         |     Unincorporated|Unincorporated - ...|West Antelope Valley|901209|901209|9012091776|      5|Antelope Valley|        0|   99037|       2|       0| 03701|       1|Antelope Valley|       5|District 5|1.0864058264932722E7|12865.930889894544| 93243|POLYGON ((-118.88...|\n",
      "|9012091|901209199037|9012091|1780|         |     Unincorporated|Unincorporated - ...|West Antelope Valley|901209|901209|9012091780|      5|Antelope Valley|        0|   99037|       3|       0| 03701|       1|Antelope Valley|       5|District 5|   632764.3873936612|4495.6756629516585| 93243|POLYGON ((-118.88...|\n",
      "|9800031|980003199037|9800031|1000|         |     Unincorporated|Unincorporated - ...|       South Edwards|980003|980003|9800031000|      5|Antelope Valley|        0|   99037|       4|       0| 03701|       1|Antelope Valley|       5|District 5| 8.436720099004808E7|44040.941816768456|      |POLYGON ((-117.77...|\n",
      "|9800031|980003199037|9800031|1002|         |     Unincorporated|Unincorporated - ...|       South Edwards|980003|980003|9800031002|      5|Antelope Valley|        0|   99037|       5|       0| 03701|       1|Antelope Valley|       5|District 5|  3440432.6271546073|30311.983847387033|      |POLYGON ((-117.82...|\n",
      "|9012091|901209199037|9012091|1053|         |     Unincorporated|Unincorporated - ...|West Antelope Valley|901209|901209|9012091053|      5|Antelope Valley|        5|   99037|       6|       5| 03701|       1|Antelope Valley|       5|District 5|   24131.11120221406| 861.3016220613971| 93243|POLYGON ((-118.70...|\n",
      "|9800031|980003199037|9800031|1060|         |     Unincorporated|Unincorporated - ...|       South Edwards|980003|980003|9800031060|      5|Antelope Valley|        0|   99037|       7|       0| 03701|       1|Antelope Valley|       5|District 5|     21518.544437332|   621.28621925602|      |POLYGON ((-117.97...|\n",
      "|2970002|297000244000|2970002|2011|    44000|        Los Angeles|Los Angeles - San...|           San Pedro|297000|297000|2970002011|     31|         Harbor|       26|   44000|       8|      69| 03767|       8|      South Bay|       4|District 4|  217909.87644297967|1980.4912547117679| 90732|POLYGON ((-118.31...|\n",
      "|4033042|403304219192|4033042|2020|    19192|        Diamond Bar| City of Diamond Bar|         Diamond Bar|403304|403304|4033042020|     54|         Pomona|       26|   19192|       9|      79| 03714|       3|    San Gabriel|       4|District 4|  374256.10070698214|3107.9271811639755| 91789|POLYGON ((-117.84...|\n",
      "|6707023|670702359514|6707023|3022|    59514|Rancho Palos Verdes|City of Rancho Pa...| Rancho Palos Verdes|670702|670702|6707023022|     31|         Harbor|        0|   59514|      10|       0| 03768|       8|      South Bay|       4|District 4|   73443.16322641971|1236.5464043943637| 90275|POLYGON ((-118.31...|\n",
      "+-------+------------+-------+----+---------+-------------------+--------------------+--------------------+------+------+----------+-------+---------------+---------+--------+--------+--------+------+--------+---------------+--------+----------+--------------------+------------------+------+--------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number, expr\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"QUERY 3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "\n",
    "\n",
    "crime19path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "crime20path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "income_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\"\n",
    "census_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "\n",
    "crime19 = spark.read.format('csv').options(header='true').load(crime19path)\n",
    "crime20 = spark.read.format('csv').options(header='true').load(crime20path)\n",
    "income = spark.read.format('csv').options(header='true').load(income_path)\n",
    "census = sedona.read.format('geojson').option(\"multiLine\", \"true\").load(census_path).selectExpr(\"explode(features) as features\").select(\"features.*\")\n",
    "\n",
    "census = census.select([col(f\"properties.{col_name}\").alias(col_name) for col_name in census.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\n",
    "\n",
    "crime19 = crime19.filter((col(\"LAT\").isNotNull()) & (col(\"LAT\") != 0) & (col(\"LON\").isNotNull()) & (col(\"LON\") != 0))\n",
    "crime20 = crime20.filter((col(\"LAT\").isNotNull()) & (col(\"LAT\") != 0) & (col(\"LON\").isNotNull()) & (col(\"LON\") != 0))\n",
    "\n",
    "census = census.filter(col(\"geometry\").isNotNull()).filter((col(\"CITY\") == \"Los Angeles\"))\n",
    "\n",
    "\n",
    "crime19.createOrReplaceTempView(\"crime19\")\n",
    "crime20.createOrReplaceTempView(\"crime20\")\n",
    "income.createOrReplaceTempView(\"income\")\n",
    "census.createOrReplaceTempView(\"census\")\n",
    "\n",
    "\n",
    "join_strategy = \"MERGE\"\n",
    "\n",
    "# Union of Crime Tables. No join we don't need to time this\n",
    "crimes = crime19.union(crime20)\n",
    "crimes = crimes.withColumn(\"point\", expr(\"ST_Point(LON, LAT)\"))\n",
    "crime_count = crimes.count()\n",
    "print(f\"[1] Created 'crimes' view with {crime_count} total rows.\")\n",
    "\n",
    "crimes.show(10)\n",
    "\n",
    "census_counter = census.count()\n",
    "print(f\"[2] Added `id` column to census with {census_counter} rows\\n\")\n",
    "census.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7197a487-a726-41c0-8a5d-e687cc524733",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 3: Spatial Join => blocks_with_crime using BROADCAST\n",
      "\n",
      "Explain plan:\n",
      "== Physical Plan ==\n",
      "BroadcastIndexJoin point#882: geometry, LeftSide, RightSide, Inner, WITHIN ST_WITHIN(point#882, geometry#718)\n",
      ":- SpatialIndex geometry#718: geometry, RTREE, false, false\n",
      ":  +- *(1) Project [features#715.properties.BG10 AS BG10#724, features#715.properties.BG10FIP10 AS BG10FIP10#725, features#715.properties.BG12 AS BG12#726, features#715.properties.CB10 AS CB10#727, features#715.properties.CEN_FIP13 AS CEN_FIP13#728, features#715.properties.CITY AS CITY#729, features#715.properties.CITYCOM AS CITYCOM#730, features#715.properties.COMM AS COMM#731, features#715.properties.CT10 AS CT10#732, features#715.properties.CT12 AS CT12#733, features#715.properties.CTCB10 AS CTCB10#734, features#715.properties.HD_2012 AS HD_2012#735L, features#715.properties.HD_NAME AS HD_NAME#736, features#715.properties.HOUSING10 AS HOUSING10#737L, features#715.properties.LA_FIP10 AS LA_FIP10#738, features#715.properties.OBJECTID AS OBJECTID#739L, features#715.properties.POP_2010 AS POP_2010#740L, features#715.properties.PUMA10 AS PUMA10#741, features#715.properties.SPA_2012 AS SPA_2012#742L, features#715.properties.SPA_NAME AS SPA_NAME#743, features#715.properties.SUP_DIST AS SUP_DIST#744, features#715.properties.SUP_LABEL AS SUP_LABEL#745, features#715.properties.ShapeSTArea AS ShapeSTArea#746, features#715.properties.ShapeSTLength AS ShapeSTLength#747, ... 2 more fields]\n",
      ":     +- *(1) Filter isnotnull(features#715.geometry)\n",
      ":        +- *(1) Generate explode(features#707), false, [features#715]\n",
      ":           +- *(1) Filter ((size(features#707, true) > 0) AND isnotnull(features#707))\n",
      ":              +- FileScan geojson [features#707] Batched: false, DataFilters: [(size(features#707, true) > 0), isnotnull(features#707)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "+- Union\n",
      "   :- Project [DR_NO#552, Date Rptd#553, DATE OCC#554, TIME OCC#555, AREA #556, AREA NAME#557, Rpt Dist No#558, Part 1-2#559, Crm Cd#560, Crm Cd Desc#561, Mocodes#562, Vict Age#563, Vict Sex#564, Vict Descent#565, Premis Cd#566, Premis Desc#567, Weapon Used Cd#568, Weapon Desc#569, Status#570, Status Desc#571, Crm Cd 1#572, Crm Cd 2#573, Crm Cd 3#574, Crm Cd 4#575, ... 5 more fields]\n",
      "   :  +- Filter ((((isnotnull(LAT#578) AND NOT (cast(LAT#578 as int) = 0)) AND isnotnull(LON#579)) AND NOT (cast(LON#579 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "   :     +- FileScan csv [DR_NO#552,Date Rptd#553,DATE OCC#554,TIME OCC#555,AREA #556,AREA NAME#557,Rpt Dist No#558,Part 1-2#559,Crm Cd#560,Crm Cd Desc#561,Mocodes#562,Vict Age#563,Vict Sex#564,Vict Descent#565,Premis Cd#566,Premis Desc#567,Weapon Used Cd#568,Weapon Desc#569,Status#570,Status Desc#571,Crm Cd 1#572,Crm Cd 2#573,Crm Cd 3#574,Crm Cd 4#575,... 4 more fields] Batched: false, DataFilters: [isnotnull(LAT#578), NOT (cast(LAT#578 as int) = 0), isnotnull(LON#579), NOT (cast(LON#579 as int..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA :string,AREA NAME:strin...\n",
      "   +- Project [DR_NO#626, Date Rptd#627, DATE OCC#628, TIME OCC#629, AREA#630, AREA NAME#631, Rpt Dist No#632, Part 1-2#633, Crm Cd#634, Crm Cd Desc#635, Mocodes#636, Vict Age#637, Vict Sex#638, Vict Descent#639, Premis Cd#640, Premis Desc#641, Weapon Used Cd#642, Weapon Desc#643, Status#644, Status Desc#645, Crm Cd 1#646, Crm Cd 2#647, Crm Cd 3#648, Crm Cd 4#649, ... 5 more fields]\n",
      "      +- Filter ((((isnotnull(LAT#652) AND NOT (cast(LAT#652 as int) = 0)) AND isnotnull(LON#653)) AND NOT (cast(LON#653 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "         +- FileScan csv [DR_NO#626,Date Rptd#627,DATE OCC#628,TIME OCC#629,AREA#630,AREA NAME#631,Rpt Dist No#632,Part 1-2#633,Crm Cd#634,Crm Cd Desc#635,Mocodes#636,Vict Age#637,Vict Sex#638,Vict Descent#639,Premis Cd#640,Premis Desc#641,Weapon Used Cd#642,Weapon Desc#643,Status#644,Status Desc#645,Crm Cd 1#646,Crm Cd 2#647,Crm Cd 3#648,Crm Cd 4#649,... 4 more fields] Batched: false, DataFilters: [isnotnull(LAT#652), NOT (cast(LAT#652 as int) = 0), isnotnull(LON#653), NOT (cast(LON#653 as int..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA:string,AREA NAME:string...\n",
      "\n",
      "\n",
      "\n",
      "[3] Spatial join => 3109471 rows. Elapsed: 34.97 s for strategy: BROADCAST\n",
      "\n",
      "============================================================\n",
      "STEP 3: Spatial Join => blocks_with_crime using MERGE\n",
      "\n",
      "Explain plan:\n",
      "== Physical Plan ==\n",
      "RangeJoin geometry#718: geometry, point#882: geometry, CONTAINS\n",
      ":- *(1) Project [features#715.properties.BG10 AS BG10#724, features#715.properties.BG10FIP10 AS BG10FIP10#725, features#715.properties.BG12 AS BG12#726, features#715.properties.CB10 AS CB10#727, features#715.properties.CEN_FIP13 AS CEN_FIP13#728, features#715.properties.CITY AS CITY#729, features#715.properties.CITYCOM AS CITYCOM#730, features#715.properties.COMM AS COMM#731, features#715.properties.CT10 AS CT10#732, features#715.properties.CT12 AS CT12#733, features#715.properties.CTCB10 AS CTCB10#734, features#715.properties.HD_2012 AS HD_2012#735L, features#715.properties.HD_NAME AS HD_NAME#736, features#715.properties.HOUSING10 AS HOUSING10#737L, features#715.properties.LA_FIP10 AS LA_FIP10#738, features#715.properties.OBJECTID AS OBJECTID#739L, features#715.properties.POP_2010 AS POP_2010#740L, features#715.properties.PUMA10 AS PUMA10#741, features#715.properties.SPA_2012 AS SPA_2012#742L, features#715.properties.SPA_NAME AS SPA_NAME#743, features#715.properties.SUP_DIST AS SUP_DIST#744, features#715.properties.SUP_LABEL AS SUP_LABEL#745, features#715.properties.ShapeSTArea AS ShapeSTArea#746, features#715.properties.ShapeSTLength AS ShapeSTLength#747, ... 2 more fields]\n",
      ":  +- *(1) Filter isnotnull(features#715.geometry)\n",
      ":     +- *(1) Generate explode(features#707), false, [features#715]\n",
      ":        +- *(1) Filter ((size(features#707, true) > 0) AND isnotnull(features#707))\n",
      ":           +- FileScan geojson [features#707] Batched: false, DataFilters: [(size(features#707, true) > 0), isnotnull(features#707)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "+- Union\n",
      "   :- Project [DR_NO#552, Date Rptd#553, DATE OCC#554, TIME OCC#555, AREA #556, AREA NAME#557, Rpt Dist No#558, Part 1-2#559, Crm Cd#560, Crm Cd Desc#561, Mocodes#562, Vict Age#563, Vict Sex#564, Vict Descent#565, Premis Cd#566, Premis Desc#567, Weapon Used Cd#568, Weapon Desc#569, Status#570, Status Desc#571, Crm Cd 1#572, Crm Cd 2#573, Crm Cd 3#574, Crm Cd 4#575, ... 5 more fields]\n",
      "   :  +- Filter ((((isnotnull(LAT#578) AND NOT (cast(LAT#578 as int) = 0)) AND isnotnull(LON#579)) AND NOT (cast(LON#579 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "   :     +- FileScan csv [DR_NO#552,Date Rptd#553,DATE OCC#554,TIME OCC#555,AREA #556,AREA NAME#557,Rpt Dist No#558,Part 1-2#559,Crm Cd#560,Crm Cd Desc#561,Mocodes#562,Vict Age#563,Vict Sex#564,Vict Descent#565,Premis Cd#566,Premis Desc#567,Weapon Used Cd#568,Weapon Desc#569,Status#570,Status Desc#571,Crm Cd 1#572,Crm Cd 2#573,Crm Cd 3#574,Crm Cd 4#575,... 4 more fields] Batched: false, DataFilters: [isnotnull(LAT#578), NOT (cast(LAT#578 as int) = 0), isnotnull(LON#579), NOT (cast(LON#579 as int..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA :string,AREA NAME:strin...\n",
      "   +- Project [DR_NO#626, Date Rptd#627, DATE OCC#628, TIME OCC#629, AREA#630, AREA NAME#631, Rpt Dist No#632, Part 1-2#633, Crm Cd#634, Crm Cd Desc#635, Mocodes#636, Vict Age#637, Vict Sex#638, Vict Descent#639, Premis Cd#640, Premis Desc#641, Weapon Used Cd#642, Weapon Desc#643, Status#644, Status Desc#645, Crm Cd 1#646, Crm Cd 2#647, Crm Cd 3#648, Crm Cd 4#649, ... 5 more fields]\n",
      "      +- Filter ((((isnotnull(LAT#652) AND NOT (cast(LAT#652 as int) = 0)) AND isnotnull(LON#653)) AND NOT (cast(LON#653 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "         +- FileScan csv [DR_NO#626,Date Rptd#627,DATE OCC#628,TIME OCC#629,AREA#630,AREA NAME#631,Rpt Dist No#632,Part 1-2#633,Crm Cd#634,Crm Cd Desc#635,Mocodes#636,Vict Age#637,Vict Sex#638,Vict Descent#639,Premis Cd#640,Premis Desc#641,Weapon Used Cd#642,Weapon Desc#643,Status#644,Status Desc#645,Crm Cd 1#646,Crm Cd 2#647,Crm Cd 3#648,Crm Cd 4#649,... 4 more fields] Batched: false, DataFilters: [isnotnull(LAT#652), NOT (cast(LAT#652 as int) = 0), isnotnull(LON#653), NOT (cast(LON#653 as int..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA:string,AREA NAME:string...\n",
      "\n",
      "\n",
      "\n",
      "[3] Spatial join => 3109471 rows. Elapsed: 81.45 s for strategy: MERGE\n",
      "\n",
      "============================================================\n",
      "STEP 3: Spatial Join => blocks_with_crime using SHUFFLE_HASH\n",
      "\n",
      "Explain plan:\n",
      "== Physical Plan ==\n",
      "RangeJoin geometry#718: geometry, point#882: geometry, CONTAINS\n",
      ":- *(1) Project [features#715.properties.BG10 AS BG10#724, features#715.properties.BG10FIP10 AS BG10FIP10#725, features#715.properties.BG12 AS BG12#726, features#715.properties.CB10 AS CB10#727, features#715.properties.CEN_FIP13 AS CEN_FIP13#728, features#715.properties.CITY AS CITY#729, features#715.properties.CITYCOM AS CITYCOM#730, features#715.properties.COMM AS COMM#731, features#715.properties.CT10 AS CT10#732, features#715.properties.CT12 AS CT12#733, features#715.properties.CTCB10 AS CTCB10#734, features#715.properties.HD_2012 AS HD_2012#735L, features#715.properties.HD_NAME AS HD_NAME#736, features#715.properties.HOUSING10 AS HOUSING10#737L, features#715.properties.LA_FIP10 AS LA_FIP10#738, features#715.properties.OBJECTID AS OBJECTID#739L, features#715.properties.POP_2010 AS POP_2010#740L, features#715.properties.PUMA10 AS PUMA10#741, features#715.properties.SPA_2012 AS SPA_2012#742L, features#715.properties.SPA_NAME AS SPA_NAME#743, features#715.properties.SUP_DIST AS SUP_DIST#744, features#715.properties.SUP_LABEL AS SUP_LABEL#745, features#715.properties.ShapeSTArea AS ShapeSTArea#746, features#715.properties.ShapeSTLength AS ShapeSTLength#747, ... 2 more fields]\n",
      ":  +- *(1) Filter isnotnull(features#715.geometry)\n",
      ":     +- *(1) Generate explode(features#707), false, [features#715]\n",
      ":        +- *(1) Filter ((size(features#707, true) > 0) AND isnotnull(features#707))\n",
      ":           +- FileScan geojson [features#707] Batched: false, DataFilters: [(size(features#707, true) > 0), isnotnull(features#707)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "+- Union\n",
      "   :- Project [DR_NO#552, Date Rptd#553, DATE OCC#554, TIME OCC#555, AREA #556, AREA NAME#557, Rpt Dist No#558, Part 1-2#559, Crm Cd#560, Crm Cd Desc#561, Mocodes#562, Vict Age#563, Vict Sex#564, Vict Descent#565, Premis Cd#566, Premis Desc#567, Weapon Used Cd#568, Weapon Desc#569, Status#570, Status Desc#571, Crm Cd 1#572, Crm Cd 2#573, Crm Cd 3#574, Crm Cd 4#575, ... 5 more fields]\n",
      "   :  +- Filter ((((isnotnull(LAT#578) AND NOT (cast(LAT#578 as int) = 0)) AND isnotnull(LON#579)) AND NOT (cast(LON#579 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "   :     +- FileScan csv [DR_NO#552,Date Rptd#553,DATE OCC#554,TIME OCC#555,AREA #556,AREA NAME#557,Rpt Dist No#558,Part 1-2#559,Crm Cd#560,Crm Cd Desc#561,Mocodes#562,Vict Age#563,Vict Sex#564,Vict Descent#565,Premis Cd#566,Premis Desc#567,Weapon Used Cd#568,Weapon Desc#569,Status#570,Status Desc#571,Crm Cd 1#572,Crm Cd 2#573,Crm Cd 3#574,Crm Cd 4#575,... 4 more fields] Batched: false, DataFilters: [isnotnull(LAT#578), NOT (cast(LAT#578 as int) = 0), isnotnull(LON#579), NOT (cast(LON#579 as int..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA :string,AREA NAME:strin...\n",
      "   +- Project [DR_NO#626, Date Rptd#627, DATE OCC#628, TIME OCC#629, AREA#630, AREA NAME#631, Rpt Dist No#632, Part 1-2#633, Crm Cd#634, Crm Cd Desc#635, Mocodes#636, Vict Age#637, Vict Sex#638, Vict Descent#639, Premis Cd#640, Premis Desc#641, Weapon Used Cd#642, Weapon Desc#643, Status#644, Status Desc#645, Crm Cd 1#646, Crm Cd 2#647, Crm Cd 3#648, Crm Cd 4#649, ... 5 more fields]\n",
      "      +- Filter ((((isnotnull(LAT#652) AND NOT (cast(LAT#652 as int) = 0)) AND isnotnull(LON#653)) AND NOT (cast(LON#653 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "         +- FileScan csv [DR_NO#626,Date Rptd#627,DATE OCC#628,TIME OCC#629,AREA#630,AREA NAME#631,Rpt Dist No#632,Part 1-2#633,Crm Cd#634,Crm Cd Desc#635,Mocodes#636,Vict Age#637,Vict Sex#638,Vict Descent#639,Premis Cd#640,Premis Desc#641,Weapon Used Cd#642,Weapon Desc#643,Status#644,Status Desc#645,Crm Cd 1#646,Crm Cd 2#647,Crm Cd 3#648,Crm Cd 4#649,... 4 more fields] Batched: false, DataFilters: [isnotnull(LAT#652), NOT (cast(LAT#652 as int) = 0), isnotnull(LON#653), NOT (cast(LON#653 as int..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA:string,AREA NAME:string...\n",
      "\n",
      "\n",
      "\n",
      "[3] Spatial join => 3109471 rows. Elapsed: 77.37 s for strategy: SHUFFLE_HASH\n",
      "\n",
      "============================================================\n",
      "STEP 3: Spatial Join => blocks_with_crime using SHUFFLE_REPLICATE_NL\n",
      "\n",
      "Explain plan:\n",
      "== Physical Plan ==\n",
      "RangeJoin geometry#718: geometry, point#882: geometry, CONTAINS\n",
      ":- *(1) Project [features#715.properties.BG10 AS BG10#724, features#715.properties.BG10FIP10 AS BG10FIP10#725, features#715.properties.BG12 AS BG12#726, features#715.properties.CB10 AS CB10#727, features#715.properties.CEN_FIP13 AS CEN_FIP13#728, features#715.properties.CITY AS CITY#729, features#715.properties.CITYCOM AS CITYCOM#730, features#715.properties.COMM AS COMM#731, features#715.properties.CT10 AS CT10#732, features#715.properties.CT12 AS CT12#733, features#715.properties.CTCB10 AS CTCB10#734, features#715.properties.HD_2012 AS HD_2012#735L, features#715.properties.HD_NAME AS HD_NAME#736, features#715.properties.HOUSING10 AS HOUSING10#737L, features#715.properties.LA_FIP10 AS LA_FIP10#738, features#715.properties.OBJECTID AS OBJECTID#739L, features#715.properties.POP_2010 AS POP_2010#740L, features#715.properties.PUMA10 AS PUMA10#741, features#715.properties.SPA_2012 AS SPA_2012#742L, features#715.properties.SPA_NAME AS SPA_NAME#743, features#715.properties.SUP_DIST AS SUP_DIST#744, features#715.properties.SUP_LABEL AS SUP_LABEL#745, features#715.properties.ShapeSTArea AS ShapeSTArea#746, features#715.properties.ShapeSTLength AS ShapeSTLength#747, ... 2 more fields]\n",
      ":  +- *(1) Filter isnotnull(features#715.geometry)\n",
      ":     +- *(1) Generate explode(features#707), false, [features#715]\n",
      ":        +- *(1) Filter ((size(features#707, true) > 0) AND isnotnull(features#707))\n",
      ":           +- FileScan geojson [features#707] Batched: false, DataFilters: [(size(features#707, true) > 0), isnotnull(features#707)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "+- Union\n",
      "   :- Project [DR_NO#552, Date Rptd#553, DATE OCC#554, TIME OCC#555, AREA #556, AREA NAME#557, Rpt Dist No#558, Part 1-2#559, Crm Cd#560, Crm Cd Desc#561, Mocodes#562, Vict Age#563, Vict Sex#564, Vict Descent#565, Premis Cd#566, Premis Desc#567, Weapon Used Cd#568, Weapon Desc#569, Status#570, Status Desc#571, Crm Cd 1#572, Crm Cd 2#573, Crm Cd 3#574, Crm Cd 4#575, ... 5 more fields]\n",
      "   :  +- Filter ((((isnotnull(LAT#578) AND NOT (cast(LAT#578 as int) = 0)) AND isnotnull(LON#579)) AND NOT (cast(LON#579 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "   :     +- FileScan csv [DR_NO#552,Date Rptd#553,DATE OCC#554,TIME OCC#555,AREA #556,AREA NAME#557,Rpt Dist No#558,Part 1-2#559,Crm Cd#560,Crm Cd Desc#561,Mocodes#562,Vict Age#563,Vict Sex#564,Vict Descent#565,Premis Cd#566,Premis Desc#567,Weapon Used Cd#568,Weapon Desc#569,Status#570,Status Desc#571,Crm Cd 1#572,Crm Cd 2#573,Crm Cd 3#574,Crm Cd 4#575,... 4 more fields] Batched: false, DataFilters: [isnotnull(LAT#578), NOT (cast(LAT#578 as int) = 0), isnotnull(LON#579), NOT (cast(LON#579 as int..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA :string,AREA NAME:strin...\n",
      "   +- Project [DR_NO#626, Date Rptd#627, DATE OCC#628, TIME OCC#629, AREA#630, AREA NAME#631, Rpt Dist No#632, Part 1-2#633, Crm Cd#634, Crm Cd Desc#635, Mocodes#636, Vict Age#637, Vict Sex#638, Vict Descent#639, Premis Cd#640, Premis Desc#641, Weapon Used Cd#642, Weapon Desc#643, Status#644, Status Desc#645, Crm Cd 1#646, Crm Cd 2#647, Crm Cd 3#648, Crm Cd 4#649, ... 5 more fields]\n",
      "      +- Filter ((((isnotnull(LAT#652) AND NOT (cast(LAT#652 as int) = 0)) AND isnotnull(LON#653)) AND NOT (cast(LON#653 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "         +- FileScan csv [DR_NO#626,Date Rptd#627,DATE OCC#628,TIME OCC#629,AREA#630,AREA NAME#631,Rpt Dist No#632,Part 1-2#633,Crm Cd#634,Crm Cd Desc#635,Mocodes#636,Vict Age#637,Vict Sex#638,Vict Descent#639,Premis Cd#640,Premis Desc#641,Weapon Used Cd#642,Weapon Desc#643,Status#644,Status Desc#645,Crm Cd 1#646,Crm Cd 2#647,Crm Cd 3#648,Crm Cd 4#649,... 4 more fields] Batched: false, DataFilters: [isnotnull(LAT#652), NOT (cast(LAT#652 as int) = 0), isnotnull(LON#653), NOT (cast(LON#653 as int..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA:string,AREA NAME:string...\n",
      "\n",
      "\n",
      "\n",
      "[3] Spatial join => 3109471 rows. Elapsed: 80.38 s for strategy: SHUFFLE_REPLICATE_NL"
     ]
    }
   ],
   "source": [
    "# Suppose you have already defined `census` and `crimes`, \n",
    "# and your ST_Within UDF or function is available.\n",
    "\n",
    "join_strategies = [\"BROADCAST\", \"MERGE\", \"SHUFFLE_HASH\", \"SHUFFLE_REPLICATE_NL\"]\n",
    "\n",
    "for strategy in join_strategies:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"STEP 3: Spatial Join => blocks_with_crime using {strategy}\")\n",
    "    \n",
    "    # Clear any cached data to avoid reusing previous results\n",
    "    # spark.catalog.clearCache()\n",
    "    \n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Build the join pipeline with the given hint\n",
    "    blocks_with_crime = (\n",
    "        census\n",
    "        .hint(strategy)\n",
    "        .join(\n",
    "            crimes.hint(strategy),\n",
    "            # The condition for your spatial join\n",
    "            ST_Within(F.col(\"point\"), F.col(\"geometry\")),\n",
    "            \"inner\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Show explain plan so we can see the chosen strategy\n",
    "    print(\"\\nExplain plan:\")\n",
    "    blocks_with_crime.explain()\n",
    "    \n",
    "    # Trigger the actual join by counting rows\n",
    "    blocks_with_crime_count = blocks_with_crime.count()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n[3] Spatial join => {blocks_with_crime_count} rows. \"\n",
    "          f\"Elapsed: {elapsed:.2f} s for strategy: {strategy}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Join census with crimes. We need to time this. NOT equijoin + Big Tables\n",
    "#print(\"=== STEP 3: Spatial Join => blocks_with_crime ===\")\n",
    "#start_time = time.time()\n",
    "\n",
    "#blocks_with_crime = (\n",
    "#    census\n",
    "#    .hint(join_strategy)\n",
    "#    .join(\n",
    "#        crimes.hint(join_strategy),\n",
    "#        ST_Within(F.col(\"point\"), F.col(\"geometry\")),\n",
    "#        \"inner\"\n",
    "#    )\n",
    "#)\n",
    "# Force evaluation (count triggers the join + aggregation)\n",
    "#blocks_with_crime_count = blocks_with_crime.count()\n",
    "#elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "#blocks_with_crime_count = blocks_with_crime.count()\n",
    "#print(f\"[3] Spatial join => {blocks_with_crime_count} rows. Elapsed: {elapsed:.2f} s\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6afc2fb9-66f3-44cb-88f9-0b5dfabb0b21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 3: Spatial Join => blocks_with_crime (no hint) ===\n",
      "\n",
      "Explain plan for no-hint join:\n",
      "== Physical Plan ==\n",
      "RangeJoin geometry#718: geometry, point#882: geometry, CONTAINS\n",
      ":- *(1) Project [features#715.properties.BG10 AS BG10#724, features#715.properties.BG10FIP10 AS BG10FIP10#725, features#715.properties.BG12 AS BG12#726, features#715.properties.CB10 AS CB10#727, features#715.properties.CEN_FIP13 AS CEN_FIP13#728, features#715.properties.CITY AS CITY#729, features#715.properties.CITYCOM AS CITYCOM#730, features#715.properties.COMM AS COMM#731, features#715.properties.CT10 AS CT10#732, features#715.properties.CT12 AS CT12#733, features#715.properties.CTCB10 AS CTCB10#734, features#715.properties.HD_2012 AS HD_2012#735L, features#715.properties.HD_NAME AS HD_NAME#736, features#715.properties.HOUSING10 AS HOUSING10#737L, features#715.properties.LA_FIP10 AS LA_FIP10#738, features#715.properties.OBJECTID AS OBJECTID#739L, features#715.properties.POP_2010 AS POP_2010#740L, features#715.properties.PUMA10 AS PUMA10#741, features#715.properties.SPA_2012 AS SPA_2012#742L, features#715.properties.SPA_NAME AS SPA_NAME#743, features#715.properties.SUP_DIST AS SUP_DIST#744, features#715.properties.SUP_LABEL AS SUP_LABEL#745, features#715.properties.ShapeSTArea AS ShapeSTArea#746, features#715.properties.ShapeSTLength AS ShapeSTLength#747, ... 2 more fields]\n",
      ":  +- *(1) Filter isnotnull(features#715.geometry)\n",
      ":     +- *(1) Generate explode(features#707), false, [features#715]\n",
      ":        +- *(1) Filter ((size(features#707, true) > 0) AND isnotnull(features#707))\n",
      ":           +- FileScan geojson [features#707] Batched: false, DataFilters: [(size(features#707, true) > 0), isnotnull(features#707)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "+- Union\n",
      "   :- Project [DR_NO#552, Date Rptd#553, DATE OCC#554, TIME OCC#555, AREA #556, AREA NAME#557, Rpt Dist No#558, Part 1-2#559, Crm Cd#560, Crm Cd Desc#561, Mocodes#562, Vict Age#563, Vict Sex#564, Vict Descent#565, Premis Cd#566, Premis Desc#567, Weapon Used Cd#568, Weapon Desc#569, Status#570, Status Desc#571, Crm Cd 1#572, Crm Cd 2#573, Crm Cd 3#574, Crm Cd 4#575, ... 5 more fields]\n",
      "   :  +- Filter ((((isnotnull(LAT#578) AND NOT (cast(LAT#578 as int) = 0)) AND isnotnull(LON#579)) AND NOT (cast(LON#579 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "   :     +- FileScan csv [DR_NO#552,Date Rptd#553,DATE OCC#554,TIME OCC#555,AREA #556,AREA NAME#557,Rpt Dist No#558,Part 1-2#559,Crm Cd#560,Crm Cd Desc#561,Mocodes#562,Vict Age#563,Vict Sex#564,Vict Descent#565,Premis Cd#566,Premis Desc#567,Weapon Used Cd#568,Weapon Desc#569,Status#570,Status Desc#571,Crm Cd 1#572,Crm Cd 2#573,Crm Cd 3#574,Crm Cd 4#575,... 4 more fields] Batched: false, DataFilters: [isnotnull(LAT#578), NOT (cast(LAT#578 as int) = 0), isnotnull(LON#579), NOT (cast(LON#579 as int..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA :string,AREA NAME:strin...\n",
      "   +- Project [DR_NO#626, Date Rptd#627, DATE OCC#628, TIME OCC#629, AREA#630, AREA NAME#631, Rpt Dist No#632, Part 1-2#633, Crm Cd#634, Crm Cd Desc#635, Mocodes#636, Vict Age#637, Vict Sex#638, Vict Descent#639, Premis Cd#640, Premis Desc#641, Weapon Used Cd#642, Weapon Desc#643, Status#644, Status Desc#645, Crm Cd 1#646, Crm Cd 2#647, Crm Cd 3#648, Crm Cd 4#649, ... 5 more fields]\n",
      "      +- Filter ((((isnotnull(LAT#652) AND NOT (cast(LAT#652 as int) = 0)) AND isnotnull(LON#653)) AND NOT (cast(LON#653 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "         +- FileScan csv [DR_NO#626,Date Rptd#627,DATE OCC#628,TIME OCC#629,AREA#630,AREA NAME#631,Rpt Dist No#632,Part 1-2#633,Crm Cd#634,Crm Cd Desc#635,Mocodes#636,Vict Age#637,Vict Sex#638,Vict Descent#639,Premis Cd#640,Premis Desc#641,Weapon Used Cd#642,Weapon Desc#643,Status#644,Status Desc#645,Crm Cd 1#646,Crm Cd 2#647,Crm Cd 3#648,Crm Cd 4#649,... 4 more fields] Batched: false, DataFilters: [isnotnull(LAT#652), NOT (cast(LAT#652 as int) = 0), isnotnull(LON#653), NOT (cast(LON#653 as int..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA:string,AREA NAME:string...\n",
      "\n",
      "\n",
      "\n",
      "Spatial join => 3109471 rows. Elapsed: 84.17 s (no hint)"
     ]
    }
   ],
   "source": [
    "# We run it once without any hints just to see what the catalyst optimizer will use on its own\n",
    "# Clear any cached data to ensure a fresh start\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "print(\"=== STEP 3: Spatial Join => blocks_with_crime (no hint) ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Build the join pipeline without a hint\n",
    "blocks_with_crime_no_hint = (\n",
    "    census\n",
    "    .join(\n",
    "        crimes,\n",
    "        ST_Within(F.col(\"point\"), F.col(\"geometry\")),\n",
    "        \"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show explain plan so we can see what strategy Spark chooses\n",
    "print(\"\\nExplain plan for no-hint join:\")\n",
    "blocks_with_crime_no_hint.explain()\n",
    "\n",
    "# Trigger the actual join by counting rows\n",
    "blocks_with_crime_count_no_hint = blocks_with_crime_no_hint.count()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nSpatial join => {blocks_with_crime_count_no_hint} rows. \"\n",
    "      f\"Elapsed: {elapsed:.2f} s (no hint)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b1fc55e-f3c6-4785-b067-12c317f135d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] Aggregation of all crimes in communities. 247 Rows \n",
      "\n",
      "+--------------------+-------------+\n",
      "|                COMM|crime_counter|\n",
      "+--------------------+-------------+\n",
      "|         Culver City|         1390|\n",
      "|Rosewood/East Gar...|          175|\n",
      "|      Toluca Terrace|          300|\n",
      "|        Elysian Park|         5714|\n",
      "|            Longwood|         3074|\n",
      "|         Pico Rivera|            2|\n",
      "|              Malibu|            1|\n",
      "|       Green Meadows|        23975|\n",
      "|    Cadillac-Corning|         4445|\n",
      "|            Mid-city|        11574|\n",
      "+--------------------+-------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "crime_per_community = (\n",
    "    blocks_with_crime.groupBy(\"COMM\")\n",
    "    .agg(F.count(\"*\").alias(\"crime_counter\"))\n",
    ")\n",
    "\n",
    "crime_per_community_count = crime_per_community.count()\n",
    "print(f\"[4] Aggregation of all crimes in communities. {crime_per_community_count} Rows \\n\")\n",
    "crime_per_community.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04bfb270-fbd1-45b3-8616-b2712fe8a18c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime per community rows: 323\n",
      "\n",
      "+--------------------+-------------+\n",
      "|                COMM|crime_counter|\n",
      "+--------------------+-------------+\n",
      "|         Culver City|         1390|\n",
      "|     North Lancaster|            0|\n",
      "|Rosewood/East Gar...|          175|\n",
      "|East Rancho Domin...|            0|\n",
      "|      Toluca Terrace|          300|\n",
      "|        Elysian Park|         5714|\n",
      "|            Longwood|         3074|\n",
      "|         Pico Rivera|            2|\n",
      "|              Malibu|            1|\n",
      "|       Green Meadows|        23975|\n",
      "+--------------------+-------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "no_crime_community = (\n",
    "    census.groupBy(\"COMM\")\n",
    ")\n",
    "\n",
    "no_crime_community = census.groupBy(\"COMM\").agg(\n",
    "    F.lit(0).alias(\"crime_counter\")\n",
    ")\n",
    "\n",
    "crime_per_community = crime_per_community.union(no_crime_community)\n",
    "\n",
    "crime_per_community = crime_per_community.groupBy(\"COMM\").agg(\n",
    "    F.max(\"crime_counter\").alias(\"crime_counter\")\n",
    ")\n",
    "\n",
    "crime_per_community_counter = crime_per_community.count()\n",
    "print(f\"Crime per community rows: {crime_per_community_counter}\\n\")\n",
    "crime_per_community.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47dbed1d-8c93-4793-9f36-414f94e72be1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] blocks => 109279 rows.\n",
      "\n",
      "+------+--------------------+--------+---------+\n",
      "|ZCTA10|                COMM|POP_2010|HOUSING10|\n",
      "+------+--------------------+--------+---------+\n",
      "| 93243|West Antelope Valley|       0|        0|\n",
      "| 93243|West Antelope Valley|       0|        0|\n",
      "| 93243|West Antelope Valley|       0|        0|\n",
      "|      |       South Edwards|       0|        0|\n",
      "|      |       South Edwards|       0|        0|\n",
      "| 93243|West Antelope Valley|       5|        5|\n",
      "|      |       South Edwards|       0|        0|\n",
      "| 90732|           San Pedro|      69|       26|\n",
      "| 91789|         Diamond Bar|      79|       26|\n",
      "| 90275| Rancho Palos Verdes|       0|        0|\n",
      "+------+--------------------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "[6] zip_pop_all => 1076 rows.\n",
      "\n",
      "+------+--------------------+----------+--------------------------------+\n",
      "|ZCTA10|                COMM|population|houses_per_zip_code_in_community|\n",
      "+------+--------------------+----------+--------------------------------+\n",
      "| 90211|       Beverly Hills|      8434|                            4052|\n",
      "| 90220|              Carson|         0|                               0|\n",
      "| 90701|             Artesia|     16522|                            4697|\n",
      "| 91789|     Rowland Heights|      3607|                            1269|\n",
      "| 90046|           Hollywood|     18586|                           11921|\n",
      "|      |            Altadena|         0|                               0|\n",
      "| 90201|              Cudahy|     23805|                            5770|\n",
      "| 90220|Rosewood/West Ran...|      3155|                             845|\n",
      "| 91301|           Calabasas|      2405|                             884|\n",
      "| 91773|            Glendora|         0|                               0|\n",
      "+------+--------------------+----------+--------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "=== STEP 7: norm_inc (cleaned income data) ===\n",
      "[7] norm_inc => 284 rows.\n",
      "\n",
      "+--------+-------+\n",
      "|zip_code|med_inc|\n",
      "+--------+-------+\n",
      "|   90001|  33887|\n",
      "|   90002|  30413|\n",
      "|   90003|  30805|\n",
      "|   90004|  40612|\n",
      "|   90005|  31142|\n",
      "|   90006|  31521|\n",
      "|   90007|  22304|\n",
      "|   90008|  36564|\n",
      "|   90010|  45786|\n",
      "|   90011|  30251|\n",
      "+--------+-------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# Aggregating population and income stats\n",
    "blocks = (\n",
    "    census\n",
    "    .select(\n",
    "        \"ZCTA10\",\n",
    "        \"COMM\",\n",
    "        \"POP_2010\",\n",
    "        \"HOUSING10\"\n",
    "    )\n",
    ")\n",
    "# Force evaluation\n",
    "blocks_count = blocks.count()\n",
    "print(f\"[4] blocks => {blocks_count} rows.\\n\")\n",
    "\n",
    "blocks.show(10)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# STEP 5: Aggregate final_blocks => zip_pop_crime_all\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "zip_pop_all = (\n",
    "    blocks\n",
    "    .groupBy(\"ZCTA10\", \"COMM\")\n",
    "    .agg(\n",
    "        F.sum(\"POP_2010\").alias(\"population\"),\n",
    "        F.sum(\"HOUSING10\").alias(\"houses_per_zip_code_in_community\")\n",
    "    )\n",
    ")\n",
    "\n",
    "zip_pop_all_count = zip_pop_all.count()\n",
    "print(f\"[6] zip_pop_all => {zip_pop_all_count} rows.\\n\")\n",
    "zip_pop_all.show(10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Formatting income, we don't time this\n",
    "print(\"=== STEP 7: norm_inc (cleaned income data) ===\")\n",
    "norm_inc = (\n",
    "    income\n",
    "    .select(\n",
    "        F.col(\"Zip Code\").alias(\"zip_code\"),\n",
    "        F.regexp_replace(\n",
    "            F.regexp_replace(F.col(\"Estimated Median Income\"), \"\\\\$\", \"\"),\n",
    "            \",\", \"\"\n",
    "        ).cast(\"int\").alias(\"med_inc\")\n",
    "    )\n",
    ")\n",
    "\n",
    "norm_inc_count = norm_inc.count()\n",
    "print(f\"[7] norm_inc => {norm_inc_count} rows.\\n\")\n",
    "norm_inc.show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a47ed5a6-c232-4c0d-bcc8-8efa848709ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "=== STEP 7: Join zip_pop_all & norm_inc => final_result, Strategy: BROADCAST ===\n",
      "\n",
      "Explain plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [ZCTA10#748], [zip_code#2639], Inner, BuildRight, false\n",
      "   :- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[sum(POP_2010#740L), sum(HOUSING10#737L)], schema specialized)\n",
      "   :  +- Exchange hashpartitioning(ZCTA10#748, COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=3943]\n",
      "   :     +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[partial_sum(POP_2010#740L), partial_sum(HOUSING10#737L)], schema specialized)\n",
      "   :        +- Project [features#715.properties.ZCTA10 AS ZCTA10#748, features#715.properties.COMM AS COMM#731, features#715.properties.POP_2010 AS POP_2010#740L, features#715.properties.HOUSING10 AS HOUSING10#737L]\n",
      "   :           +- Filter (isnotnull(features#715.geometry) AND isnotnull(features#715.properties.ZCTA10))\n",
      "   :              +- Generate explode(features#707), false, [features#715]\n",
      "   :                 +- Filter ((size(features#707, true) > 0) AND isnotnull(features#707))\n",
      "   :                    +- FileScan geojson [features#707] Batched: false, DataFilters: [(size(features#707, true) > 0), isnotnull(features#707)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=3946]\n",
      "      +- Project [Zip Code#700 AS zip_code#2639, cast(regexp_replace(regexp_replace(Estimated Median Income#702, \\$, , 1), ,, , 1) as int) AS med_inc#2640]\n",
      "         +- Filter isnotnull(Zip Code#700)\n",
      "            +- FileScan csv [Zip Code#700,Estimated Median Income#702] Batched: false, DataFilters: [isnotnull(Zip Code#700)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "\n",
      "\n",
      "Join completed in 11.03 seconds with 1042 rows using BROADCAST.\n",
      "\n",
      "============================================================\n",
      "=== STEP 7: Join zip_pop_all & norm_inc => final_result, Strategy: MERGE ===\n",
      "\n",
      "Explain plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [ZCTA10#748], [zip_code#2639], Inner\n",
      "   :- Sort [ZCTA10#748 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(ZCTA10#748, 1000), ENSURE_REQUIREMENTS, [plan_id=4298]\n",
      "   :     +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[sum(POP_2010#740L), sum(HOUSING10#737L)], schema specialized)\n",
      "   :        +- Exchange hashpartitioning(ZCTA10#748, COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=4294]\n",
      "   :           +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[partial_sum(POP_2010#740L), partial_sum(HOUSING10#737L)], schema specialized)\n",
      "   :              +- Project [features#715.properties.ZCTA10 AS ZCTA10#748, features#715.properties.COMM AS COMM#731, features#715.properties.POP_2010 AS POP_2010#740L, features#715.properties.HOUSING10 AS HOUSING10#737L]\n",
      "   :                 +- Filter (isnotnull(features#715.geometry) AND isnotnull(features#715.properties.ZCTA10))\n",
      "   :                    +- Generate explode(features#707), false, [features#715]\n",
      "   :                       +- Filter ((size(features#707, true) > 0) AND isnotnull(features#707))\n",
      "   :                          +- FileScan geojson [features#707] Batched: false, DataFilters: [(size(features#707, true) > 0), isnotnull(features#707)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "   +- Sort [zip_code#2639 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(zip_code#2639, 1000), ENSURE_REQUIREMENTS, [plan_id=4299]\n",
      "         +- Project [Zip Code#700 AS zip_code#2639, cast(regexp_replace(regexp_replace(Estimated Median Income#702, \\$, , 1), ,, , 1) as int) AS med_inc#2640]\n",
      "            +- Filter isnotnull(Zip Code#700)\n",
      "               +- FileScan csv [Zip Code#700,Estimated Median Income#702] Batched: false, DataFilters: [isnotnull(Zip Code#700)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "\n",
      "\n",
      "Join completed in 11.57 seconds with 1042 rows using MERGE.\n",
      "\n",
      "============================================================\n",
      "=== STEP 7: Join zip_pop_all & norm_inc => final_result, Strategy: SHUFFLE_HASH ===\n",
      "\n",
      "Explain plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- ShuffledHashJoin [ZCTA10#748], [zip_code#2639], Inner, BuildRight\n",
      "   :- Exchange hashpartitioning(ZCTA10#748, 1000), ENSURE_REQUIREMENTS, [plan_id=4792]\n",
      "   :  +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[sum(POP_2010#740L), sum(HOUSING10#737L)], schema specialized)\n",
      "   :     +- Exchange hashpartitioning(ZCTA10#748, COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=4788]\n",
      "   :        +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[partial_sum(POP_2010#740L), partial_sum(HOUSING10#737L)], schema specialized)\n",
      "   :           +- Project [features#715.properties.ZCTA10 AS ZCTA10#748, features#715.properties.COMM AS COMM#731, features#715.properties.POP_2010 AS POP_2010#740L, features#715.properties.HOUSING10 AS HOUSING10#737L]\n",
      "   :              +- Filter (isnotnull(features#715.geometry) AND isnotnull(features#715.properties.ZCTA10))\n",
      "   :                 +- Generate explode(features#707), false, [features#715]\n",
      "   :                    +- Filter ((size(features#707, true) > 0) AND isnotnull(features#707))\n",
      "   :                       +- FileScan geojson [features#707] Batched: false, DataFilters: [(size(features#707, true) > 0), isnotnull(features#707)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "   +- Exchange hashpartitioning(zip_code#2639, 1000), ENSURE_REQUIREMENTS, [plan_id=4793]\n",
      "      +- Project [Zip Code#700 AS zip_code#2639, cast(regexp_replace(regexp_replace(Estimated Median Income#702, \\$, , 1), ,, , 1) as int) AS med_inc#2640]\n",
      "         +- Filter isnotnull(Zip Code#700)\n",
      "            +- FileScan csv [Zip Code#700,Estimated Median Income#702] Batched: false, DataFilters: [isnotnull(Zip Code#700)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "\n",
      "\n",
      "Join completed in 14.19 seconds with 1042 rows using SHUFFLE_HASH.\n",
      "\n",
      "============================================================\n",
      "=== STEP 7: Join zip_pop_all & norm_inc => final_result, Strategy: SHUFFLE_REPLICATE_NL ===\n",
      "\n",
      "Explain plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- CartesianProduct (ZCTA10#748 = zip_code#2639)\n",
      "   :- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[sum(POP_2010#740L), sum(HOUSING10#737L)], schema specialized)\n",
      "   :  +- Exchange hashpartitioning(ZCTA10#748, COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=5229]\n",
      "   :     +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[partial_sum(POP_2010#740L), partial_sum(HOUSING10#737L)], schema specialized)\n",
      "   :        +- Project [features#715.properties.ZCTA10 AS ZCTA10#748, features#715.properties.COMM AS COMM#731, features#715.properties.POP_2010 AS POP_2010#740L, features#715.properties.HOUSING10 AS HOUSING10#737L]\n",
      "   :           +- Filter (isnotnull(features#715.geometry) AND isnotnull(features#715.properties.ZCTA10))\n",
      "   :              +- Generate explode(features#707), false, [features#715]\n",
      "   :                 +- Filter ((size(features#707, true) > 0) AND isnotnull(features#707))\n",
      "   :                    +- FileScan geojson [features#707] Batched: false, DataFilters: [(size(features#707, true) > 0), isnotnull(features#707)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "   +- Project [Zip Code#700 AS zip_code#2639, cast(regexp_replace(regexp_replace(Estimated Median Income#702, \\$, , 1), ,, , 1) as int) AS med_inc#2640]\n",
      "      +- Filter isnotnull(Zip Code#700)\n",
      "         +- FileScan csv [Zip Code#700,Estimated Median Income#702] Batched: false, DataFilters: [isnotnull(Zip Code#700)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "\n",
      "\n",
      "Join completed in 10.90 seconds with 1042 rows using SHUFFLE_REPLICATE_NL.\n",
      "\n",
      "+------+--------------------+----------+--------------------------------+--------+-------+\n",
      "|ZCTA10|                COMM|population|houses_per_zip_code_in_community|zip_code|med_inc|\n",
      "+------+--------------------+----------+--------------------------------+--------+-------+\n",
      "| 90211|       Beverly Hills|      8434|                            4052|   90211|  69638|\n",
      "| 90220|              Carson|         0|                               0|   90220|  47882|\n",
      "| 90701|             Artesia|     16522|                            4697|   90701|  60749|\n",
      "| 91789|     Rowland Heights|      3607|                            1269|   91789|  93301|\n",
      "| 90046|           Hollywood|     18586|                           11921|   90046|  57941|\n",
      "| 90201|              Cudahy|     23805|                            5770|   90201|  37254|\n",
      "| 90220|Rosewood/West Ran...|      3155|                             845|   90220|  47882|\n",
      "| 91301|           Calabasas|      2405|                             884|   91301| 114417|\n",
      "| 91773|            Glendora|         0|                               0|   91773|  77742|\n",
      "| 90077|        Sherman Oaks|        63|                              28|   90077| 164281|\n",
      "+------+--------------------+----------+--------------------------------+--------+-------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "join_strategies = [\"BROADCAST\", \"MERGE\", \"SHUFFLE_HASH\", \"SHUFFLE_REPLICATE_NL\"]\n",
    "\n",
    "for strategy in join_strategies:\n",
    "    print(\"============================================================\")\n",
    "    print(f\"=== STEP 7: Join zip_pop_all & norm_inc => final_result, Strategy: {strategy} ===\")\n",
    "    \n",
    "    # Clear any cached data so previous strategies don't affect this one\n",
    "    spark.catalog.clearCache()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Apply hints for both DataFrames so Spark respects the join strategy\n",
    "    joined_data = (\n",
    "        zip_pop_all\n",
    "        .hint(strategy)\n",
    "        .join(\n",
    "            norm_inc.hint(strategy),\n",
    "            zip_pop_all.ZCTA10 == norm_inc.zip_code,\n",
    "            \"inner\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Show the explain plan to confirm which strategy Spark actually uses\n",
    "    print(\"\\nExplain plan:\")\n",
    "    joined_data.explain()\n",
    "    \n",
    "    # Force execution of the join by counting rows\n",
    "    joined_data_counter = joined_data.count()\n",
    "    \n",
    "    join_time = time.time() - start_time\n",
    "    print(f\"\\nJoin completed in {join_time:.2f} seconds with {joined_data_counter} rows using {strategy}.\\n\")\n",
    "    \n",
    "# (Optional) Show a few rows\n",
    "joined_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "029749fa-3567-4ad0-968f-f876bf3dae77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "=== STEP 7: Join zip_pop_crime_all & norm_inc => final_result, No Hint ===\n",
      "\n",
      "Explain plan (no hint):\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [ZCTA10#748], [zip_code#2639], Inner, BuildRight, false\n",
      "   :- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[sum(POP_2010#740L), sum(HOUSING10#737L)], schema specialized)\n",
      "   :  +- Exchange hashpartitioning(ZCTA10#748, COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=5698]\n",
      "   :     +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[partial_sum(POP_2010#740L), partial_sum(HOUSING10#737L)], schema specialized)\n",
      "   :        +- Project [features#715.properties.ZCTA10 AS ZCTA10#748, features#715.properties.COMM AS COMM#731, features#715.properties.POP_2010 AS POP_2010#740L, features#715.properties.HOUSING10 AS HOUSING10#737L]\n",
      "   :           +- Filter (isnotnull(features#715.geometry) AND isnotnull(features#715.properties.ZCTA10))\n",
      "   :              +- Generate explode(features#707), false, [features#715]\n",
      "   :                 +- Filter ((size(features#707, true) > 0) AND isnotnull(features#707))\n",
      "   :                    +- FileScan geojson [features#707] Batched: false, DataFilters: [(size(features#707, true) > 0), isnotnull(features#707)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=5701]\n",
      "      +- Project [Zip Code#700 AS zip_code#2639, cast(regexp_replace(regexp_replace(Estimated Median Income#702, \\$, , 1), ,, , 1) as int) AS med_inc#2640]\n",
      "         +- Filter isnotnull(Zip Code#700)\n",
      "            +- FileScan csv [Zip Code#700,Estimated Median Income#702] Batched: false, DataFilters: [isnotnull(Zip Code#700)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "\n",
      "\n",
      "Join completed in 9.62 seconds with 1042 rows (no hint).\n",
      "\n",
      "+------+--------------------+----------+--------------------------------+--------+-------+\n",
      "|ZCTA10|                COMM|population|houses_per_zip_code_in_community|zip_code|med_inc|\n",
      "+------+--------------------+----------+--------------------------------+--------+-------+\n",
      "| 90211|       Beverly Hills|      8434|                            4052|   90211|  69638|\n",
      "| 90220|              Carson|         0|                               0|   90220|  47882|\n",
      "| 90701|             Artesia|     16522|                            4697|   90701|  60749|\n",
      "| 91789|     Rowland Heights|      3607|                            1269|   91789|  93301|\n",
      "| 90046|           Hollywood|     18586|                           11921|   90046|  57941|\n",
      "| 90201|              Cudahy|     23805|                            5770|   90201|  37254|\n",
      "| 90220|Rosewood/West Ran...|      3155|                             845|   90220|  47882|\n",
      "| 91301|           Calabasas|      2405|                             884|   91301| 114417|\n",
      "| 91773|            Glendora|         0|                               0|   91773|  77742|\n",
      "| 90077|        Sherman Oaks|        63|                              28|   90077| 164281|\n",
      "+------+--------------------+----------+--------------------------------+--------+-------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Clear any cached data so previous runs don't affect this one\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "print(\"============================================================\")\n",
    "print(\"=== STEP 7: Join zip_pop_crime_all & norm_inc => final_result, No Hint ===\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform the join without any hint\n",
    "joined_data_no_hint = (\n",
    "    zip_pop_all\n",
    "    .join(\n",
    "        norm_inc,\n",
    "        zip_pop_all.ZCTA10 == norm_inc.zip_code,\n",
    "        \"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the explain plan to see what strategy Spark chooses automatically\n",
    "print(\"\\nExplain plan (no hint):\")\n",
    "joined_data_no_hint.explain()\n",
    "\n",
    "# Force execution of the join by counting rows\n",
    "joined_data_no_hint_count = joined_data_no_hint.count()\n",
    "\n",
    "join_time = time.time() - start_time\n",
    "print(f\"\\nJoin completed in {join_time:.2f} seconds \"\n",
    "      f\"with {joined_data_no_hint_count} rows (no hint).\\n\")\n",
    "\n",
    "# (Optional) show a few rows\n",
    "joined_data_no_hint.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7610bba-94ca-45cf-8cdc-43c0f8f45d97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unioned data rows: 1076"
     ]
    }
   ],
   "source": [
    "# copy the population for the income (we will make zero anything without income data)\n",
    "joined_data = joined_data.withColumn(\"population_for_income\", joined_data.population)\n",
    "# Drop the zip_code column\n",
    "joined_data = joined_data.drop(\"zip_code\")\n",
    "\n",
    "\n",
    "# Add the med_inc and population_for_income to the zip_pop_crime_all to do the left join\n",
    "zip_pop_all = zip_pop_all.withColumn(\"med_inc\", F.lit(0)) \\\n",
    "                                     .withColumn(\"population_for_income\", F.lit(0))\n",
    "\n",
    "# Perform the union of the two DataFrames\n",
    "unioned_data = joined_data.union(zip_pop_all)\n",
    "\n",
    "# Get the column names to group by (all columns except \"population_for_income\" and \"med_inc\")\n",
    "group_by_columns = [\"ZCTA10\", \"COMM\",\"population\",\"houses_per_zip_code_in_community\"]\n",
    "\n",
    "# Perform the grouping and aggregation\n",
    "income_data = unioned_data.groupBy(*group_by_columns).agg(\n",
    "    F.max(\"population_for_income\").alias(\"population_for_income\"),\n",
    "    F.max(\"med_inc\").alias(\"med_inc\")\n",
    ")\n",
    "\n",
    "income_data_counter = income_data.count()\n",
    "print(f\"Unioned data rows: {income_data_counter}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b407fcf6-1fc5-4a26-86d6-35c3f1f53e54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income result table rows: 323"
     ]
    }
   ],
   "source": [
    "income_result = income_data.groupBy(\"COMM\").agg(\n",
    "    F.expr(\"CAST(SUM(med_inc * houses_per_zip_code_in_community) / SUM(population_for_income) AS INT)\").alias(\"average_income\"),\n",
    "    F.expr(\"SUM(population)\").alias(\"population\")\n",
    ")\n",
    "\n",
    "income_result_counter = income_result.count()\n",
    "print(f\"Income result table rows: {income_result_counter}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99ee4d0b-d012-409f-baf8-e56a72f473ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "=== Join income_result & crime_per_community => all_join, Strategy: BROADCAST ===\n",
      "\n",
      "Explain plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#731, average_income#3046, population#3047L, crime_counter#2482L]\n",
      "   +- BroadcastHashJoin [COMM#731], [COMM#3087], Inner, BuildLeft, false\n",
      "      :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=7430]\n",
      "      :  +- HashAggregate(keys=[COMM#731], functions=[sum((cast(med_inc#2992 as bigint) * houses_per_zip_code_in_community#2586L)), sum(population_for_income#2990L), sum(population#2584L)], schema specialized)\n",
      "      :     +- Exchange hashpartitioning(COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=7418]\n",
      "      :        +- HashAggregate(keys=[COMM#731], functions=[partial_sum((cast(med_inc#2992 as bigint) * houses_per_zip_code_in_community#2586L)), partial_sum(population_for_income#2990L), partial_sum(population#2584L)], schema specialized)\n",
      "      :           +- HashAggregate(keys=[ZCTA10#748, COMM#731, population#2584L, houses_per_zip_code_in_community#2586L], functions=[max(population_for_income#2938L), max(med_inc#2640)], schema specialized)\n",
      "      :              +- Exchange hashpartitioning(ZCTA10#748, COMM#731, population#2584L, houses_per_zip_code_in_community#2586L, 1000), ENSURE_REQUIREMENTS, [plan_id=7414]\n",
      "      :                 +- HashAggregate(keys=[ZCTA10#748, COMM#731, population#2584L, houses_per_zip_code_in_community#2586L], functions=[partial_max(population_for_income#2938L), partial_max(med_inc#2640)], schema specialized)\n",
      "      :                    +- Union\n",
      "      :                       :- Project [ZCTA10#748, COMM#731, population#2584L, houses_per_zip_code_in_community#2586L, med_inc#2640, population#2584L AS population_for_income#2938L]\n",
      "      :                       :  +- CartesianProduct (ZCTA10#748 = zip_code#2639)\n",
      "      :                       :     :- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[sum(POP_2010#740L), sum(HOUSING10#737L)], schema specialized)\n",
      "      :                       :     :  +- Exchange hashpartitioning(ZCTA10#748, COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=7405]\n",
      "      :                       :     :     +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[partial_sum(POP_2010#740L), partial_sum(HOUSING10#737L)], schema specialized)\n",
      "      :                       :     :        +- Project [features#715.properties.ZCTA10 AS ZCTA10#748, features#715.properties.COMM AS COMM#731, features#715.properties.POP_2010 AS POP_2010#740L, features#715.properties.HOUSING10 AS HOUSING10#737L]\n",
      "      :                       :     :           +- Filter (isnotnull(features#715.geometry) AND (isnotnull(features#715.properties.ZCTA10) AND isnotnull(features#715.properties.COMM)))\n",
      "      :                       :     :              +- Generate explode(features#707), false, [features#715]\n",
      "      :                       :     :                 +- Filter ((size(features#707, true) > 0) AND isnotnull(features#707))\n",
      "      :                       :     :                    +- FileScan geojson [features#707] Batched: false, DataFilters: [(size(features#707, true) > 0), isnotnull(features#707)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      :                       :     +- Project [Zip Code#700 AS zip_code#2639, cast(regexp_replace(regexp_replace(Estimated Median Income#702, \\$, , 1), ,, , 1) as int) AS med_inc#2640]\n",
      "      :                       :        +- Filter isnotnull(Zip Code#700)\n",
      "      :                       :           +- FileScan csv [Zip Code#700,Estimated Median Income#702] Batched: false, DataFilters: [isnotnull(Zip Code#700)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "      :                       +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[sum(POP_2010#740L), sum(HOUSING10#737L)], schema specialized)\n",
      "      :                          +- Exchange hashpartitioning(ZCTA10#748, COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=7409]\n",
      "      :                             +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[partial_sum(POP_2010#740L), partial_sum(HOUSING10#737L)], schema specialized)\n",
      "      :                                +- Project [features#715.properties.ZCTA10 AS ZCTA10#748, features#715.properties.COMM AS COMM#731, features#715.properties.POP_2010 AS POP_2010#740L, features#715.properties.HOUSING10 AS HOUSING10#737L]\n",
      "      :                                   +- Filter (isnotnull(features#715.geometry) AND isnotnull(features#715.properties.COMM))\n",
      "      :                                      +- Generate explode(features#2966), false, [features#715]\n",
      "      :                                         +- Filter ((size(features#2966, true) > 0) AND isnotnull(features#2966))\n",
      "      :                                            +- FileScan geojson [features#2966] Batched: false, DataFilters: [(size(features#2966, true) > 0), isnotnull(features#2966)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      +- HashAggregate(keys=[COMM#3087], functions=[max(crime_counter#2373L)], schema specialized)\n",
      "         +- Exchange hashpartitioning(COMM#3087, 1000), ENSURE_REQUIREMENTS, [plan_id=7427]\n",
      "            +- HashAggregate(keys=[COMM#3087], functions=[partial_max(crime_counter#2373L)], schema specialized)\n",
      "               +- Union\n",
      "                  :- HashAggregate(keys=[COMM#3087], functions=[count(1)], schema specialized)\n",
      "                  :  +- Exchange hashpartitioning(COMM#3087, 1000), ENSURE_REQUIREMENTS, [plan_id=7420]\n",
      "                  :     +- HashAggregate(keys=[COMM#3087], functions=[partial_count(1)], schema specialized)\n",
      "                  :        +- Project [COMM#3087]\n",
      "                  :           +- RangeJoin geometry#718: geometry, point#882: geometry, CONTAINS\n",
      "                  :              :- Project [features#715.properties.COMM AS COMM#3087, features#715.geometry AS geometry#718]\n",
      "                  :              :  +- Filter (isnotnull(features#715.geometry) AND isnotnull(features#715.properties.COMM))\n",
      "                  :              :     +- Generate explode(features#3077), false, [features#715]\n",
      "                  :              :        +- Filter ((size(features#3077, true) > 0) AND isnotnull(features#3077))\n",
      "                  :              :           +- FileScan geojson [features#3077] Batched: false, DataFilters: [(size(features#3077, true) > 0), isnotnull(features#3077)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "                  :              +- Union\n",
      "                  :                 :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#882]\n",
      "                  :                 :  +- Filter ((((isnotnull(LAT#578) AND NOT (cast(LAT#578 as int) = 0)) AND isnotnull(LON#579)) AND NOT (cast(LON#579 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                  :                 :     +- FileScan csv [LAT#578,LON#579] Batched: false, DataFilters: [isnotnull(LAT#578), NOT (cast(LAT#578 as int) = 0), isnotnull(LON#579), NOT (cast(LON#579 as int..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<LAT:string,LON:string>\n",
      "                  :                 +- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#3110]\n",
      "                  :                    +- Filter ((((isnotnull(LAT#652) AND NOT (cast(LAT#652 as int) = 0)) AND isnotnull(LON#653)) AND NOT (cast(LON#653 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                  :                       +- FileScan csv [LAT#652,LON#653] Batched: false, DataFilters: [isnotnull(LAT#652), NOT (cast(LAT#652 as int) = 0), isnotnull(LON#653), NOT (cast(LON#653 as int..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<LAT:string,LON:string>\n",
      "                  +- HashAggregate(keys=[COMM#731], functions=[], schema specialized)\n",
      "                     +- Exchange hashpartitioning(COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=7422]\n",
      "                        +- HashAggregate(keys=[COMM#731], functions=[], schema specialized)\n",
      "                           +- Project [features#715.properties.COMM AS COMM#731]\n",
      "                              +- Filter (isnotnull(features#715.geometry) AND isnotnull(features#715.properties.COMM))\n",
      "                                 +- Generate explode(features#2470), false, [features#715]\n",
      "                                    +- Filter ((size(features#2470, true) > 0) AND isnotnull(features#2470))\n",
      "                                       +- FileScan geojson [features#2470] Batched: false, DataFilters: [(size(features#2470, true) > 0), isnotnull(features#2470)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "\n",
      "\n",
      "Join completed in 76.49 seconds with 323 rows using BROADCAST.\n",
      "\n",
      "====================================================\n",
      "=== Join income_result & crime_per_community => all_join, Strategy: MERGE ===\n",
      "\n",
      "Explain plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#731, average_income#3046, population#3047L, crime_counter#2482L]\n",
      "   +- SortMergeJoin [COMM#731], [COMM#3200], Inner\n",
      "      :- Sort [COMM#731 ASC NULLS FIRST], false, 0\n",
      "      :  +- HashAggregate(keys=[COMM#731], functions=[sum((cast(med_inc#2992 as bigint) * houses_per_zip_code_in_community#2586L)), sum(population_for_income#2990L), sum(population#2584L)], schema specialized)\n",
      "      :     +- Exchange hashpartitioning(COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=8816]\n",
      "      :        +- HashAggregate(keys=[COMM#731], functions=[partial_sum((cast(med_inc#2992 as bigint) * houses_per_zip_code_in_community#2586L)), partial_sum(population_for_income#2990L), partial_sum(population#2584L)], schema specialized)\n",
      "      :           +- HashAggregate(keys=[ZCTA10#748, COMM#731, population#2584L, houses_per_zip_code_in_community#2586L], functions=[max(population_for_income#2938L), max(med_inc#2640)], schema specialized)\n",
      "      :              +- Exchange hashpartitioning(ZCTA10#748, COMM#731, population#2584L, houses_per_zip_code_in_community#2586L, 1000), ENSURE_REQUIREMENTS, [plan_id=8812]\n",
      "      :                 +- HashAggregate(keys=[ZCTA10#748, COMM#731, population#2584L, houses_per_zip_code_in_community#2586L], functions=[partial_max(population_for_income#2938L), partial_max(med_inc#2640)], schema specialized)\n",
      "      :                    +- Union\n",
      "      :                       :- Project [ZCTA10#748, COMM#731, population#2584L, houses_per_zip_code_in_community#2586L, med_inc#2640, population#2584L AS population_for_income#2938L]\n",
      "      :                       :  +- CartesianProduct (ZCTA10#748 = zip_code#2639)\n",
      "      :                       :     :- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[sum(POP_2010#740L), sum(HOUSING10#737L)], schema specialized)\n",
      "      :                       :     :  +- Exchange hashpartitioning(ZCTA10#748, COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=8803]\n",
      "      :                       :     :     +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[partial_sum(POP_2010#740L), partial_sum(HOUSING10#737L)], schema specialized)\n",
      "      :                       :     :        +- Project [features#715.properties.ZCTA10 AS ZCTA10#748, features#715.properties.COMM AS COMM#731, features#715.properties.POP_2010 AS POP_2010#740L, features#715.properties.HOUSING10 AS HOUSING10#737L]\n",
      "      :                       :     :           +- Filter (isnotnull(features#715.geometry) AND (isnotnull(features#715.properties.ZCTA10) AND isnotnull(features#715.properties.COMM)))\n",
      "      :                       :     :              +- Generate explode(features#707), false, [features#715]\n",
      "      :                       :     :                 +- Filter ((size(features#707, true) > 0) AND isnotnull(features#707))\n",
      "      :                       :     :                    +- FileScan geojson [features#707] Batched: false, DataFilters: [(size(features#707, true) > 0), isnotnull(features#707)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      :                       :     +- Project [Zip Code#700 AS zip_code#2639, cast(regexp_replace(regexp_replace(Estimated Median Income#702, \\$, , 1), ,, , 1) as int) AS med_inc#2640]\n",
      "      :                       :        +- Filter isnotnull(Zip Code#700)\n",
      "      :                       :           +- FileScan csv [Zip Code#700,Estimated Median Income#702] Batched: false, DataFilters: [isnotnull(Zip Code#700)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "      :                       +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[sum(POP_2010#740L), sum(HOUSING10#737L)], schema specialized)\n",
      "      :                          +- Exchange hashpartitioning(ZCTA10#748, COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=8807]\n",
      "      :                             +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[partial_sum(POP_2010#740L), partial_sum(HOUSING10#737L)], schema specialized)\n",
      "      :                                +- Project [features#715.properties.ZCTA10 AS ZCTA10#748, features#715.properties.COMM AS COMM#731, features#715.properties.POP_2010 AS POP_2010#740L, features#715.properties.HOUSING10 AS HOUSING10#737L]\n",
      "      :                                   +- Filter (isnotnull(features#715.geometry) AND isnotnull(features#715.properties.COMM))\n",
      "      :                                      +- Generate explode(features#2966), false, [features#715]\n",
      "      :                                         +- Filter ((size(features#2966, true) > 0) AND isnotnull(features#2966))\n",
      "      :                                            +- FileScan geojson [features#2966] Batched: false, DataFilters: [(size(features#2966, true) > 0), isnotnull(features#2966)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      +- Sort [COMM#3200 ASC NULLS FIRST], false, 0\n",
      "         +- HashAggregate(keys=[COMM#3200], functions=[max(crime_counter#2373L)], schema specialized)\n",
      "            +- Exchange hashpartitioning(COMM#3200, 1000), ENSURE_REQUIREMENTS, [plan_id=8825]\n",
      "               +- HashAggregate(keys=[COMM#3200], functions=[partial_max(crime_counter#2373L)], schema specialized)\n",
      "                  +- Union\n",
      "                     :- HashAggregate(keys=[COMM#3200], functions=[count(1)], schema specialized)\n",
      "                     :  +- Exchange hashpartitioning(COMM#3200, 1000), ENSURE_REQUIREMENTS, [plan_id=8818]\n",
      "                     :     +- HashAggregate(keys=[COMM#3200], functions=[partial_count(1)], schema specialized)\n",
      "                     :        +- Project [COMM#3200]\n",
      "                     :           +- RangeJoin geometry#718: geometry, point#882: geometry, CONTAINS\n",
      "                     :              :- Project [features#715.properties.COMM AS COMM#3200, features#715.geometry AS geometry#718]\n",
      "                     :              :  +- Filter (isnotnull(features#715.geometry) AND isnotnull(features#715.properties.COMM))\n",
      "                     :              :     +- Generate explode(features#3190), false, [features#715]\n",
      "                     :              :        +- Filter ((size(features#3190, true) > 0) AND isnotnull(features#3190))\n",
      "                     :              :           +- FileScan geojson [features#3190] Batched: false, DataFilters: [(size(features#3190, true) > 0), isnotnull(features#3190)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "                     :              +- Union\n",
      "                     :                 :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#882]\n",
      "                     :                 :  +- Filter ((((isnotnull(LAT#578) AND NOT (cast(LAT#578 as int) = 0)) AND isnotnull(LON#579)) AND NOT (cast(LON#579 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                     :                 :     +- FileScan csv [LAT#578,LON#579] Batched: false, DataFilters: [isnotnull(LAT#578), NOT (cast(LAT#578 as int) = 0), isnotnull(LON#579), NOT (cast(LON#579 as int..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<LAT:string,LON:string>\n",
      "                     :                 +- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#3223]\n",
      "                     :                    +- Filter ((((isnotnull(LAT#652) AND NOT (cast(LAT#652 as int) = 0)) AND isnotnull(LON#653)) AND NOT (cast(LON#653 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                     :                       +- FileScan csv [LAT#652,LON#653] Batched: false, DataFilters: [isnotnull(LAT#652), NOT (cast(LAT#652 as int) = 0), isnotnull(LON#653), NOT (cast(LON#653 as int..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<LAT:string,LON:string>\n",
      "                     +- HashAggregate(keys=[COMM#731], functions=[], schema specialized)\n",
      "                        +- Exchange hashpartitioning(COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=8820]\n",
      "                           +- HashAggregate(keys=[COMM#731], functions=[], schema specialized)\n",
      "                              +- Project [features#715.properties.COMM AS COMM#731]\n",
      "                                 +- Filter (isnotnull(features#715.geometry) AND isnotnull(features#715.properties.COMM))\n",
      "                                    +- Generate explode(features#2470), false, [features#715]\n",
      "                                       +- Filter ((size(features#2470, true) > 0) AND isnotnull(features#2470))\n",
      "                                          +- FileScan geojson [features#2470] Batched: false, DataFilters: [(size(features#2470, true) > 0), isnotnull(features#2470)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "\n",
      "\n",
      "Join completed in 80.19 seconds with 323 rows using MERGE.\n",
      "\n",
      "====================================================\n",
      "=== Join income_result & crime_per_community => all_join, Strategy: SHUFFLE_HASH ===\n",
      "\n",
      "Explain plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#731, average_income#3046, population#3047L, crime_counter#2482L]\n",
      "   +- ShuffledHashJoin [COMM#731], [COMM#3304], Inner, BuildLeft\n",
      "      :- HashAggregate(keys=[COMM#731], functions=[sum((cast(med_inc#2992 as bigint) * houses_per_zip_code_in_community#2586L)), sum(population_for_income#2990L), sum(population#2584L)], schema specialized)\n",
      "      :  +- Exchange hashpartitioning(COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=10462]\n",
      "      :     +- HashAggregate(keys=[COMM#731], functions=[partial_sum((cast(med_inc#2992 as bigint) * houses_per_zip_code_in_community#2586L)), partial_sum(population_for_income#2990L), partial_sum(population#2584L)], schema specialized)\n",
      "      :        +- HashAggregate(keys=[ZCTA10#748, COMM#731, population#2584L, houses_per_zip_code_in_community#2586L], functions=[max(population_for_income#2938L), max(med_inc#2640)], schema specialized)\n",
      "      :           +- Exchange hashpartitioning(ZCTA10#748, COMM#731, population#2584L, houses_per_zip_code_in_community#2586L, 1000), ENSURE_REQUIREMENTS, [plan_id=10458]\n",
      "      :              +- HashAggregate(keys=[ZCTA10#748, COMM#731, population#2584L, houses_per_zip_code_in_community#2586L], functions=[partial_max(population_for_income#2938L), partial_max(med_inc#2640)], schema specialized)\n",
      "      :                 +- Union\n",
      "      :                    :- Project [ZCTA10#748, COMM#731, population#2584L, houses_per_zip_code_in_community#2586L, med_inc#2640, population#2584L AS population_for_income#2938L]\n",
      "      :                    :  +- CartesianProduct (ZCTA10#748 = zip_code#2639)\n",
      "      :                    :     :- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[sum(POP_2010#740L), sum(HOUSING10#737L)], schema specialized)\n",
      "      :                    :     :  +- Exchange hashpartitioning(ZCTA10#748, COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=10449]\n",
      "      :                    :     :     +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[partial_sum(POP_2010#740L), partial_sum(HOUSING10#737L)], schema specialized)\n",
      "      :                    :     :        +- Project [features#715.properties.ZCTA10 AS ZCTA10#748, features#715.properties.COMM AS COMM#731, features#715.properties.POP_2010 AS POP_2010#740L, features#715.properties.HOUSING10 AS HOUSING10#737L]\n",
      "      :                    :     :           +- Filter (isnotnull(features#715.geometry) AND (isnotnull(features#715.properties.ZCTA10) AND isnotnull(features#715.properties.COMM)))\n",
      "      :                    :     :              +- Generate explode(features#707), false, [features#715]\n",
      "      :                    :     :                 +- Filter ((size(features#707, true) > 0) AND isnotnull(features#707))\n",
      "      :                    :     :                    +- FileScan geojson [features#707] Batched: false, DataFilters: [(size(features#707, true) > 0), isnotnull(features#707)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      :                    :     +- Project [Zip Code#700 AS zip_code#2639, cast(regexp_replace(regexp_replace(Estimated Median Income#702, \\$, , 1), ,, , 1) as int) AS med_inc#2640]\n",
      "      :                    :        +- Filter isnotnull(Zip Code#700)\n",
      "      :                    :           +- FileScan csv [Zip Code#700,Estimated Median Income#702] Batched: false, DataFilters: [isnotnull(Zip Code#700)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "      :                    +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[sum(POP_2010#740L), sum(HOUSING10#737L)], schema specialized)\n",
      "      :                       +- Exchange hashpartitioning(ZCTA10#748, COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=10453]\n",
      "      :                          +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[partial_sum(POP_2010#740L), partial_sum(HOUSING10#737L)], schema specialized)\n",
      "      :                             +- Project [features#715.properties.ZCTA10 AS ZCTA10#748, features#715.properties.COMM AS COMM#731, features#715.properties.POP_2010 AS POP_2010#740L, features#715.properties.HOUSING10 AS HOUSING10#737L]\n",
      "      :                                +- Filter (isnotnull(features#715.geometry) AND isnotnull(features#715.properties.COMM))\n",
      "      :                                   +- Generate explode(features#2966), false, [features#715]\n",
      "      :                                      +- Filter ((size(features#2966, true) > 0) AND isnotnull(features#2966))\n",
      "      :                                         +- FileScan geojson [features#2966] Batched: false, DataFilters: [(size(features#2966, true) > 0), isnotnull(features#2966)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      +- HashAggregate(keys=[COMM#3304], functions=[max(crime_counter#2373L)], schema specialized)\n",
      "         +- Exchange hashpartitioning(COMM#3304, 1000), ENSURE_REQUIREMENTS, [plan_id=10471]\n",
      "            +- HashAggregate(keys=[COMM#3304], functions=[partial_max(crime_counter#2373L)], schema specialized)\n",
      "               +- Union\n",
      "                  :- HashAggregate(keys=[COMM#3304], functions=[count(1)], schema specialized)\n",
      "                  :  +- Exchange hashpartitioning(COMM#3304, 1000), ENSURE_REQUIREMENTS, [plan_id=10464]\n",
      "                  :     +- HashAggregate(keys=[COMM#3304], functions=[partial_count(1)], schema specialized)\n",
      "                  :        +- Project [COMM#3304]\n",
      "                  :           +- RangeJoin geometry#718: geometry, point#882: geometry, CONTAINS\n",
      "                  :              :- Project [features#715.properties.COMM AS COMM#3304, features#715.geometry AS geometry#718]\n",
      "                  :              :  +- Filter (isnotnull(features#715.geometry) AND isnotnull(features#715.properties.COMM))\n",
      "                  :              :     +- Generate explode(features#3294), false, [features#715]\n",
      "                  :              :        +- Filter ((size(features#3294, true) > 0) AND isnotnull(features#3294))\n",
      "                  :              :           +- FileScan geojson [features#3294] Batched: false, DataFilters: [(size(features#3294, true) > 0), isnotnull(features#3294)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "                  :              +- Union\n",
      "                  :                 :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#882]\n",
      "                  :                 :  +- Filter ((((isnotnull(LAT#578) AND NOT (cast(LAT#578 as int) = 0)) AND isnotnull(LON#579)) AND NOT (cast(LON#579 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                  :                 :     +- FileScan csv [LAT#578,LON#579] Batched: false, DataFilters: [isnotnull(LAT#578), NOT (cast(LAT#578 as int) = 0), isnotnull(LON#579), NOT (cast(LON#579 as int..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<LAT:string,LON:string>\n",
      "                  :                 +- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#3327]\n",
      "                  :                    +- Filter ((((isnotnull(LAT#652) AND NOT (cast(LAT#652 as int) = 0)) AND isnotnull(LON#653)) AND NOT (cast(LON#653 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                  :                       +- FileScan csv [LAT#652,LON#653] Batched: false, DataFilters: [isnotnull(LAT#652), NOT (cast(LAT#652 as int) = 0), isnotnull(LON#653), NOT (cast(LON#653 as int..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<LAT:string,LON:string>\n",
      "                  +- HashAggregate(keys=[COMM#731], functions=[], schema specialized)\n",
      "                     +- Exchange hashpartitioning(COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=10466]\n",
      "                        +- HashAggregate(keys=[COMM#731], functions=[], schema specialized)\n",
      "                           +- Project [features#715.properties.COMM AS COMM#731]\n",
      "                              +- Filter (isnotnull(features#715.geometry) AND isnotnull(features#715.properties.COMM))\n",
      "                                 +- Generate explode(features#2470), false, [features#715]\n",
      "                                    +- Filter ((size(features#2470, true) > 0) AND isnotnull(features#2470))\n",
      "                                       +- FileScan geojson [features#2470] Batched: false, DataFilters: [(size(features#2470, true) > 0), isnotnull(features#2470)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "\n",
      "\n",
      "Join completed in 78.72 seconds with 323 rows using SHUFFLE_HASH.\n",
      "\n",
      "====================================================\n",
      "=== Join income_result & crime_per_community => all_join, Strategy: SHUFFLE_REPLICATE_NL ===\n",
      "\n",
      "Explain plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#731, average_income#3046, population#3047L, crime_counter#2482L]\n",
      "   +- CartesianProduct (COMM#731 = COMM#3408)\n",
      "      :- HashAggregate(keys=[COMM#731], functions=[sum((cast(med_inc#2992 as bigint) * houses_per_zip_code_in_community#2586L)), sum(population_for_income#2990L), sum(population#2584L)], schema specialized)\n",
      "      :  +- Exchange hashpartitioning(COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=12061]\n",
      "      :     +- HashAggregate(keys=[COMM#731], functions=[partial_sum((cast(med_inc#2992 as bigint) * houses_per_zip_code_in_community#2586L)), partial_sum(population_for_income#2990L), partial_sum(population#2584L)], schema specialized)\n",
      "      :        +- HashAggregate(keys=[ZCTA10#748, COMM#731, population#2584L, houses_per_zip_code_in_community#2586L], functions=[max(population_for_income#2938L), max(med_inc#2640)], schema specialized)\n",
      "      :           +- Exchange hashpartitioning(ZCTA10#748, COMM#731, population#2584L, houses_per_zip_code_in_community#2586L, 1000), ENSURE_REQUIREMENTS, [plan_id=12057]\n",
      "      :              +- HashAggregate(keys=[ZCTA10#748, COMM#731, population#2584L, houses_per_zip_code_in_community#2586L], functions=[partial_max(population_for_income#2938L), partial_max(med_inc#2640)], schema specialized)\n",
      "      :                 +- Union\n",
      "      :                    :- Project [ZCTA10#748, COMM#731, population#2584L, houses_per_zip_code_in_community#2586L, med_inc#2640, population#2584L AS population_for_income#2938L]\n",
      "      :                    :  +- CartesianProduct (ZCTA10#748 = zip_code#2639)\n",
      "      :                    :     :- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[sum(POP_2010#740L), sum(HOUSING10#737L)], schema specialized)\n",
      "      :                    :     :  +- Exchange hashpartitioning(ZCTA10#748, COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=12048]\n",
      "      :                    :     :     +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[partial_sum(POP_2010#740L), partial_sum(HOUSING10#737L)], schema specialized)\n",
      "      :                    :     :        +- Project [features#715.properties.ZCTA10 AS ZCTA10#748, features#715.properties.COMM AS COMM#731, features#715.properties.POP_2010 AS POP_2010#740L, features#715.properties.HOUSING10 AS HOUSING10#737L]\n",
      "      :                    :     :           +- Filter (isnotnull(features#715.geometry) AND (isnotnull(features#715.properties.ZCTA10) AND isnotnull(features#715.properties.COMM)))\n",
      "      :                    :     :              +- Generate explode(features#707), false, [features#715]\n",
      "      :                    :     :                 +- Filter ((size(features#707, true) > 0) AND isnotnull(features#707))\n",
      "      :                    :     :                    +- FileScan geojson [features#707] Batched: false, DataFilters: [(size(features#707, true) > 0), isnotnull(features#707)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      :                    :     +- Project [Zip Code#700 AS zip_code#2639, cast(regexp_replace(regexp_replace(Estimated Median Income#702, \\$, , 1), ,, , 1) as int) AS med_inc#2640]\n",
      "      :                    :        +- Filter isnotnull(Zip Code#700)\n",
      "      :                    :           +- FileScan csv [Zip Code#700,Estimated Median Income#702] Batched: false, DataFilters: [isnotnull(Zip Code#700)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "      :                    +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[sum(POP_2010#740L), sum(HOUSING10#737L)], schema specialized)\n",
      "      :                       +- Exchange hashpartitioning(ZCTA10#748, COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=12052]\n",
      "      :                          +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[partial_sum(POP_2010#740L), partial_sum(HOUSING10#737L)], schema specialized)\n",
      "      :                             +- Project [features#715.properties.ZCTA10 AS ZCTA10#748, features#715.properties.COMM AS COMM#731, features#715.properties.POP_2010 AS POP_2010#740L, features#715.properties.HOUSING10 AS HOUSING10#737L]\n",
      "      :                                +- Filter (isnotnull(features#715.geometry) AND isnotnull(features#715.properties.COMM))\n",
      "      :                                   +- Generate explode(features#2966), false, [features#715]\n",
      "      :                                      +- Filter ((size(features#2966, true) > 0) AND isnotnull(features#2966))\n",
      "      :                                         +- FileScan geojson [features#2966] Batched: false, DataFilters: [(size(features#2966, true) > 0), isnotnull(features#2966)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      +- HashAggregate(keys=[COMM#3408], functions=[max(crime_counter#2373L)], schema specialized)\n",
      "         +- Exchange hashpartitioning(COMM#3408, 1000), ENSURE_REQUIREMENTS, [plan_id=12070]\n",
      "            +- HashAggregate(keys=[COMM#3408], functions=[partial_max(crime_counter#2373L)], schema specialized)\n",
      "               +- Union\n",
      "                  :- HashAggregate(keys=[COMM#3408], functions=[count(1)], schema specialized)\n",
      "                  :  +- Exchange hashpartitioning(COMM#3408, 1000), ENSURE_REQUIREMENTS, [plan_id=12063]\n",
      "                  :     +- HashAggregate(keys=[COMM#3408], functions=[partial_count(1)], schema specialized)\n",
      "                  :        +- Project [COMM#3408]\n",
      "                  :           +- RangeJoin geometry#718: geometry, point#882: geometry, CONTAINS\n",
      "                  :              :- Project [features#715.properties.COMM AS COMM#3408, features#715.geometry AS geometry#718]\n",
      "                  :              :  +- Filter (isnotnull(features#715.geometry) AND isnotnull(features#715.properties.COMM))\n",
      "                  :              :     +- Generate explode(features#3398), false, [features#715]\n",
      "                  :              :        +- Filter ((size(features#3398, true) > 0) AND isnotnull(features#3398))\n",
      "                  :              :           +- FileScan geojson [features#3398] Batched: false, DataFilters: [(size(features#3398, true) > 0), isnotnull(features#3398)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "                  :              +- Union\n",
      "                  :                 :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#882]\n",
      "                  :                 :  +- Filter ((((isnotnull(LAT#578) AND NOT (cast(LAT#578 as int) = 0)) AND isnotnull(LON#579)) AND NOT (cast(LON#579 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                  :                 :     +- FileScan csv [LAT#578,LON#579] Batched: false, DataFilters: [isnotnull(LAT#578), NOT (cast(LAT#578 as int) = 0), isnotnull(LON#579), NOT (cast(LON#579 as int..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<LAT:string,LON:string>\n",
      "                  :                 +- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#3431]\n",
      "                  :                    +- Filter ((((isnotnull(LAT#652) AND NOT (cast(LAT#652 as int) = 0)) AND isnotnull(LON#653)) AND NOT (cast(LON#653 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                  :                       +- FileScan csv [LAT#652,LON#653] Batched: false, DataFilters: [isnotnull(LAT#652), NOT (cast(LAT#652 as int) = 0), isnotnull(LON#653), NOT (cast(LON#653 as int..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<LAT:string,LON:string>\n",
      "                  +- HashAggregate(keys=[COMM#731], functions=[], schema specialized)\n",
      "                     +- Exchange hashpartitioning(COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=12065]\n",
      "                        +- HashAggregate(keys=[COMM#731], functions=[], schema specialized)\n",
      "                           +- Project [features#715.properties.COMM AS COMM#731]\n",
      "                              +- Filter (isnotnull(features#715.geometry) AND isnotnull(features#715.properties.COMM))\n",
      "                                 +- Generate explode(features#2470), false, [features#715]\n",
      "                                    +- Filter ((size(features#2470, true) > 0) AND isnotnull(features#2470))\n",
      "                                       +- FileScan geojson [features#2470] Batched: false, DataFilters: [(size(features#2470, true) > 0), isnotnull(features#2470)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "\n",
      "\n",
      "Join completed in 75.17 seconds with 323 rows using SHUFFLE_REPLICATE_NL.\n",
      "\n",
      "+--------------------+--------------+----------+-------------+\n",
      "|                COMM|average_income|population|crime_counter|\n",
      "+--------------------+--------------+----------+-------------+\n",
      "|         Culver City|         33650|     38883|         1390|\n",
      "|     North Lancaster|         20102|      1151|            0|\n",
      "|Rosewood/East Gar...|         16211|      1164|          175|\n",
      "|East Rancho Domin...|          8832|     15135|            0|\n",
      "|      Toluca Terrace|         20167|      1301|          300|\n",
      "|        Elysian Park|         13871|      5267|         5714|\n",
      "|            Longwood|         13420|      4210|         3074|\n",
      "|         Pico Rivera|         15157|     62942|            2|\n",
      "|              Malibu|         67135|     12645|            1|\n",
      "|       Green Meadows|          8027|     19821|        23975|\n",
      "+--------------------+--------------+----------+-------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "join_strategies = [\"BROADCAST\", \"MERGE\", \"SHUFFLE_HASH\", \"SHUFFLE_REPLICATE_NL\"]\n",
    "\n",
    "for strategy in join_strategies:\n",
    "    print(\"====================================================\")\n",
    "    print(f\"=== Join income_result & crime_per_community => all_join, Strategy: {strategy} ===\")\n",
    "    \n",
    "    # Clear any cached data so previous strategies don't affect this run\n",
    "    spark.catalog.clearCache()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    all_join = (\n",
    "        income_result.hint(strategy)\n",
    "        .join(\n",
    "            crime_per_community.hint(strategy),\n",
    "            on=\"COMM\",\n",
    "            how=\"inner\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Print the physical plan to confirm which strategy Spark uses\n",
    "    print(\"\\nExplain plan:\")\n",
    "    all_join.explain()\n",
    "    \n",
    "    # Trigger the actual join and measure time\n",
    "    all_join_counter = all_join.count()\n",
    "    join_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nJoin completed in {join_time:.2f} seconds with {all_join_counter} rows using {strategy}.\\n\")\n",
    "    \n",
    "# (Optional) Show some result rows\n",
    "all_join.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66a83d80-f10a-4e81-8b64-2ebb0eb442f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#731, average_income#3046, population#3047L, crime_counter#2482L]\n",
      "   +- SortMergeJoin [COMM#731], [COMM#3609], Inner\n",
      "      :- Sort [COMM#731 ASC NULLS FIRST], false, 0\n",
      "      :  +- HashAggregate(keys=[COMM#731], functions=[sum((cast(med_inc#2992 as bigint) * houses_per_zip_code_in_community#2586L)), sum(population_for_income#2990L), sum(population#2584L)], schema specialized)\n",
      "      :     +- Exchange hashpartitioning(COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=15881]\n",
      "      :        +- HashAggregate(keys=[COMM#731], functions=[partial_sum((cast(med_inc#2992 as bigint) * houses_per_zip_code_in_community#2586L)), partial_sum(population_for_income#2990L), partial_sum(population#2584L)], schema specialized)\n",
      "      :           +- HashAggregate(keys=[ZCTA10#748, COMM#731, population#2584L, houses_per_zip_code_in_community#2586L], functions=[max(population_for_income#2938L), max(med_inc#2640)], schema specialized)\n",
      "      :              +- Exchange hashpartitioning(ZCTA10#748, COMM#731, population#2584L, houses_per_zip_code_in_community#2586L, 1000), ENSURE_REQUIREMENTS, [plan_id=15877]\n",
      "      :                 +- HashAggregate(keys=[ZCTA10#748, COMM#731, population#2584L, houses_per_zip_code_in_community#2586L], functions=[partial_max(population_for_income#2938L), partial_max(med_inc#2640)], schema specialized)\n",
      "      :                    +- Union\n",
      "      :                       :- Project [ZCTA10#748, COMM#731, population#2584L, houses_per_zip_code_in_community#2586L, med_inc#2640, population#2584L AS population_for_income#2938L]\n",
      "      :                       :  +- CartesianProduct (ZCTA10#748 = zip_code#2639)\n",
      "      :                       :     :- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[sum(POP_2010#740L), sum(HOUSING10#737L)], schema specialized)\n",
      "      :                       :     :  +- Exchange hashpartitioning(ZCTA10#748, COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=15868]\n",
      "      :                       :     :     +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[partial_sum(POP_2010#740L), partial_sum(HOUSING10#737L)], schema specialized)\n",
      "      :                       :     :        +- Project [features#715.properties.ZCTA10 AS ZCTA10#748, features#715.properties.COMM AS COMM#731, features#715.properties.POP_2010 AS POP_2010#740L, features#715.properties.HOUSING10 AS HOUSING10#737L]\n",
      "      :                       :     :           +- Filter (isnotnull(features#715.geometry) AND (isnotnull(features#715.properties.ZCTA10) AND isnotnull(features#715.properties.COMM)))\n",
      "      :                       :     :              +- Generate explode(features#707), false, [features#715]\n",
      "      :                       :     :                 +- Filter ((size(features#707, true) > 0) AND isnotnull(features#707))\n",
      "      :                       :     :                    +- FileScan geojson [features#707] Batched: false, DataFilters: [(size(features#707, true) > 0), isnotnull(features#707)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      :                       :     +- Project [Zip Code#700 AS zip_code#2639, cast(regexp_replace(regexp_replace(Estimated Median Income#702, \\$, , 1), ,, , 1) as int) AS med_inc#2640]\n",
      "      :                       :        +- Filter isnotnull(Zip Code#700)\n",
      "      :                       :           +- FileScan csv [Zip Code#700,Estimated Median Income#702] Batched: false, DataFilters: [isnotnull(Zip Code#700)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "      :                       +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[sum(POP_2010#740L), sum(HOUSING10#737L)], schema specialized)\n",
      "      :                          +- Exchange hashpartitioning(ZCTA10#748, COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=15872]\n",
      "      :                             +- HashAggregate(keys=[ZCTA10#748, COMM#731], functions=[partial_sum(POP_2010#740L), partial_sum(HOUSING10#737L)], schema specialized)\n",
      "      :                                +- Project [features#715.properties.ZCTA10 AS ZCTA10#748, features#715.properties.COMM AS COMM#731, features#715.properties.POP_2010 AS POP_2010#740L, features#715.properties.HOUSING10 AS HOUSING10#737L]\n",
      "      :                                   +- Filter (isnotnull(features#715.geometry) AND isnotnull(features#715.properties.COMM))\n",
      "      :                                      +- Generate explode(features#2966), false, [features#715]\n",
      "      :                                         +- Filter ((size(features#2966, true) > 0) AND isnotnull(features#2966))\n",
      "      :                                            +- FileScan geojson [features#2966] Batched: false, DataFilters: [(size(features#2966, true) > 0), isnotnull(features#2966)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      +- Sort [COMM#3609 ASC NULLS FIRST], false, 0\n",
      "         +- HashAggregate(keys=[COMM#3609], functions=[max(crime_counter#2373L)], schema specialized)\n",
      "            +- Exchange hashpartitioning(COMM#3609, 1000), ENSURE_REQUIREMENTS, [plan_id=15890]\n",
      "               +- HashAggregate(keys=[COMM#3609], functions=[partial_max(crime_counter#2373L)], schema specialized)\n",
      "                  +- Union\n",
      "                     :- HashAggregate(keys=[COMM#3609], functions=[count(1)], schema specialized)\n",
      "                     :  +- Exchange hashpartitioning(COMM#3609, 1000), ENSURE_REQUIREMENTS, [plan_id=15883]\n",
      "                     :     +- HashAggregate(keys=[COMM#3609], functions=[partial_count(1)], schema specialized)\n",
      "                     :        +- Project [COMM#3609]\n",
      "                     :           +- RangeJoin geometry#718: geometry, point#882: geometry, CONTAINS\n",
      "                     :              :- Project [features#715.properties.COMM AS COMM#3609, features#715.geometry AS geometry#718]\n",
      "                     :              :  +- Filter (isnotnull(features#715.geometry) AND isnotnull(features#715.properties.COMM))\n",
      "                     :              :     +- Generate explode(features#3599), false, [features#715]\n",
      "                     :              :        +- Filter ((size(features#3599, true) > 0) AND isnotnull(features#3599))\n",
      "                     :              :           +- FileScan geojson [features#3599] Batched: false, DataFilters: [(size(features#3599, true) > 0), isnotnull(features#3599)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "                     :              +- Union\n",
      "                     :                 :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#882]\n",
      "                     :                 :  +- Filter ((((isnotnull(LAT#578) AND NOT (cast(LAT#578 as int) = 0)) AND isnotnull(LON#579)) AND NOT (cast(LON#579 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                     :                 :     +- FileScan csv [LAT#578,LON#579] Batched: false, DataFilters: [isnotnull(LAT#578), NOT (cast(LAT#578 as int) = 0), isnotnull(LON#579), NOT (cast(LON#579 as int..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<LAT:string,LON:string>\n",
      "                     :                 +- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#3671]\n",
      "                     :                    +- Filter ((((isnotnull(LAT#652) AND NOT (cast(LAT#652 as int) = 0)) AND isnotnull(LON#653)) AND NOT (cast(LON#653 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                     :                       +- FileScan csv [LAT#652,LON#653] Batched: false, DataFilters: [isnotnull(LAT#652), NOT (cast(LAT#652 as int) = 0), isnotnull(LON#653), NOT (cast(LON#653 as int..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<LAT:string,LON:string>\n",
      "                     +- HashAggregate(keys=[COMM#731], functions=[], schema specialized)\n",
      "                        +- Exchange hashpartitioning(COMM#731, 1000), ENSURE_REQUIREMENTS, [plan_id=15885]\n",
      "                           +- HashAggregate(keys=[COMM#731], functions=[], schema specialized)\n",
      "                              +- Project [features#715.properties.COMM AS COMM#731]\n",
      "                                 +- Filter (isnotnull(features#715.geometry) AND isnotnull(features#715.properties.COMM))\n",
      "                                    +- Generate explode(features#2470), false, [features#715]\n",
      "                                       +- Filter ((size(features#2470, true) > 0) AND isnotnull(features#2470))\n",
      "                                          +- FileScan geojson [features#2470] Batched: false, DataFilters: [(size(features#2470, true) > 0), isnotnull(features#2470)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "\n",
      "Join completed in 76.99 seconds with 323 rows.\n",
      "\n",
      "+--------------------+--------------+----------+-------------+\n",
      "|                COMM|average_income|population|crime_counter|\n",
      "+--------------------+--------------+----------+-------------+\n",
      "|         Culver City|         33650|     38883|         1390|\n",
      "|Rosewood/East Gar...|         16211|      1164|          175|\n",
      "|      Toluca Terrace|         20167|      1301|          300|\n",
      "|        Elysian Park|         13871|      5267|         5714|\n",
      "|            Longwood|         13420|      4210|         3074|\n",
      "|         Pico Rivera|         15157|     62942|            2|\n",
      "|              Malibu|         67135|     12645|            1|\n",
      "|       Green Meadows|          8027|     19821|        23975|\n",
      "|    Cadillac-Corning|         19572|      6665|         4445|\n",
      "|            Mid-city|         21734|     14339|        11574|\n",
      "+--------------------+--------------+----------+-------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "all_join =  income_result.join(crime_per_community, on=\"COMM\", how=\"inner\")\n",
    "\n",
    "all_join_counter = all_join.count()\n",
    "all_join.explain()\n",
    "# Show the result (optional)\n",
    "join_time = time.time() - start_time\n",
    "print(f\"Join completed in {join_time:.2f} seconds with {all_join_counter} rows.\\n\")\n",
    "all_join.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a67451ac-10ff-4854-abe8-9bcf3feae336",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final table has 323 rows\n",
      "\n",
      "+--------------------+--------------+--------------------+\n",
      "|                COMM|average_income|   crimes_per_person|\n",
      "+--------------------+--------------+--------------------+\n",
      "|         Culver City|         33650| 0.03574827045238279|\n",
      "|     North Lancaster|         20102|                 0.0|\n",
      "|Rosewood/East Gar...|         16211| 0.15034364261168384|\n",
      "|East Rancho Domin...|          8832|                 0.0|\n",
      "|      Toluca Terrace|         20167| 0.23059185242121444|\n",
      "|        Elysian Park|         13871|   1.084868046326182|\n",
      "|            Longwood|         13420|   0.730166270783848|\n",
      "|         Pico Rivera|         15157|3.177528518318452E-5|\n",
      "|              Malibu|         67135|7.908264136022143E-5|\n",
      "|       Green Meadows|          8027|  1.2095757025377125|\n",
      "|    Hacienda Heights|         24046|                 0.0|\n",
      "|    Cadillac-Corning|         19572|  0.6669167291822956|\n",
      "|  West Puente Valley|         12533|                 0.0|\n",
      "|          Montebello|         14514|              9.6E-5|\n",
      "|            Mid-city|         21734|  0.8071692586651789|\n",
      "|          Lake Manor|         33720|                 0.0|\n",
      "|    Hawaiian Gardens|          9911|                 0.0|\n",
      "|     Lincoln Heights|         10902|  0.6226239404058567|\n",
      "|    Westlake Village|         42843|1.209189842805320...|\n",
      "|            Van Nuys|         14488|  0.9170415838361292|\n",
      "|    Placerita Canyon|         19498|                 0.0|\n",
      "|              Carson|         20417|0.004699391586889679|\n",
      "|     Bandini Islands|          NULL|                NULL|\n",
      "|     Rowland Heights|         18998|4.078137106969536...|\n",
      "|        Agoura Hills|         42688|2.951303492375799...|\n",
      "|      Universal City|          NULL|                NULL|\n",
      "|            Glendale|         23254|7.093715281218867E-4|\n",
      "|Northeast San Gab...|         23807|                 0.0|\n",
      "|      Gramercy Place|         14936|  1.0759579191197761|\n",
      "|      Bouquet Canyon|         53803|                 0.0|\n",
      "+--------------------+--------------+--------------------+\n",
      "only showing top 30 rows"
     ]
    }
   ],
   "source": [
    "# Create the final table with the desired columns and calculations\n",
    "final_table = all_join.select(\n",
    "    \"COMM\",\n",
    "    \"average_income\",\n",
    "    (F.col(\"crime_counter\") / F.col(\"population\")).alias(\"crimes_per_person\")\n",
    ")\n",
    "\n",
    "final_table_counter = final_table.count()\n",
    "print(f\"The final table has {final_table_counter} rows\\n\")\n",
    "# Show the result (optional)\n",
    "final_table.show(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
