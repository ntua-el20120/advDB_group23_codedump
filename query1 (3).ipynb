{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5f5634-70fe-428a-b0d1-561284426d47",
   "metadata": {},
   "source": [
    "Output URI\n",
    "s3://groups-bucket-dblab-905418150721/group23/outputs/query1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f83ee2-0b1f-4e66-9fb6-25580fd8280c",
   "metadata": {},
   "source": [
    "Να ταξινομημηθούν, σε φθίνουσα σειρά, οι ηλικιακές ομάδες των θυμάτων σε περιστατικά που περιλαμβάνουν οποιαδήποτε μορφή “βαριάς σωματικής βλάβης”. Θεωρείστε τις εξής ηλικιακές ομάδες:\\\n",
    "• Παιδιά: < 18 • Νεαροί ενήλικοι: 18 – 24\n",
    "• Ενήλικοι: 25 – 64 • Ηλικιωμένοι: >64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878ac2d7-d4ca-40a7-b4ca-7c5517ec1f21",
   "metadata": {},
   "source": [
    "Να υλοποιηθεί το Query 1 χρησιμοποιώντας τα DataFrame και RDD APIs. Να εκτελέσετε και\n",
    "τις δύο υλοποιήσεις με 4 Spark executors. Υπάρχει διαφορά στην επίδοση μεταξύ των δύο APIs;\\\n",
    "Αιτιολογήσετε την απάντησή σας."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeaccc0-c784-4e47-aa69-e6b3ab6e663a",
   "metadata": {},
   "source": [
    "Αρχικά θέτουμε τον configuration όπως το θέλει η εκφώνηση:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a59456f0-aeca-4421-b4f0-2f6ae37578cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '4', 'spark.executor.memory': '2g', 'spark.executor.cores': '2', 'spark.driver.memory': '4g'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"4\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"2\",\n",
    "        \"spark.driver.memory\": \"4g\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974551c-5c75-4c6e-bab6-f0923fc76a2d",
   "metadata": {},
   "source": [
    "Για να υλοποιήσουμε το query με RDD θα κάνουμε τις εξής πράξεις:\n",
    "* Ανοίγουμε τα 2 csv\n",
    "* Κάνουμε union στα 2 csv\n",
    "* κάνουμε map στη συνάρτηση parse_csv (θέλουμε να αγνοούνται τα \",\" μέσα σε strings γι αυτό δεν μπορούμε να κάνουμε split στο \",\")\n",
    "* κρατάμε μόνο τις γραμμές που έχουν τον όρο \"aggravated assault\" στην σωστή στήλη (10)\n",
    "* για κάθε μια από τις γραμμές κρατάμε μόνο την ηλιακή ομάδα, βρίσκοντας ηλικία, κανοντάς την int και περνόντας την απότ την συνάρτηση age_ranges, ως κλειδί και τον αριθμό 1 ως value\n",
    "* κάνουμε reduce by key παίρνοντας το άθροισμα των επιμέρους τιμών\n",
    "* ταξινομούμε κατα φθίνουσα σειρά"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edeb5e6f-f6e0-4c71-9b11-f8b33bb2f29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2015</td><td>application_1732639283265_1975</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1975/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-178.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1975_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"wordcount example\") \\\n",
    "    .getOrCreate() \\\n",
    "    .sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bba6c700-4b81-430d-a1a4-7d9d590b6111",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Adults', 121093), ('Young Adults', 33605), ('Children', 15928), ('Elderly', 5985)]\n",
      "Time taken: 19.45 seconds"
     ]
    }
   ],
   "source": [
    "# Spark RDD code\n",
    "\n",
    "# To log our application's execution time:\n",
    "import time\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "def age_ranges(age: int):\n",
    "    if age < 18:\n",
    "        return \"Children\"\n",
    "    elif age < 25:\n",
    "        return \"Young Adults\"\n",
    "    elif age < 65:\n",
    "        return \"Adults\"\n",
    "    else:\n",
    "        return \"Elderly\"\n",
    "\n",
    "def parse_csv(line):\n",
    "    return next(csv.reader(StringIO(line)))\n",
    "    \n",
    "crime_2019_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "crime_2020_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "first_csv = sc.textFile(crime_2019_path)\n",
    "second_csv = sc.textFile(crime_2020_path)\n",
    "\n",
    "something = first_csv.union(second_csv) \\\n",
    "            .map(parse_csv) \\\n",
    "            .filter(lambda f: \"aggravated assault\" in f[9].lower()) \\\n",
    "            .map(lambda f: (age_ranges(int(f[11])), 1)) \\\n",
    "            .reduceByKey(lambda x,y: x+y) \\\n",
    "            .sortBy(lambda f: f[1], ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "print(something.collect())\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "852cba83-1afe-4a48-aa8b-6c6c4a64186c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"wordcount example\") \\\n",
    "    .getOrCreate() \\\n",
    "    .sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80be201d-1ac1-4387-bf9d-efd28cb96805",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "+------------+------+\n",
      "|   Age Range| count|\n",
      "+------------+------+\n",
      "|      Adults|121093|\n",
      "|Young Adults| 33605|\n",
      "|    Children| 15928|\n",
      "|     Elderly|  5985|\n",
      "+------------+------+\n",
      "\n",
      "Time taken: 20.84 seconds"
     ]
    }
   ],
   "source": [
    "# Spark DataFrame code\n",
    "from pyspark.sql.functions import col, lower, udf\n",
    "from pyspark.sql.types import StringType\n",
    "import time\n",
    "\n",
    "def age_ranges(age: str):\n",
    "    age = int(age)\n",
    "    if age < 18:\n",
    "        return \"Children\"\n",
    "    elif age < 25:\n",
    "        return \"Young Adults\"\n",
    "    elif age < 65:\n",
    "        return \"Adults\"\n",
    "    else:\n",
    "        return \"Elderly\"\n",
    "\n",
    "crime_2019_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "crime_2020_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "age_ranges_udf = udf(age_ranges, StringType())\n",
    "\n",
    "crime_2019 = spark.read.csv(crime_2019_path, header=True)\n",
    "crime_2020 = spark.read.csv(crime_2020_path, header=True)\n",
    "\n",
    "something = crime_2019.union(crime_2020)\n",
    "something = something.filter(lower(col(\"Crm Cd Desc\")).contains(\"aggravated assault\"))\n",
    "something = something.withColumn(\"Age Range\", age_ranges_udf(col(\"Vict Age\")))\n",
    "something = something.select(\"Age Range\")\n",
    "something = something.groupBy(\"Age Range\").count()\n",
    "something = something.orderBy(\"count\", ascending=False)\n",
    "something.show()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45359312-2457-433d-96d8-dfd43aa5583d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
